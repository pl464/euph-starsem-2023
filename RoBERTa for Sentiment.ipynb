{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19fb1dd4-a270-474b-a2b6-9bcab6a06ce9",
   "metadata": {},
   "source": [
    "# Assigning sentiment scores\n",
    "This code takes a euphemism corpus and assigns each example sentiment/offensiveness scores. This data can then be used for experimentation with BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c9917-dd2e-46f0-ba30-fd453c64f14f",
   "metadata": {},
   "source": [
    "First, the sentiment packages are loaded using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce28f07-d74b-44ff-b055-85fc92dd4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import urllib.request\n",
    "import csv\n",
    "from scipy.special import softmax\n",
    "import re\n",
    "\n",
    "# download the roberta models/data, if it's not already there, and then load the labels, model, and tokenizer\n",
    "def load_roberta(task):\n",
    "    # Tasks:\n",
    "    # emoji, emotion, hate, irony, offensive, sentiment\n",
    "    # stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "    # task='sentiment' or 'offensive'\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "    # download label mapping\n",
    "    labels=[]\n",
    "    mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "    with urllib.request.urlopen(mapping_link) as f:\n",
    "        html = f.read().decode('utf-8').split(\"\\n\")\n",
    "        csvreader = csv.reader(html, delimiter='\\t')\n",
    "    labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "    # pretrained\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    model.save_pretrained(MODEL)\n",
    "    tokenizer.save_pretrained(MODEL)\n",
    "\n",
    "    return labels, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968caab9-f3fc-4463-8537-a4c0dfd06ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9507522, 0.039947942, 0.00929983, 0.19243449, 0.80756557]\n"
     ]
    }
   ],
   "source": [
    "# puts the labels, model, and tokenizer from each roberta model into a list (more concise) \n",
    "sentiment_pack = [x for x in load_roberta('sentiment')]\n",
    "offensive_pack = [x for x in load_roberta('offensive')]\n",
    "\n",
    "# actually use the labels, model and tokenizer to generate a sentiment/offensiveness score\n",
    "def get_sentiment(s, pack):\n",
    "    labels, model, tokenizer = pack[0], pack[1], pack[2]\n",
    "    encoded_input = tokenizer(s, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return scores\n",
    "\n",
    "# get the sentiment/offensive scores for this paraphrase\n",
    "s = \"You stink!\"\n",
    "scores = list(get_sentiment(s, sentiment_pack))\n",
    "scores = scores + list(get_sentiment(s, offensive_pack))\n",
    "print(scores) # in order: NEGATIVE, NEUTRAL, POSITIVE, NON-OFFENSIVE, OFFENSIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "476bde6a-533b-4f07-aeec-146187cf6399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad277fc84de242daba7e7869807fa7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>neg_PET</th>\n",
       "      <th>neu_PET</th>\n",
       "      <th>pos_PET</th>\n",
       "      <th>...</th>\n",
       "      <th>neg_TEXT</th>\n",
       "      <th>neu_TEXT</th>\n",
       "      <th>pos_TEXT</th>\n",
       "      <th>noff_TEXT</th>\n",
       "      <th>off_TEXT</th>\n",
       "      <th>neg_SENT</th>\n",
       "      <th>neu_SENT</th>\n",
       "      <th>pos_SENT</th>\n",
       "      <th>noff_SENT</th>\n",
       "      <th>off_SENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tinkle</td>\n",
       "      <td>We're just getting back what was TAKEN from us...</td>\n",
       "      <td>1</td>\n",
       "      <td>body functions/parts</td>\n",
       "      <td>tinkle</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>We're just getting back what was TAKEN from us...</td>\n",
       "      <td>0.215733</td>\n",
       "      <td>0.593470</td>\n",
       "      <td>0.190797</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.633027</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>0.039291</td>\n",
       "      <td>0.561716</td>\n",
       "      <td>0.438283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tinkle</td>\n",
       "      <td>I think AB390 will pass next year now that the...</td>\n",
       "      <td>1</td>\n",
       "      <td>body functions/parts</td>\n",
       "      <td>tinkle</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I think AB390 will pass next year now that the...</td>\n",
       "      <td>0.215733</td>\n",
       "      <td>0.593470</td>\n",
       "      <td>0.190797</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.513002</td>\n",
       "      <td>0.423893</td>\n",
       "      <td>0.063104</td>\n",
       "      <td>0.887314</td>\n",
       "      <td>0.112686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>venereal disease</td>\n",
       "      <td>He's being sued by a woman who claims he gave ...</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>venereal disease</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>And Kris Humphries plans on being' relentless'...</td>\n",
       "      <td>0.717446</td>\n",
       "      <td>0.265028</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.191349</td>\n",
       "      <td>0.667033</td>\n",
       "      <td>0.141619</td>\n",
       "      <td>0.810757</td>\n",
       "      <td>0.189243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>venereal disease</td>\n",
       "      <td>The pair then allegedly went to Humphries' hot...</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>venereal disease</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>The court documents state the two' had sexual ...</td>\n",
       "      <td>0.717446</td>\n",
       "      <td>0.265028</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.681177</td>\n",
       "      <td>0.307028</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>0.556158</td>\n",
       "      <td>0.443842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>venereal disease</td>\n",
       "      <td>For hemorrhoids take tow and put salt on it an...</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>venereal disease</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>For a man who suffers from swelling and from v...</td>\n",
       "      <td>0.717446</td>\n",
       "      <td>0.265028</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.769499</td>\n",
       "      <td>0.216996</td>\n",
       "      <td>0.013505</td>\n",
       "      <td>0.632338</td>\n",
       "      <td>0.367662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>sleep with</td>\n",
       "      <td>There were other photos she wanted me to see: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep with</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>There were other photos she wanted me to see B...</td>\n",
       "      <td>0.145681</td>\n",
       "      <td>0.730261</td>\n",
       "      <td>0.124058</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.017071</td>\n",
       "      <td>0.460846</td>\n",
       "      <td>0.522083</td>\n",
       "      <td>0.755889</td>\n",
       "      <td>0.244111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>sleep with</td>\n",
       "      <td>I am relieved to see two pup tents marked STAF...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep with</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Thank God I don't have to sleep with Ace Wands</td>\n",
       "      <td>0.145681</td>\n",
       "      <td>0.730261</td>\n",
       "      <td>0.124058</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.206293</td>\n",
       "      <td>0.771650</td>\n",
       "      <td>0.832420</td>\n",
       "      <td>0.167580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>sleep around</td>\n",
       "      <td>Nothing serious, just long nights of me hackin...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep around</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>With all my caterwauling it's a wonder anyone ...</td>\n",
       "      <td>0.139588</td>\n",
       "      <td>0.747621</td>\n",
       "      <td>0.112791</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.725939</td>\n",
       "      <td>0.246083</td>\n",
       "      <td>0.027978</td>\n",
       "      <td>0.604972</td>\n",
       "      <td>0.395028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>with child</td>\n",
       "      <td>sounds more like Jonestown. They cant leave @ ...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>with child</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>They cant leave best advice I can give them is...</td>\n",
       "      <td>0.182822</td>\n",
       "      <td>0.721971</td>\n",
       "      <td>0.095207</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.876646</td>\n",
       "      <td>0.117473</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>0.706067</td>\n",
       "      <td>0.293933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>with child</td>\n",
       "      <td>Nickname of a girl named Diana. 41. What you d...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>with child</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>What you do with Child Life 42</td>\n",
       "      <td>0.182822</td>\n",
       "      <td>0.721971</td>\n",
       "      <td>0.095207</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.112453</td>\n",
       "      <td>0.854126</td>\n",
       "      <td>0.033421</td>\n",
       "      <td>0.922798</td>\n",
       "      <td>0.077202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1292 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               keyword                                        edited_text  \\\n",
       "0               tinkle  We're just getting back what was TAKEN from us...   \n",
       "1               tinkle  I think AB390 will pass next year now that the...   \n",
       "23    venereal disease  He's being sued by a woman who claims he gave ...   \n",
       "24    venereal disease  The pair then allegedly went to Humphries' hot...   \n",
       "25    venereal disease  For hemorrhoids take tow and put salt on it an...   \n",
       "...                ...                                                ...   \n",
       "1960        sleep with  There were other photos she wanted me to see: ...   \n",
       "1961        sleep with  I am relieved to see two pup tents marked STAF...   \n",
       "1962      sleep around  Nothing serious, just long nights of me hackin...   \n",
       "1963        with child  sounds more like Jonestown. They cant leave @ ...   \n",
       "1964        with child  Nickname of a girl named Diana. 41. What you d...   \n",
       "\n",
       "      is_euph                    category              type     euph_status  \\\n",
       "0           1        body functions/parts            tinkle     always_euph   \n",
       "1           1        body functions/parts            tinkle     always_euph   \n",
       "23          1             sexual activity  venereal disease     always_euph   \n",
       "24          1             sexual activity  venereal disease     always_euph   \n",
       "25          1             sexual activity  venereal disease     always_euph   \n",
       "...       ...                         ...               ...             ...   \n",
       "1960        0             sexual activity        sleep with  sometimes_euph   \n",
       "1961        0             sexual activity        sleep with  sometimes_euph   \n",
       "1962        0             sexual activity      sleep around  sometimes_euph   \n",
       "1963        0  physical/mental attributes        with child  sometimes_euph   \n",
       "1964        0  physical/mental attributes        with child  sometimes_euph   \n",
       "\n",
       "                                               sentence   neg_PET   neu_PET  \\\n",
       "0     We're just getting back what was TAKEN from us...  0.215733  0.593470   \n",
       "1     I think AB390 will pass next year now that the...  0.215733  0.593470   \n",
       "23    And Kris Humphries plans on being' relentless'...  0.717446  0.265028   \n",
       "24    The court documents state the two' had sexual ...  0.717446  0.265028   \n",
       "25    For a man who suffers from swelling and from v...  0.717446  0.265028   \n",
       "...                                                 ...       ...       ...   \n",
       "1960  There were other photos she wanted me to see B...  0.145681  0.730261   \n",
       "1961    Thank God I don't have to sleep with Ace Wands   0.145681  0.730261   \n",
       "1962  With all my caterwauling it's a wonder anyone ...  0.139588  0.747621   \n",
       "1963  They cant leave best advice I can give them is...  0.182822  0.721971   \n",
       "1964                    What you do with Child Life 42   0.182822  0.721971   \n",
       "\n",
       "       pos_PET  ...  neg_TEXT  neu_TEXT  pos_TEXT  noff_TEXT  off_TEXT  \\\n",
       "0     0.190797  ...        -1        -1        -1         -1        -1   \n",
       "1     0.190797  ...        -1        -1        -1         -1        -1   \n",
       "23    0.017526  ...        -1        -1        -1         -1        -1   \n",
       "24    0.017526  ...        -1        -1        -1         -1        -1   \n",
       "25    0.017526  ...        -1        -1        -1         -1        -1   \n",
       "...        ...  ...       ...       ...       ...        ...       ...   \n",
       "1960  0.124058  ...        -1        -1        -1         -1        -1   \n",
       "1961  0.124058  ...        -1        -1        -1         -1        -1   \n",
       "1962  0.112791  ...        -1        -1        -1         -1        -1   \n",
       "1963  0.095207  ...        -1        -1        -1         -1        -1   \n",
       "1964  0.095207  ...        -1        -1        -1         -1        -1   \n",
       "\n",
       "      neg_SENT  neu_SENT  pos_SENT  noff_SENT  off_SENT  \n",
       "0     0.633027  0.327682  0.039291   0.561716  0.438283  \n",
       "1     0.513002  0.423893  0.063104   0.887314  0.112686  \n",
       "23    0.191349  0.667033  0.141619   0.810757  0.189243  \n",
       "24    0.681177  0.307028  0.011795   0.556158  0.443842  \n",
       "25    0.769499  0.216996  0.013505   0.632338  0.367662  \n",
       "...        ...       ...       ...        ...       ...  \n",
       "1960  0.017071  0.460846  0.522083   0.755889  0.244111  \n",
       "1961  0.022057  0.206293  0.771650   0.832420  0.167580  \n",
       "1962  0.725939  0.246083  0.027978   0.604972  0.395028  \n",
       "1963  0.876646  0.117473  0.005881   0.706067  0.293933  \n",
       "1964  0.112453  0.854126  0.033421   0.922798  0.077202  \n",
       "\n",
       "[1292 rows x 22 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "euph_corpus = pd.read_csv('Euphemism_Corpus_v2.1.csv', index_col=0)\n",
    "sent_corpus = euph_corpus.copy() # initialize new corpus and new columns\n",
    "sent_corpus['neg_PET'], sent_corpus['neu_PET'], sent_corpus['pos_PET'], sent_corpus['noff_PET'], sent_corpus['off_PET'], sent_corpus['neg_TEXT'], sent_corpus['neu_TEXT'], sent_corpus['pos_TEXT'], sent_corpus['noff_TEXT'], sent_corpus['off_TEXT']  = -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n",
    "\n",
    "# for now, let's only work with PETs that have shown to have high agreement in some examples\n",
    "high_agreement_PETs = ['slim', 'between jobs', 'accident', 'late', 'number one', 'sleep with', 'seasoned', 'wealthy', 'over the hill', 'plump', 'let go of', 'go all the way', 'overweight', 'sober', 'number two', 'slept with', 'dismissed', 'let them go', 'aging', 'expecting', 'stout', 'troubled', 'with child', 'invalid', 'experienced', 'getting clean', 'custodian', 'got clean', 'long sleep', 'mixed up', 'chest', 'same-sex', 'economical', 'passing on', 'neutralize', 'outspoken', 'gluteus maximus', 'sleep around', 'pass on', 'disabled', 'special needs', 'pass away', 'a certain age', 'well off', 'less fortunate', 'mistruths', 'droppings', 'lose your lunch', 'pregnancy termination', 'let him go', 'golden years', 'mentally challenged', 'tinkle', 'demise', 'drinking problem', 'indigent', 'detainee', 'advanced age', 'comfort women', 'time of the month', 'pass gas', 'portly', 'went to heaven', 'venereal disease', 'put to sleep', 'mistruth', 'differently-abled', 'intoxicated', 'economical with the truth', 'lavatory', 'birds and the bees', 'deceased', 'terminating a pregnancy', 'inebriated', 'inner city', 'regime change', 'enhanced interrogation techniques', 'adult beverages', 'to go to heaven', 'dearly departed', 'passed away', 'downsize', 'ethnic cleansing', 'substance abusers', 'broken home', 'made love', 'plus-sized', 'underprivileged', 'rear end', 'armed conflict', 'substance abuse', 'disadvantaged', 'neutralized', 'capital punishment', 'street person', 'making love', 'freedom fighters']\n",
    "sent_corpus = sent_corpus.loc[sent_corpus['keyword'].isin(high_agreement_PETs)]\n",
    "\n",
    "for i, row in tqdm(sent_corpus.iterrows()):\n",
    "    # i = 300 # 300, 1300, 1000\n",
    "    PET = sent_corpus.loc[i, 'type']\n",
    "    scores = list(get_sentiment(PET, sentiment_pack))\n",
    "    scores = scores + list(get_sentiment(PET, offensive_pack))\n",
    "    sent_corpus.loc[i, 'neg_PET'] = float(scores[0])\n",
    "    sent_corpus.loc[i, 'neu_PET'] = float(scores[1])\n",
    "    sent_corpus.loc[i, 'pos_PET'] = float(scores[2])\n",
    "    sent_corpus.loc[i, 'noff_PET'] = float(scores[3])\n",
    "    sent_corpus.loc[i, 'off_PET'] = float(scores[4])\n",
    "    \n",
    "    text = sent_corpus.loc[i, 'sentence']\n",
    "    scores = list(get_sentiment(text, sentiment_pack))\n",
    "    scores = scores + list(get_sentiment(text, offensive_pack))\n",
    "    sent_corpus.loc[i, 'neg_SENT'] = float(scores[0])\n",
    "    sent_corpus.loc[i, 'neu_SENT'] = float(scores[1])\n",
    "    sent_corpus.loc[i, 'pos_SENT'] = float(scores[2])\n",
    "    sent_corpus.loc[i, 'noff_SENT'] = float(scores[3])\n",
    "    sent_corpus.loc[i, 'off_SENT'] = float(scores[4])\n",
    "    \n",
    "sent_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f34eb5f-9972-4afa-ad6e-ed2321feae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_corpus.to_csv(\"Sentiment_Corpus_v1.2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26bc7f4c-c84f-4d8d-bf3e-b4e89746bcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5637176243025203\n",
      "0.5891607105731964\n",
      "304\n",
      "138\n",
      "540\n",
      "310\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "sent_corpus = pd.read_csv(\"Sentiment_Corpus_v1.2.csv\", index_col=0)\n",
    "\n",
    "# analysis to generalize labels in the Excel file\n",
    "PROPERTY = \"neu_PET\"\n",
    "scores = sent_corpus[PROPERTY].unique()\n",
    "mean_value = np.mean(scores)\n",
    "median_value = np.median(scores)\n",
    "print(mean_value)\n",
    "print(median_value)\n",
    "THRESHOLD = mean_value # how to determine the quality (choose one that results in best split)\n",
    "\n",
    "pos = sent_corpus.loc[sent_corpus[PROPERTY] > THRESHOLD]\n",
    "neg = sent_corpus.loc[sent_corpus[PROPERTY] < THRESHOLD]\n",
    "\n",
    "pos_1s = pos.loc[pos['is_euph']==1]\n",
    "pos_0s = pos.loc[pos['is_euph']==0]\n",
    "neg_1s = neg.loc[neg['is_euph']==1]\n",
    "neg_0s = neg.loc[neg['is_euph']==0]\n",
    "\n",
    "print(len(pos_1s))\n",
    "print(len(pos_0s))\n",
    "print(len(neg_1s))\n",
    "print(len(neg_0s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "342a9f52-1f16-4891-a783-4832225adf3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>neg_PET</th>\n",
       "      <th>neu_PET</th>\n",
       "      <th>pos_PET</th>\n",
       "      <th>...</th>\n",
       "      <th>neu_TEXT</th>\n",
       "      <th>pos_TEXT</th>\n",
       "      <th>noff_TEXT</th>\n",
       "      <th>off_TEXT</th>\n",
       "      <th>neg_SENT</th>\n",
       "      <th>neu_SENT</th>\n",
       "      <th>pos_SENT</th>\n",
       "      <th>noff_SENT</th>\n",
       "      <th>off_SENT</th>\n",
       "      <th>is_neu_PET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tinkle</td>\n",
       "      <td>We're just getting back what was TAKEN from us...</td>\n",
       "      <td>1</td>\n",
       "      <td>body functions/parts</td>\n",
       "      <td>tinkle</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>We're just getting back what was TAKEN from us...</td>\n",
       "      <td>0.215733</td>\n",
       "      <td>0.593470</td>\n",
       "      <td>0.190797</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.633027</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>0.039291</td>\n",
       "      <td>0.561716</td>\n",
       "      <td>0.438283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tinkle</td>\n",
       "      <td>I think AB390 will pass next year now that the...</td>\n",
       "      <td>1</td>\n",
       "      <td>body functions/parts</td>\n",
       "      <td>tinkle</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I think AB390 will pass next year now that the...</td>\n",
       "      <td>0.215733</td>\n",
       "      <td>0.593470</td>\n",
       "      <td>0.190797</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.513002</td>\n",
       "      <td>0.423893</td>\n",
       "      <td>0.063104</td>\n",
       "      <td>0.887314</td>\n",
       "      <td>0.112686</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>venereal disease</td>\n",
       "      <td>He's being sued by a woman who claims he gave ...</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>venereal disease</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>And Kris Humphries plans on being' relentless'...</td>\n",
       "      <td>0.717446</td>\n",
       "      <td>0.265028</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.191349</td>\n",
       "      <td>0.667033</td>\n",
       "      <td>0.141619</td>\n",
       "      <td>0.810757</td>\n",
       "      <td>0.189243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>venereal disease</td>\n",
       "      <td>The pair then allegedly went to Humphries' hot...</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>venereal disease</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>The court documents state the two' had sexual ...</td>\n",
       "      <td>0.717446</td>\n",
       "      <td>0.265028</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.681177</td>\n",
       "      <td>0.307028</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>0.556158</td>\n",
       "      <td>0.443842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>venereal disease</td>\n",
       "      <td>For hemorrhoids take tow and put salt on it an...</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>venereal disease</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>For a man who suffers from swelling and from v...</td>\n",
       "      <td>0.717446</td>\n",
       "      <td>0.265028</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.769499</td>\n",
       "      <td>0.216996</td>\n",
       "      <td>0.013505</td>\n",
       "      <td>0.632338</td>\n",
       "      <td>0.367662</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>sleep with</td>\n",
       "      <td>There were other photos she wanted me to see: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep with</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>There were other photos she wanted me to see B...</td>\n",
       "      <td>0.145681</td>\n",
       "      <td>0.730261</td>\n",
       "      <td>0.124058</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.017071</td>\n",
       "      <td>0.460846</td>\n",
       "      <td>0.522083</td>\n",
       "      <td>0.755889</td>\n",
       "      <td>0.244111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>sleep with</td>\n",
       "      <td>I am relieved to see two pup tents marked STAF...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep with</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Thank God I don't have to sleep with Ace Wands</td>\n",
       "      <td>0.145681</td>\n",
       "      <td>0.730261</td>\n",
       "      <td>0.124058</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.206293</td>\n",
       "      <td>0.771650</td>\n",
       "      <td>0.832420</td>\n",
       "      <td>0.167580</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>sleep around</td>\n",
       "      <td>Nothing serious, just long nights of me hackin...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep around</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>With all my caterwauling it's a wonder anyone ...</td>\n",
       "      <td>0.139588</td>\n",
       "      <td>0.747621</td>\n",
       "      <td>0.112791</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.725939</td>\n",
       "      <td>0.246083</td>\n",
       "      <td>0.027978</td>\n",
       "      <td>0.604972</td>\n",
       "      <td>0.395028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>with child</td>\n",
       "      <td>sounds more like Jonestown. They cant leave @ ...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>with child</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>They cant leave best advice I can give them is...</td>\n",
       "      <td>0.182822</td>\n",
       "      <td>0.721971</td>\n",
       "      <td>0.095207</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.876646</td>\n",
       "      <td>0.117473</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>0.706067</td>\n",
       "      <td>0.293933</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>with child</td>\n",
       "      <td>Nickname of a girl named Diana. 41. What you d...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>with child</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>What you do with Child Life 42</td>\n",
       "      <td>0.182822</td>\n",
       "      <td>0.721971</td>\n",
       "      <td>0.095207</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.112453</td>\n",
       "      <td>0.854126</td>\n",
       "      <td>0.033421</td>\n",
       "      <td>0.922798</td>\n",
       "      <td>0.077202</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1292 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               keyword                                        edited_text  \\\n",
       "0               tinkle  We're just getting back what was TAKEN from us...   \n",
       "1               tinkle  I think AB390 will pass next year now that the...   \n",
       "23    venereal disease  He's being sued by a woman who claims he gave ...   \n",
       "24    venereal disease  The pair then allegedly went to Humphries' hot...   \n",
       "25    venereal disease  For hemorrhoids take tow and put salt on it an...   \n",
       "...                ...                                                ...   \n",
       "1960        sleep with  There were other photos she wanted me to see: ...   \n",
       "1961        sleep with  I am relieved to see two pup tents marked STAF...   \n",
       "1962      sleep around  Nothing serious, just long nights of me hackin...   \n",
       "1963        with child  sounds more like Jonestown. They cant leave @ ...   \n",
       "1964        with child  Nickname of a girl named Diana. 41. What you d...   \n",
       "\n",
       "      is_euph                    category              type     euph_status  \\\n",
       "0           1        body functions/parts            tinkle     always_euph   \n",
       "1           1        body functions/parts            tinkle     always_euph   \n",
       "23          1             sexual activity  venereal disease     always_euph   \n",
       "24          1             sexual activity  venereal disease     always_euph   \n",
       "25          1             sexual activity  venereal disease     always_euph   \n",
       "...       ...                         ...               ...             ...   \n",
       "1960        0             sexual activity        sleep with  sometimes_euph   \n",
       "1961        0             sexual activity        sleep with  sometimes_euph   \n",
       "1962        0             sexual activity      sleep around  sometimes_euph   \n",
       "1963        0  physical/mental attributes        with child  sometimes_euph   \n",
       "1964        0  physical/mental attributes        with child  sometimes_euph   \n",
       "\n",
       "                                               sentence   neg_PET   neu_PET  \\\n",
       "0     We're just getting back what was TAKEN from us...  0.215733  0.593470   \n",
       "1     I think AB390 will pass next year now that the...  0.215733  0.593470   \n",
       "23    And Kris Humphries plans on being' relentless'...  0.717446  0.265028   \n",
       "24    The court documents state the two' had sexual ...  0.717446  0.265028   \n",
       "25    For a man who suffers from swelling and from v...  0.717446  0.265028   \n",
       "...                                                 ...       ...       ...   \n",
       "1960  There were other photos she wanted me to see B...  0.145681  0.730261   \n",
       "1961    Thank God I don't have to sleep with Ace Wands   0.145681  0.730261   \n",
       "1962  With all my caterwauling it's a wonder anyone ...  0.139588  0.747621   \n",
       "1963  They cant leave best advice I can give them is...  0.182822  0.721971   \n",
       "1964                    What you do with Child Life 42   0.182822  0.721971   \n",
       "\n",
       "       pos_PET  ...  neu_TEXT  pos_TEXT  noff_TEXT  off_TEXT  neg_SENT  \\\n",
       "0     0.190797  ...        -1        -1         -1        -1  0.633027   \n",
       "1     0.190797  ...        -1        -1         -1        -1  0.513002   \n",
       "23    0.017526  ...        -1        -1         -1        -1  0.191349   \n",
       "24    0.017526  ...        -1        -1         -1        -1  0.681177   \n",
       "25    0.017526  ...        -1        -1         -1        -1  0.769499   \n",
       "...        ...  ...       ...       ...        ...       ...       ...   \n",
       "1960  0.124058  ...        -1        -1         -1        -1  0.017071   \n",
       "1961  0.124058  ...        -1        -1         -1        -1  0.022057   \n",
       "1962  0.112791  ...        -1        -1         -1        -1  0.725939   \n",
       "1963  0.095207  ...        -1        -1         -1        -1  0.876646   \n",
       "1964  0.095207  ...        -1        -1         -1        -1  0.112453   \n",
       "\n",
       "      neu_SENT  pos_SENT  noff_SENT  off_SENT  is_neu_PET  \n",
       "0     0.327682  0.039291   0.561716  0.438283           1  \n",
       "1     0.423893  0.063104   0.887314  0.112686           1  \n",
       "23    0.667033  0.141619   0.810757  0.189243           0  \n",
       "24    0.307028  0.011795   0.556158  0.443842           0  \n",
       "25    0.216996  0.013505   0.632338  0.367662           0  \n",
       "...        ...       ...        ...       ...         ...  \n",
       "1960  0.460846  0.522083   0.755889  0.244111           1  \n",
       "1961  0.206293  0.771650   0.832420  0.167580           1  \n",
       "1962  0.246083  0.027978   0.604972  0.395028           1  \n",
       "1963  0.117473  0.005881   0.706067  0.293933           1  \n",
       "1964  0.854126  0.033421   0.922798  0.077202           1  \n",
       "\n",
       "[1292 rows x 23 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actually put in the label\n",
    "# sent_corpus = pd.read_csv('Sentiment_Corpus_v1.0.csv', index_col=0)\n",
    "\n",
    "sent_corpus['is_' + PROPERTY] = -1\n",
    "for i, row in sent_corpus.iterrows():\n",
    "    if (sent_corpus.loc[i, PROPERTY] >= THRESHOLD):\n",
    "        sent_corpus.loc[i, 'is_' + PROPERTY] = 1\n",
    "    else:\n",
    "        sent_corpus.loc[i, 'is_' + PROPERTY] = 0\n",
    "sent_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbd1c85f-ecd2-44f7-a8ea-9070ee9593e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for PET in sent_corpus['type'].unique():\n",
    "    PET_ex = sent_corpus.loc[sent_corpus['type']==PET].reset_index(drop=True)\n",
    "    # display(PET_ex)\n",
    "    # break\n",
    "    is_neu = PET_ex.loc[0, 'is_neu_PET']\n",
    "    d[PET] = is_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dda284b0-5a9b-4b5c-b763-77640df7282a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 51, 0: 35})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dfdb05d-5b8d-4bcf-9b66-15c9a84bef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_corpus.to_csv(\"Sentiment_Corpus_v1.2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e908c0b9-3ef6-439a-95c0-17ba6e8ffb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401\n",
      "1382\n",
      "\n",
      "368\n",
      "583\n"
     ]
    }
   ],
   "source": [
    "# ANalysis of Sentiment Corpus\n",
    "import pandas as pd\n",
    "\n",
    "sent_corpus = pd.read_csv(\"VET_Corpus_0.1.csv\", index_col=0)\n",
    "sent_corpus\n",
    "\n",
    "euph_examples = sent_corpus.loc[sent_corpus['is_euph'] == 1]\n",
    "noneuph_examples = sent_corpus.loc[sent_corpus['is_euph'] == 0]\n",
    "\n",
    "print(euph_examples['is_vague'].sum())\n",
    "print(len(euph_examples))\n",
    "# print(euph_examples['is_vagu'].mean())\n",
    "# print(euph_examples['off_PET'].mean())\n",
    "# print(euph_examples['off_TEXT'].mean())\n",
    "print()\n",
    "print(noneuph_examples['is_vague'].sum())\n",
    "print(len(noneuph_examples))\n",
    "# print(noneuph_examples['neg_TEXT'].mean())\n",
    "# print(noneuph_examples['off_PET'].mean())\n",
    "# print(noneuph_examples['off_TEXT'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e40db-9920-4e7c-ae58-71654a5ed2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "euphemismEnv",
   "language": "python",
   "name": "euphemismenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
