{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad62fb1-72ed-42fb-b3c3-f56a46a4a11c",
   "metadata": {},
   "source": [
    "# Experiment Processor\n",
    "- Given a dataset labelled for positive (1) and negative (0) presence of some pragmatic feature, create euphemism-detection train/test splits, and process the results.\n",
    "- Specify parameters in the first chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f15e46a-6af0-4ed4-93d9-48b816511c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper notebook to create a number of train-test splits from the data\n",
    "# DATASET = \"VET_Corpus_v0.2.csv\" # the dataset containing pragmatic labels\n",
    "# NUM_TESTS = 10 # number of splits\n",
    "# FOLDER = \"TEST_4.0.3\" # directory where train/test files should be output\n",
    "# PROPERTY = 'is_vague' # linguistic property being investigated\n",
    "# PREPROCESS = True\n",
    "# NOTES = 'Preprocessing, including the removal of <> (the subject of study). Strong assumption vagueness labels, includes all examples.'\n",
    "DATASET = \"Sentiment_Corpus_v1.2.csv\" # the dataset containing pragmatic labels\n",
    "NUM_TESTS = 10 # number of splits\n",
    "FOLDER = \"TEST_5.1\" # directory where train/test files should be output\n",
    "PROPERTY = 'is_neu_PET' # linguistic property being investigated\n",
    "PREPROCESS = False\n",
    "NOTES = 'High-agreement PETs only. Neutral label using mean neutral sentiment strength as the threshold. From Euphemism_Corpus_v2.1.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70838d21-9c61-46ca-be6e-1e45e5988f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function, HuggingFace-ify, takes a sample of euphemism corpus and it makes into an appropriate format for the HuggingFace Trainer class\n",
    "# NEW: also preprocesses it, if specified in the first chunk\n",
    "import re\n",
    "def hfify(df):\n",
    "    # df = df.drop(['keyword', 'category', 'type', 'euph_status', 'sentence', PROPERTY], axis=1)\n",
    "    df = df[['edited_text', 'is_euph']]\n",
    "    if (PREPROCESS == False):\n",
    "        df = df.rename(columns={'edited_text':'text', 'is_euph':'label'})\n",
    "    elif (PREPROCESS == True):\n",
    "        df['preprocessed_text'] = \"\"\n",
    "        for i, row in df.iterrows():\n",
    "            text = df.loc[i, 'edited_text']\n",
    "            text = re.sub(r\"[^a-zA-Z<>]\", \" \", text.lower())\n",
    "            text = re.sub(r\"[^a-zA-Z]\", \" \", text.lower()) # we're removing brackets...\n",
    "            df.loc[i, 'preprocessed_text'] = \" \".join(text.split()) \n",
    "        df = df[['preprocessed_text','is_euph']]\n",
    "        df = df.rename(columns={'preprocessed_text':'text', 'is_euph':'label'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8bc07-ea62-46be-94b8-e3438092ad1a",
   "metadata": {},
   "source": [
    "## Produce multiple training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14b8f150-aa46-440d-9d54-a54574facc56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# POS 1s: 402\n",
      "# POS 0s: 203\n",
      "# NEG 1s: 442\n",
      "# NEG 0s: 245\n",
      "The smallest group is 203 examples. Training set will have 324 of each label; test set will have 82 of each label\n",
      "The training set will have 648 total examples; the test set, 164, for a total of 812 examples\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ok? yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests successfully output to TEST_5.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# makes train-test splits BALANCED for a particular linguistic property\n",
    "\n",
    "corpus = pd.read_csv(DATASET, index_col=0)\n",
    "\n",
    "# excluding certain terms for balance\n",
    "# banned_list = ['aging', 'deprived', 'gluteus maximus', 'got clean', 'let [pro] go', 'oldest profession', 'plump', 'sober', 'wealthy', 'with child']\n",
    "# for term in banned_list:\n",
    "#     banned_rows = corpus.loc[corpus['type'] == term]\n",
    "#     # print(banned_rows.index)\n",
    "#     corpus = corpus.drop(banned_rows.index)\n",
    "    \n",
    "# this chunk is for limiting data to high-agreement PETs\n",
    "# high_agreement_PETs = ['slim', 'between jobs', 'accident', 'late', 'number one', 'sleep with', 'seasoned', 'wealthy', 'over the hill', 'plump', 'let go of', 'go all the way', 'overweight', 'sober', 'number two', 'slept with', 'dismissed', 'let them go', 'aging', 'expecting', 'stout', 'troubled', 'with child', 'invalid', 'experienced', 'getting clean', 'custodian', 'got clean', 'long sleep', 'mixed up', 'chest', 'same-sex', 'economical', 'passing on', 'neutralize', 'outspoken', 'gluteus maximus', 'sleep around', 'pass on', 'disabled', 'special needs', 'pass away', 'a certain age', 'well off', 'less fortunate', 'mistruths', 'droppings', 'lose your lunch', 'pregnancy termination', 'let him go', 'golden years', 'mentally challenged', 'tinkle', 'demise', 'drinking problem', 'indigent', 'detainee', 'advanced age', 'comfort women', 'time of the month', 'pass gas', 'portly', 'went to heaven', 'venereal disease', 'put to sleep', 'mistruth', 'differently-abled', 'intoxicated', 'economical with the truth', 'lavatory', 'birds and the bees', 'deceased', 'terminating a pregnancy', 'inebriated', 'inner city', 'regime change', 'enhanced interrogation techniques', 'adult beverages', 'to go to heaven', 'dearly departed', 'passed away', 'downsize', 'ethnic cleansing', 'substance abusers', 'broken home', 'made love', 'plus-sized', 'underprivileged', 'rear end', 'armed conflict', 'substance abuse', 'disadvantaged', 'neutralized', 'capital punishment', 'street person', 'making love', 'freedom fighters']\n",
    "# corpus = corpus.loc[corpus['keyword'].isin(high_agreement_PETs)]\n",
    "\n",
    "# this is for limiting to parallel-examples only\n",
    "# corpus = corpus.loc[corpus['euph_status']=='somestimes_euph']\n",
    "\n",
    "pos_examples = corpus.loc[corpus[PROPERTY]==1] # note, \"pos\" and \"neg\" stand for \"positive\" and \"negative\" for the target property\n",
    "neg_examples = corpus.loc[corpus[PROPERTY]==0]\n",
    "pos_1s = pos_examples.loc[pos_examples['is_euph']==1]\n",
    "pos_0s = pos_examples.loc[pos_examples['is_euph']==0]\n",
    "neg_1s = neg_examples.loc[neg_examples['is_euph']==1]\n",
    "neg_0s = neg_examples.loc[neg_examples['is_euph']==0]\n",
    "\n",
    "print(\"# POS 1s:\", len(pos_1s))\n",
    "print(\"# POS 0s:\", len(pos_0s))\n",
    "print(\"# NEG 1s:\", len(neg_1s))\n",
    "print(\"# NEG 0s:\", len(neg_0s))\n",
    "\n",
    "# choosing sizes to balance number of 0s an 1s in experiments\n",
    "constraint = min(len(pos_1s),len(pos_0s),len(neg_1s),len(neg_0s)) # constrained by smallest number of pos/neg 0s/1s\n",
    "train_size = round(constraint*0.8) - round(constraint*0.8)%2 # make it an even number for easier split between pos/neg later; this is the size of each P/N-0/1 sample\n",
    "test_size = constraint - train_size # how much test-set samples to take from each P/N-0/1 sample\n",
    "# information\n",
    "s1 = \"The smallest group is {} examples. Training set will have {} of each label; test set will have {} of each label\".format(constraint, train_size*2, test_size*2)\n",
    "s2 = \"The training set will have {} total examples; the test set, {}, for a total of {} examples\".format(train_size*4, test_size*4, train_size*4+test_size*4)\n",
    "print(s1)\n",
    "print(s2)\n",
    "if ('no' in input(\"Ok?\")):\n",
    "    raise KeyboardInterrupt\n",
    "    \n",
    "os.mkdir(FOLDER)\n",
    "f = open(FOLDER + \"/info.txt\", \"a\")\n",
    "f.write(s1 + \"\\n\" + s2 + \"\\n\\nCUSTOM NOTES: \" + NOTES)\n",
    "f.close()\n",
    "\n",
    "for x in range(0, NUM_TESTS):\n",
    "    # create the training sample\n",
    "    pos_1s_train_sample = pos_1s.sample(train_size).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "    pos_0s_train_sample = pos_0s.sample(train_size).sample(frac=1)\n",
    "    neg_1s_train_sample = neg_1s.sample(train_size).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "    neg_0s_train_sample = neg_0s.sample(train_size).sample(frac=1)\n",
    "    # remove train selections from data\n",
    "    remaining_pos_1s = pos_1s.drop(pos_1s_train_sample.index)\n",
    "    remaining_pos_0s = pos_0s.drop(pos_0s_train_sample.index)\n",
    "    remaining_neg_1s = neg_1s.drop(neg_1s_train_sample.index)\n",
    "    remaining_neg_0s = neg_0s.drop(neg_0s_train_sample.index)\n",
    "    # randomly select test samples from remaining data\n",
    "    pos_1s_test_sample = remaining_pos_1s.sample(test_size).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "    pos_0s_test_sample = remaining_pos_0s.sample(test_size).sample(frac=1)\n",
    "    neg_1s_test_sample = remaining_neg_1s.sample(test_size).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "    neg_0s_test_sample = remaining_neg_0s.sample(test_size).sample(frac=1)   \n",
    "    \n",
    "    train_set = pd.concat([pos_1s_train_sample, pos_0s_train_sample, neg_1s_train_sample, neg_0s_train_sample]).sample(frac=1)\n",
    "    test_set = pd.concat([pos_1s_test_sample, pos_0s_test_sample, neg_1s_test_sample, neg_0s_test_sample]).sample(frac=1)\n",
    "    \n",
    "    # save them to folder\n",
    "    train_set.to_csv(FOLDER + '/train_' + str(x) + '.csv')\n",
    "    train_set = hfify(train_set)\n",
    "    train_set.to_csv(FOLDER + '/hf_train_' + str(x) + '.csv', index=False)\n",
    "    test_set.to_csv(FOLDER + '/test_' + str(x) + '.csv')\n",
    "    test_set = hfify(test_set)\n",
    "    test_set.to_csv(FOLDER + '/hf_test_' + str(x) + '.csv', index=False)\n",
    "    # print(len(pos_1s_train_sample))\n",
    "    # print(len(pos_0s_train_sample))\n",
    "    # print(len(neg_1s_train_sample))\n",
    "    # print(len(neg_0s_train_sample))\n",
    "    # print(len(pos_1s_test_sample))\n",
    "    # print(len(pos_0s_test_sample))\n",
    "    # print(len(neg_1s_test_sample))\n",
    "    # print(len(neg_0s_test_sample))\n",
    "    # break\n",
    "print(\"Tests successfully output to \" + FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ccb5f07-5dd3-4a21-a381-346c197e8e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 0\n",
      "NUM AMBIGUOUS PETS IN VAGUE: 13\n",
      "NUM AMBIGUOUS EXAMPLES IN VAGUE: 150\n",
      "NUM AMBIGUOUS PETS IN UNVAGUE: 16\n",
      "NUM AMBIGUOUS EXAMPLES IN UNVAGUE: 187\n",
      "\n",
      "TEST 1\n",
      "NUM AMBIGUOUS PETS IN VAGUE: 14\n",
      "NUM AMBIGUOUS EXAMPLES IN VAGUE: 151\n",
      "NUM AMBIGUOUS PETS IN UNVAGUE: 14\n",
      "NUM AMBIGUOUS EXAMPLES IN UNVAGUE: 168\n",
      "\n",
      "TEST 2\n",
      "NUM AMBIGUOUS PETS IN VAGUE: 14\n",
      "NUM AMBIGUOUS EXAMPLES IN VAGUE: 153\n",
      "NUM AMBIGUOUS PETS IN UNVAGUE: 13\n",
      "NUM AMBIGUOUS EXAMPLES IN UNVAGUE: 169\n",
      "\n",
      "TEST 3\n",
      "NUM AMBIGUOUS PETS IN VAGUE: 12\n",
      "NUM AMBIGUOUS EXAMPLES IN VAGUE: 140\n",
      "NUM AMBIGUOUS PETS IN UNVAGUE: 16\n",
      "NUM AMBIGUOUS EXAMPLES IN UNVAGUE: 186\n",
      "\n",
      "TEST 4\n",
      "NUM AMBIGUOUS PETS IN VAGUE: 13\n",
      "NUM AMBIGUOUS EXAMPLES IN VAGUE: 138\n",
      "NUM AMBIGUOUS PETS IN UNVAGUE: 10\n",
      "NUM AMBIGUOUS EXAMPLES IN UNVAGUE: 157\n",
      "\n",
      "TEST 5\n",
      "NUM AMBIGUOUS PETS IN VAGUE: 14\n",
      "NUM AMBIGUOUS EXAMPLES IN VAGUE: 154\n",
      "NUM AMBIGUOUS PETS IN UNVAGUE: 13\n",
      "NUM AMBIGUOUS EXAMPLES IN UNVAGUE: 173\n",
      "\n",
      "TEST 6\n",
      "NUM AMBIGUOUS PETS IN VAGUE: 14\n",
      "NUM AMBIGUOUS EXAMPLES IN VAGUE: 152\n",
      "NUM AMBIGUOUS PETS IN UNVAGUE: 15\n",
      "NUM AMBIGUOUS EXAMPLES IN UNVAGUE: 181\n",
      "\n",
      "TEST 7\n",
      "NUM AMBIGUOUS PETS IN VAGUE: 13\n",
      "NUM AMBIGUOUS EXAMPLES IN VAGUE: 148\n",
      "NUM AMBIGUOUS PETS IN UNVAGUE: 13\n",
      "NUM AMBIGUOUS EXAMPLES IN UNVAGUE: 180\n",
      "\n",
      "TEST 8\n",
      "NUM AMBIGUOUS PETS IN VAGUE: 12\n",
      "NUM AMBIGUOUS EXAMPLES IN VAGUE: 149\n",
      "NUM AMBIGUOUS PETS IN UNVAGUE: 15\n",
      "NUM AMBIGUOUS EXAMPLES IN UNVAGUE: 179\n",
      "\n",
      "TEST 9\n",
      "NUM AMBIGUOUS PETS IN VAGUE: 12\n",
      "NUM AMBIGUOUS EXAMPLES IN VAGUE: 140\n",
      "NUM AMBIGUOUS PETS IN UNVAGUE: 12\n",
      "NUM AMBIGUOUS EXAMPLES IN UNVAGUE: 170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# just verifying sizes here...\n",
    "import os \n",
    "FOLDER = 'TEST_4.0.2'\n",
    "PROPERTY = 'is_vague'\n",
    "\n",
    "# FOLDER = 'TEST_5.0'\n",
    "# PROPERTY = 'is_neu_PET'\n",
    "# curr_intersection = set()\n",
    "# for file in os.listdir(FOLDER):\n",
    "#     if (file[0:5] != 'train'):\n",
    "#         continue\n",
    "#     fn = FOLDER + '/' + file\n",
    "#     data = pd.read_csv(fn, index_col=0)\n",
    "#     data_indices = data.index\n",
    "#     if (len(curr_intersection) == 0):\n",
    "#         curr_intersection = data_indices\n",
    "#     else:\n",
    "#         curr_intersection = curr_intersection.intersection(data_indices)\n",
    "        \n",
    "# print(curr_intersection)\n",
    "# print(len(curr_intersection))\n",
    "\n",
    "for n in range(0, 10):\n",
    "    print(\"TEST \" + str(n))\n",
    "    data = pd.read_csv(FOLDER + '/train_' + str(n) + '.csv', index_col=0)\n",
    "    # data2 = pd.read_csv(FOLDER + '/train_3.csv', index_col=0)\n",
    "    # data1 = set(data1.index)\n",
    "    # data2 = set(data2.index)\n",
    "    # intersection = data1.intersection(data2)\n",
    "    # print(len(intersection))\n",
    "    euphs = data.loc[data['is_euph'] == 1]\n",
    "    noneuphs = data.loc[data['is_euph'] == 0]\n",
    "    vague = data.loc[data[PROPERTY] == 1]\n",
    "    nonvague = data.loc[data[PROPERTY] == 0]\n",
    "\n",
    "    vague_euphs = euphs.loc[euphs[PROPERTY] == 1]\n",
    "    nonvague_euphs = euphs.loc[euphs[PROPERTY] == 0]\n",
    "    vague_noneuphs = noneuphs.loc[noneuphs[PROPERTY] == 1]\n",
    "    nonvague_noneuphs = noneuphs.loc[noneuphs[PROPERTY] == 0]\n",
    "    # print(file)\n",
    "    # print(len(vague_euphs))\n",
    "    # print(len(nonvague_euphs))\n",
    "    # print(len(vague_noneuphs))\n",
    "    # print(len(nonvague_noneuphs))\n",
    "    # print()\n",
    "    # print(len(data['type'].unique()))\n",
    "    # print(len(vague['type'].unique()))\n",
    "    # print(len(nonvague['type'].unique()))\n",
    "\n",
    "    num_ambiguous_PETs = 0\n",
    "    num_ambiguous_examples = 0\n",
    "    for PET in vague['type'].unique():\n",
    "        sel = vague.loc[vague['type'] == PET]\n",
    "        if (0 in sel['is_euph'].tolist() and 1 in sel['is_euph'].tolist()):\n",
    "            num_ambiguous_PETs += 1\n",
    "            num_ambiguous_examples += len(sel['is_euph'].tolist())\n",
    "    print(\"NUM AMBIGUOUS PETS IN VAGUE:\", num_ambiguous_PETs)\n",
    "    print(\"NUM AMBIGUOUS EXAMPLES IN VAGUE:\", num_ambiguous_examples)\n",
    "\n",
    "    num_ambiguous_PETs = 0\n",
    "    num_ambiguous_examples = 0\n",
    "\n",
    "    for PET in nonvague['type'].unique():\n",
    "        sel = nonvague.loc[nonvague['type'] == PET]\n",
    "        if (0 in sel['is_euph'].tolist() and 1 in sel['is_euph'].tolist()):\n",
    "            num_ambiguous_PETs += 1\n",
    "            num_ambiguous_examples += len(sel['is_euph'].tolist())\n",
    "    print(\"NUM AMBIGUOUS PETS IN UNVAGUE:\", num_ambiguous_PETs)\n",
    "    print(\"NUM AMBIGUOUS EXAMPLES IN UNVAGUE:\", num_ambiguous_examples)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8a925-d475-48f6-992f-c7e03e2adf20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Produce multiple, CUSTOM-SIZED splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eed5e6bc-129f-4419-81b7-b506e8806c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# POS 1s: 401\n",
      "# POS 0s: 368\n",
      "# NEG 1s: 981\n",
      "# NEG 0s: 215\n",
      "TRAIN: There will be 320 positive 1s, 320 negative 1s, 160 positive 0s, and 160 negative 0s.\n",
      "TEST: There will be 80 positive 1s, 80 negative 1s, 40 positive 0s, and 40 negative 0s.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ok? yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests successfully output to TEST_4.2.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# specify numbers here\n",
    "tot_num_pos_1s, tot_num_neg_1s = 400, 400 # 'pos' and 'neg' refer to the property of interest; e.g., 'is_vague'\n",
    "tot_num_pos_0s, tot_num_neg_0s = 200, 200 \n",
    "\n",
    "# makes train-test splits BALANCED for a particular linguistic property\n",
    "\n",
    "corpus = pd.read_csv(DATASET, index_col=0)\n",
    "\n",
    "# this chunk is for limiting data to high-agreement PETs\n",
    "# high_agreement_PETs = ['slim', 'between jobs', 'accident', 'late', 'number one', 'sleep with', 'seasoned', 'wealthy', 'over the hill', 'plump', 'let go of', 'go all the way', 'overweight', 'sober', 'number two', 'slept with', 'dismissed', 'let them go', 'aging', 'expecting', 'stout', 'troubled', 'with child', 'invalid', 'experienced', 'getting clean', 'custodian', 'got clean', 'long sleep', 'mixed up', 'chest', 'same-sex', 'economical', 'passing on', 'neutralize', 'outspoken', 'gluteus maximus', 'sleep around', 'pass on', 'disabled', 'special needs', 'pass away', 'a certain age', 'well off', 'less fortunate', 'mistruths', 'droppings', 'lose your lunch', 'pregnancy termination', 'let him go', 'golden years', 'mentally challenged', 'tinkle', 'demise', 'drinking problem', 'indigent', 'detainee', 'advanced age', 'comfort women', 'time of the month', 'pass gas', 'portly', 'went to heaven', 'venereal disease', 'put to sleep', 'mistruth', 'differently-abled', 'intoxicated', 'economical with the truth', 'lavatory', 'birds and the bees', 'deceased', 'terminating a pregnancy', 'inebriated', 'inner city', 'regime change', 'enhanced interrogation techniques', 'adult beverages', 'to go to heaven', 'dearly departed', 'passed away', 'downsize', 'ethnic cleansing', 'substance abusers', 'broken home', 'made love', 'plus-sized', 'underprivileged', 'rear end', 'armed conflict', 'substance abuse', 'disadvantaged', 'neutralized', 'capital punishment', 'street person', 'making love', 'freedom fighters']\n",
    "# corpus = corpus.loc[corpus['keyword'].isin(high_agreement_PETs)]\n",
    "\n",
    "# this is for limiting to parallel-examples only\n",
    "# corpus = corpus.loc[corpus['euph_status']=='somestimes_euph']\n",
    "\n",
    "pos_examples = corpus.loc[corpus[PROPERTY]==1] # note, \"pos\" and \"neg\" stand for \"positive\" and \"negative\" for the target property\n",
    "neg_examples = corpus.loc[corpus[PROPERTY]==0]\n",
    "pos_1s = pos_examples.loc[pos_examples['is_euph']==1]\n",
    "pos_0s = pos_examples.loc[pos_examples['is_euph']==0]\n",
    "neg_1s = neg_examples.loc[neg_examples['is_euph']==1]\n",
    "neg_0s = neg_examples.loc[neg_examples['is_euph']==0]\n",
    "\n",
    "print(\"# POS 1s:\", len(pos_1s))\n",
    "print(\"# POS 0s:\", len(pos_0s))\n",
    "print(\"# NEG 1s:\", len(neg_1s))\n",
    "print(\"# NEG 0s:\", len(neg_0s))\n",
    "\n",
    "# determine train and test sizes\n",
    "train_pos_1s, train_neg_1s = round(tot_num_pos_1s*0.8), round(tot_num_neg_1s*0.8)\n",
    "train_pos_0s, train_neg_0s = round(tot_num_pos_0s*0.8), round(tot_num_neg_0s*0.8)\n",
    "test_pos_1s, test_neg_1s = tot_num_pos_1s-train_pos_1s, tot_num_neg_1s-train_neg_1s\n",
    "test_pos_0s, test_neg_0s = tot_num_pos_0s-train_pos_0s, tot_num_neg_0s-train_neg_0s\n",
    "\n",
    "# information\n",
    "s1 = \"TRAIN: There will be {} positive 1s, {} negative 1s, {} positive 0s, and {} negative 0s.\".format(train_pos_1s, train_neg_1s, train_pos_0s, train_neg_0s)\n",
    "s2 = \"TEST: There will be {} positive 1s, {} negative 1s, {} positive 0s, and {} negative 0s.\".format(test_pos_1s, test_neg_1s, test_pos_0s, test_neg_0s)\n",
    "print(s1)\n",
    "print(s2)\n",
    "if ('no' in input(\"Ok?\")):\n",
    "    raise KeyboardInterrupt\n",
    "    \n",
    "os.mkdir(FOLDER)\n",
    "f = open(FOLDER + \"/info.txt\", \"a\")\n",
    "f.write(s1 + \"\\n\" + s2 + \"\\n\\nCUSTOM NOTES: \" + NOTES)\n",
    "f.close()\n",
    "\n",
    "for x in range(0, NUM_TESTS):\n",
    "    # create the training sample\n",
    "    pos_1s_train_sample = pos_1s.sample(train_pos_1s).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "    pos_0s_train_sample = pos_0s.sample(train_pos_0s).sample(frac=1)\n",
    "    neg_1s_train_sample = neg_1s.sample(train_neg_1s).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "    neg_0s_train_sample = neg_0s.sample(train_neg_0s).sample(frac=1)\n",
    "    # remove train selections from data\n",
    "    remaining_pos_1s = pos_1s.drop(pos_1s_train_sample.index)\n",
    "    remaining_pos_0s = pos_0s.drop(pos_0s_train_sample.index)\n",
    "    remaining_neg_1s = neg_1s.drop(neg_1s_train_sample.index)\n",
    "    remaining_neg_0s = neg_0s.drop(neg_0s_train_sample.index)\n",
    "    # randomly select test samples from remaining data\n",
    "    pos_1s_test_sample = remaining_pos_1s.sample(test_pos_1s).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "    pos_0s_test_sample = remaining_pos_0s.sample(test_pos_0s).sample(frac=1)\n",
    "    neg_1s_test_sample = remaining_neg_1s.sample(test_neg_1s).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "    neg_0s_test_sample = remaining_neg_0s.sample(test_neg_0s).sample(frac=1)   \n",
    "    \n",
    "    train_set = pd.concat([pos_1s_train_sample, pos_0s_train_sample, neg_1s_train_sample, neg_0s_train_sample]).sample(frac=1)\n",
    "    test_set = pd.concat([pos_1s_test_sample, pos_0s_test_sample, neg_1s_test_sample, neg_0s_test_sample]).sample(frac=1)\n",
    "    \n",
    "    # save them to folder\n",
    "    train_set.to_csv(FOLDER + '/train_' + str(x) + '.csv')\n",
    "    train_set = hfify(train_set)\n",
    "    train_set.to_csv(FOLDER + '/hf_train_' + str(x) + '.csv', index=False)\n",
    "    test_set.to_csv(FOLDER + '/test_' + str(x) + '.csv')\n",
    "    test_set = hfify(test_set)\n",
    "    test_set.to_csv(FOLDER + '/hf_test_' + str(x) + '.csv', index=False)\n",
    "    # print(len(pos_1s_train_sample))\n",
    "    # print(len(pos_0s_train_sample))\n",
    "    # print(len(neg_1s_train_sample))\n",
    "    # print(len(neg_0s_train_sample))\n",
    "    # print(len(pos_1s_test_sample))\n",
    "    # print(len(pos_0s_test_sample))\n",
    "    # print(len(neg_1s_test_sample))\n",
    "    # print(len(neg_0s_test_sample))\n",
    "    # break\n",
    "print(\"Tests successfully output to \" + FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253858f1-93eb-490a-b022-bacc861b72cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Temporary - make customized splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4e9156-a721-427f-a3d6-0223eba9c836",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# POS 1s: 401\n",
      "# POS 0s: 368\n",
      "# NEG 1s: 981\n",
      "# NEG 0s: 215\n",
      "The smallest group is 215 examples. Training set will have 344 of each label; test set will have 86 of each label\n",
      "The training set will have 688 total examples; the test set, 172, for a total of 860 examples\n",
      "Tests successfully output to TEST_2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# makes train-test splits, restricting train/test to a particular linguistic property\n",
    "\n",
    "corpus = pd.read_csv(DATASET, index_col=0)\n",
    "# this chunk is for limiting data to high-agreement PETs\n",
    "# high_agreement_PETs = ['slim', 'between jobs', 'accident', 'late', 'number one', 'sleep with', 'seasoned', 'wealthy', 'over the hill', 'plump', 'let go of', 'go all the way', 'overweight', 'sober', 'number two', 'slept with', 'dismissed', 'let them go', 'aging', 'expecting', 'stout', 'troubled', 'with child', 'invalid', 'experienced', 'getting clean', 'custodian', 'got clean', 'long sleep', 'mixed up', 'chest', 'same-sex', 'economical', 'passing on', 'neutralize', 'outspoken', 'gluteus maximus', 'sleep around', 'pass on', 'disabled', 'special needs', 'pass away', 'a certain age', 'well off', 'less fortunate', 'mistruths', 'droppings', 'lose your lunch', 'pregnancy termination', 'let him go', 'golden years', 'mentally challenged', 'tinkle', 'demise', 'drinking problem', 'indigent', 'detainee', 'advanced age', 'comfort women', 'time of the month', 'pass gas', 'portly', 'went to heaven', 'venereal disease', 'put to sleep', 'mistruth', 'differently-abled', 'intoxicated', 'economical with the truth', 'lavatory', 'birds and the bees', 'deceased', 'terminating a pregnancy', 'inebriated', 'inner city', 'regime change', 'enhanced interrogation techniques', 'adult beverages', 'to go to heaven', 'dearly departed', 'passed away', 'downsize', 'ethnic cleansing', 'substance abusers', 'broken home', 'made love', 'plus-sized', 'underprivileged', 'rear end', 'armed conflict', 'substance abuse', 'disadvantaged', 'neutralized', 'capital punishment', 'street person', 'making love', 'freedom fighters']\n",
    "# corpus = corpus.loc[corpus['keyword'].isin(high_agreement_PETs)]\n",
    "\n",
    "pos_examples = corpus.loc[corpus[PROPERTY]==1] # note, \"pos\" and \"neg\" stand for \"positive\" and \"negative\" for the target property\n",
    "neg_examples = corpus.loc[corpus[PROPERTY]==0]\n",
    "pos_1s = pos_examples.loc[pos_examples['is_euph']==1]\n",
    "pos_0s = pos_examples.loc[pos_examples['is_euph']==0]\n",
    "neg_1s = neg_examples.loc[neg_examples['is_euph']==1]\n",
    "neg_0s = neg_examples.loc[neg_examples['is_euph']==0]\n",
    "\n",
    "print(\"# POS 1s:\", len(pos_1s))\n",
    "print(\"# POS 0s:\", len(pos_0s))\n",
    "print(\"# NEG 1s:\", len(neg_1s))\n",
    "print(\"# NEG 0s:\", len(neg_0s))\n",
    "\n",
    "# choosing sizes to balance number of 0s an 1s in experiments\n",
    "constraint = min(len(pos_1s),len(pos_0s),len(neg_1s),len(neg_0s)) # constrained by smallest number of pos/neg 0s/1s\n",
    "train_size = round(constraint*0.8) - round(constraint*0.8)%2 # make it an even number for easier split between pos/neg later; this is the size of each P/N-0/1 sample\n",
    "test_size = constraint - train_size # how much test-set samples to take from each P/N-0/1 sample\n",
    "# information\n",
    "s1 = \"The smallest group is {} examples. Training set will have {} of each label; test set will have {} of each label\".format(constraint, train_size*2, test_size*2)\n",
    "s2 = \"The training set will have {} total examples; the test set, {}, for a total of {} examples\".format(train_size*4, test_size*4, train_size*4+test_size*4)\n",
    "print(s1)\n",
    "print(s2)\n",
    "# make directories\n",
    "os.mkdir(FOLDER)\n",
    "POS_TRAIN_FOLDER = FOLDER + '/pos_train'\n",
    "NEG_TRAIN_FOLDER = FOLDER + '/neg_train'\n",
    "POS_TEST_FOLDER = FOLDER + '/pos_test'\n",
    "NEG_TEST_FOLDER = FOLDER + '/neg_test'\n",
    "MIXED_TEST_FOLDER = FOLDER + '/mixed_test'\n",
    "os.mkdir(POS_TRAIN_FOLDER)\n",
    "os.mkdir(NEG_TRAIN_FOLDER)\n",
    "os.mkdir(POS_TEST_FOLDER)\n",
    "os.mkdir(NEG_TEST_FOLDER)\n",
    "os.mkdir(MIXED_TEST_FOLDER)\n",
    "\n",
    "# info\n",
    "f = open(FOLDER + \"/info.txt\", \"a\")\n",
    "f.write(s1 + \"\\n\" + s2 + \"\\n\\nCUSTOM NOTES: \" + NOTES)\n",
    "f.close()\n",
    "\n",
    "for x in range(0, NUM_TESTS):\n",
    "    # create the training sample\n",
    "    pos_1s_train_sample = pos_1s.sample(train_size).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "    pos_0s_train_sample = pos_0s.sample(train_size).sample(frac=1)\n",
    "    neg_1s_train_sample = neg_1s.sample(train_size).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "    neg_0s_train_sample = neg_0s.sample(train_size).sample(frac=1)\n",
    "    # remove train selections from data\n",
    "    remaining_pos_1s = pos_1s.drop(pos_1s_train_sample.index)\n",
    "    remaining_pos_0s = pos_0s.drop(pos_0s_train_sample.index)\n",
    "    remaining_neg_1s = neg_1s.drop(neg_1s_train_sample.index)\n",
    "    remaining_neg_0s = neg_0s.drop(neg_0s_train_sample.index)\n",
    "    # randomly select test samples from remaining data\n",
    "    pos_1s_test_sample = remaining_pos_1s.sample(test_size).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "    pos_0s_test_sample = remaining_pos_0s.sample(test_size).sample(frac=1)\n",
    "    neg_1s_test_sample = remaining_neg_1s.sample(test_size).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "    neg_0s_test_sample = remaining_neg_0s.sample(test_size).sample(frac=1)   \n",
    "    \n",
    "    # make restricted training sets\n",
    "    pos_only_train_set = pd.concat([pos_1s_train_sample, pos_0s_train_sample]).sample(frac=1)\n",
    "    pos_only_train_set.to_csv(POS_TRAIN_FOLDER + '/train_' + str(x) + '.csv')\n",
    "    pos_only_train_set = hfify(pos_only_train_set)\n",
    "    pos_only_train_set.to_csv(POS_TRAIN_FOLDER + '/hf_train_' + str(x) + '.csv', index=False)\n",
    "    \n",
    "    neg_only_train_set = pd.concat([neg_1s_train_sample, neg_0s_train_sample]).sample(frac=1)\n",
    "    neg_only_train_set.to_csv(NEG_TRAIN_FOLDER + '/train_' + str(x) + '.csv')\n",
    "    neg_only_train_set = hfify(neg_only_train_set)\n",
    "    neg_only_train_set.to_csv(NEG_TRAIN_FOLDER + '/hf_train_' + str(x) + '.csv', index=False)\n",
    "    \n",
    "    # make resricted test sets\n",
    "    pos_only_test_set = pd.concat([pos_1s_test_sample, pos_0s_test_sample]).sample(frac=1)\n",
    "    pos_only_test_set.to_csv(POS_TEST_FOLDER + '/test_' + str(x) + '.csv')\n",
    "    hf_pos_only_test_set = hfify(pos_only_test_set)\n",
    "    hf_pos_only_test_set.to_csv(POS_TEST_FOLDER + '/hf_test_' + str(x) + '.csv', index=False)\n",
    "    \n",
    "    neg_only_test_set = pd.concat([neg_1s_test_sample, neg_0s_test_sample]).sample(frac=1)\n",
    "    neg_only_test_set.to_csv(NEG_TEST_FOLDER + '/test_' + str(x) + '.csv')\n",
    "    hf_neg_only_test_set = hfify(neg_only_test_set)\n",
    "    hf_neg_only_test_set.to_csv(NEG_TEST_FOLDER + '/hf_test_' + str(x) + '.csv', index=False)\n",
    "    \n",
    "    mixed_test_set = pd.concat([pos_only_test_set.sample(frac=0.5), neg_only_test_set.sample(frac=0.5)]).sample(frac=1)\n",
    "    mixed_test_set.to_csv(MIXED_TEST_FOLDER + '/test_' + str(x) + '.csv')\n",
    "    mixed_test_set = hfify(mixed_test_set)\n",
    "    mixed_test_set.to_csv(MIXED_TEST_FOLDER + '/hf_test_' + str(x) + '.csv', index=False)\n",
    "    \n",
    "#     train_set = pd.concat([pos_1s_train_sample, pos_0s_train_sample, neg_1s_train_sample, neg_0s_train_sample]).sample(frac=1)\n",
    "#     test_set = pd.concat([pos_1s_test_sample, pos_0s_test_sample, neg_1s_test_sample, neg_0s_test_sample]).sample(frac=1)\n",
    "    \n",
    "#     # save them to folder\n",
    "#     train_set.to_csv(FOLDER + '/train_' + str(x) + '.csv')\n",
    "#     train_set = hfify(train_set)\n",
    "#     train_set.to_csv(FOLDER + '/hf_train_' + str(x) + '.csv', index=False)\n",
    "#     test_set.to_csv(FOLDER + '/test_' + str(x) + '.csv')\n",
    "#     test_set = hfify(test_set)\n",
    "#     test_set.to_csv(FOLDER + '/hf_test_' + str(x) + '.csv', index=False)\n",
    "    # print(len(pos_1s_train_sample))\n",
    "    # print(len(pos_0s_train_sample))\n",
    "    # print(len(neg_1s_train_sample))\n",
    "    # print(len(neg_0s_train_sample))\n",
    "    # print(len(pos_1s_test_sample))\n",
    "    # print(len(pos_0s_test_sample))\n",
    "    # print(len(neg_1s_test_sample))\n",
    "    # print(len(neg_0s_test_sample))\n",
    "    # break\n",
    "print(\"Tests successfully output to \" + FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d22dc-8797-4927-ba52-89003a6ebb99",
   "metadata": {},
   "source": [
    "## Analyze `results.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fcd9a0e3-a441-4123-8665-caf632b1b077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>A-1</th>\n",
       "      <th>F1-1</th>\n",
       "      <th>P-1</th>\n",
       "      <th>R-1</th>\n",
       "      <th>A-0</th>\n",
       "      <th>F1-0</th>\n",
       "      <th>P-0</th>\n",
       "      <th>R-0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.862857</td>\n",
       "      <td>0.896104</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>76.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928531</td>\n",
       "      <td>0.929545</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.795269</td>\n",
       "      <td>0.811943</td>\n",
       "      <td>0.797619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.821023</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>73.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.845040</td>\n",
       "      <td>0.847009</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.794088</td>\n",
       "      <td>0.819537</td>\n",
       "      <td>0.797619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.851059</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>74.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857062</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.855102</td>\n",
       "      <td>0.845238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.821327</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>71.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.880682</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.814945</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>73.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.869029</td>\n",
       "      <td>0.869257</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.759725</td>\n",
       "      <td>0.771765</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.815313</td>\n",
       "      <td>0.835443</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>71.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.855102</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.797590</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>66.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.808547</td>\n",
       "      <td>0.815972</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785227</td>\n",
       "      <td>0.788330</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.803564</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>68.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785593</td>\n",
       "      <td>0.786364</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.821201</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>69.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.845040</td>\n",
       "      <td>0.847009</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.797361</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.797619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.833239</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>72.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.868880</td>\n",
       "      <td>0.870940</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.796204</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.797619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG</th>\n",
       "      <td>0.824235</td>\n",
       "      <td>0.842042</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>71.3</td>\n",
       "      <td>12.7</td>\n",
       "      <td>16.8</td>\n",
       "      <td>67.2</td>\n",
       "      <td>0.853571</td>\n",
       "      <td>0.853256</td>\n",
       "      <td>0.856359</td>\n",
       "      <td>0.853571</td>\n",
       "      <td>0.795238</td>\n",
       "      <td>0.793638</td>\n",
       "      <td>0.804804</td>\n",
       "      <td>0.795238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           F1         P         R    tn    fp    fn    tp       A-1      F1-1  \\\n",
       "0    0.862857  0.896104  0.821429  76.0   8.0  15.0  69.0  0.928571  0.928531   \n",
       "1    0.821023  0.855263  0.773810  73.0  11.0  19.0  65.0  0.845238  0.845040   \n",
       "2    0.851059  0.873418  0.821429  74.0  10.0  15.0  69.0  0.857143  0.857062   \n",
       "3    0.821327  0.837500  0.797619  71.0  13.0  17.0  67.0  0.880952  0.880682   \n",
       "4    0.814945  0.853333  0.761905  73.0  11.0  20.0  64.0  0.869048  0.869029   \n",
       "5    0.815313  0.835443  0.785714  71.0  13.0  18.0  66.0  0.845238  0.844156   \n",
       "6    0.797590  0.790698  0.809524  66.0  18.0  16.0  68.0  0.809524  0.808547   \n",
       "7    0.803564  0.807229  0.797619  68.0  16.0  17.0  67.0  0.785714  0.785593   \n",
       "8    0.821429  0.821429  0.821429  69.0  15.0  15.0  69.0  0.845238  0.845040   \n",
       "9    0.833239  0.850000  0.809524  72.0  12.0  16.0  68.0  0.869048  0.868880   \n",
       "AVG  0.824235  0.842042  0.800000  71.3  12.7  16.8  67.2  0.853571  0.853256   \n",
       "\n",
       "          P-1       R-1       A-0      F1-0       P-0       R-0  \n",
       "0    0.929545  0.928571  0.797619  0.795269  0.811943  0.797619  \n",
       "1    0.847009  0.845238  0.797619  0.794088  0.819537  0.797619  \n",
       "2    0.857955  0.857143  0.845238  0.844156  0.855102  0.845238  \n",
       "3    0.884439  0.880952  0.761905  0.761905  0.761905  0.761905  \n",
       "4    0.869257  0.869048  0.761905  0.759725  0.771765  0.761905  \n",
       "5    0.855102  0.845238  0.785714  0.781250  0.811111  0.785714  \n",
       "6    0.815972  0.809524  0.785714  0.785227  0.788330  0.785714  \n",
       "7    0.786364  0.785714  0.821429  0.821201  0.823077  0.821429  \n",
       "8    0.847009  0.845238  0.797619  0.797361  0.799145  0.797619  \n",
       "9    0.870940  0.869048  0.797619  0.796204  0.806122  0.797619  \n",
       "AVG  0.856359  0.853571  0.795238  0.793638  0.804804  0.795238  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to analyze results (results.csv must be in the TEST folder)\n",
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# FOLDER = 'TEST_0.0' # the directory containing the train/test files and the results\n",
    "# PROPERTY = 'is_neg_PET' # linguistic property being investigated\n",
    "FOLDER = 'TEST_4.0.2'#  + '/NEG-MIXED-EXPERIMENT'\n",
    "# FOLDER = '../Euphemisms/BERT_Trainer/Bracket_Test/Without_Brackets'\n",
    "PROPERTY = 'is_vague'\n",
    "# FOLDER = 'TEST_5.0'\n",
    "# PROPERTY = 'is_neu_PET'\n",
    "df = pd.read_csv(FOLDER + '/results_base.csv', index_col=0)\n",
    "\n",
    "results = pd.DataFrame(columns=['F1', 'P', 'R', 'tn', 'fp', 'fn', 'tp', 'A-1', 'F1-1', 'P-1', 'R-1', 'A-0', 'F1-0', 'P-0', 'R-0'])\n",
    "\n",
    "# for each test, select the row with the best F1, then evaluate separate F1s for pos vs neg examples\n",
    "for x in range(0, 10):\n",
    "    test = df.loc[10*x:10*x+9]\n",
    "    max_f1 = test.loc[test['f1'].idxmax()] # this is the best row from this test\n",
    "\n",
    "    best_preds = max_f1['preds'].replace(' ', ', ') # the labels don't have a comma between them...\n",
    "    best_preds = ast.literal_eval(best_preds)\n",
    "    ref_df = pd.read_csv(FOLDER + '/test_' + str(x) + '.csv')\n",
    "    \n",
    "    # approach: get row IDs of pos/neg rows from test file, then pick the preds at the corresponding indices for evaluation\n",
    "    pos_examples = ref_df.loc[ref_df[PROPERTY]==1]\n",
    "    neg_examples = ref_df.loc[ref_df[PROPERTY]==0]\n",
    "\n",
    "    # get predictions\n",
    "    pos_preds = [best_preds[i] for i in pos_examples.index.tolist()]\n",
    "    neg_preds = [best_preds[i] for i in neg_examples.index.tolist()]\n",
    "    # get labels\n",
    "    pos_labels = pos_examples['is_euph'].tolist()\n",
    "    neg_labels = neg_examples['is_euph'].tolist()\n",
    "    # gather metrics\n",
    "    pos_a = accuracy_score(pos_labels, pos_preds)\n",
    "    pos_f1 = f1_score(pos_labels, pos_preds, average='macro')\n",
    "    pos_p = precision_score(pos_labels, pos_preds, average='macro')\n",
    "    pos_r = recall_score(pos_labels, pos_preds, average='macro')\n",
    "\n",
    "    # neg examples\n",
    "    neg_a = accuracy_score(neg_labels, neg_preds)\n",
    "    neg_f1 = f1_score(neg_labels, neg_preds, average='macro')\n",
    "    neg_p = precision_score(neg_labels, neg_preds, average='macro')\n",
    "    neg_r = recall_score(neg_labels, neg_preds, average='macro')\n",
    "    # all examples, just to make sure it matches original scores\n",
    "    # pos_acc = metric_acc.compute(predictions=best_preds, \n",
    "    #                           references=ref_df['is_euph'].tolist())\n",
    "    # pos_f1 = metric_f1.compute(predictions=best_preds, \n",
    "    #                    references=ref_df['is_euph'].tolist(), \n",
    "    #                    average='macro')\n",
    "    # print(pos_f1)\n",
    "    # print(pos_acc)\n",
    "    # print()\n",
    "    # add to an overall dataframe\n",
    "    stats = max_f1[0:7].tolist() # take the base stats from the best row\n",
    "    stats.extend([pos_a, pos_f1, pos_p, pos_r, neg_a, neg_f1, neg_p, neg_r]) # add on the pos/neg ones\n",
    "    results.loc[len(results.index)] = stats\n",
    "results.loc['AVG'] = results.mean()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d05647-4a46-4069-bcaf-dbd05a98fe42",
   "metadata": {},
   "source": [
    "## Alternative analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94c725fb-643f-4df0-8dc6-368540b1030c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>preds</th>\n",
       "      <th>num_vague_incorrect</th>\n",
       "      <th>num_unvague_incorrect</th>\n",
       "      <th>f1_vague</th>\n",
       "      <th>f1_unvague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.704554</td>\n",
       "      <td>0.669903</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>50</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>69</td>\n",
       "      <td>[1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0...</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>0.719471</td>\n",
       "      <td>0.688889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>72</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>58</td>\n",
       "      <td>[0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0...</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>0.797361</td>\n",
       "      <td>0.745638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.791482</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>69</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>[0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0...</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>0.821201</td>\n",
       "      <td>0.759725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.832168</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>77</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>63</td>\n",
       "      <td>[0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0...</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.868880</td>\n",
       "      <td>0.794088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.843441</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>62</td>\n",
       "      <td>[0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0...</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>0.904545</td>\n",
       "      <td>0.779592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.809416</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>66</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>70</td>\n",
       "      <td>[0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1...</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.773521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.815417</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>67</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>70</td>\n",
       "      <td>[0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1...</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>0.856410</td>\n",
       "      <td>0.773521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.833239</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>72</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>68</td>\n",
       "      <td>[0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1...</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.868880</td>\n",
       "      <td>0.796204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.797159</td>\n",
       "      <td>0.771739</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>63</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>71</td>\n",
       "      <td>[0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1...</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>0.843441</td>\n",
       "      <td>0.749965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.815417</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>67</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>70</td>\n",
       "      <td>[0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1...</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>0.856410</td>\n",
       "      <td>0.773521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1  precision    recall  tn  fp  fn  tp  \\\n",
       "0   0.704554   0.669903  0.821429  50  34  15  69   \n",
       "1   0.772228   0.828571  0.690476  72  12  26  58   \n",
       "2   0.791482   0.810127  0.761905  69  15  20  64   \n",
       "3   0.832168   0.900000  0.750000  77   7  21  63   \n",
       "4   0.843441   0.939394  0.738095  80   4  22  62   \n",
       "..       ...        ...       ...  ..  ..  ..  ..   \n",
       "95  0.809416   0.795455  0.833333  66  18  14  70   \n",
       "96  0.815417   0.804598  0.833333  67  17  14  70   \n",
       "97  0.833239   0.850000  0.809524  72  12  16  68   \n",
       "98  0.797159   0.771739  0.845238  63  21  13  71   \n",
       "99  0.815417   0.804598  0.833333  67  17  14  70   \n",
       "\n",
       "                                                preds  num_vague_incorrect  \\\n",
       "0   [1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0...                   23   \n",
       "1   [0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0...                   17   \n",
       "2   [0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0...                   15   \n",
       "3   [0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0...                   11   \n",
       "4   [0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0...                    8   \n",
       "..                                                ...                  ...   \n",
       "95  [0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1...                   13   \n",
       "96  [0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1...                   12   \n",
       "97  [0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1...                   11   \n",
       "98  [0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1...                   13   \n",
       "99  [0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1...                   12   \n",
       "\n",
       "    num_unvague_incorrect  f1_vague  f1_unvague  \n",
       "0                      26  0.719471    0.688889  \n",
       "1                      21  0.797361    0.745638  \n",
       "2                      20  0.821201    0.759725  \n",
       "3                      17  0.868880    0.794088  \n",
       "4                      18  0.904545    0.779592  \n",
       "..                    ...       ...         ...  \n",
       "95                     19  0.844156    0.773521  \n",
       "96                     19  0.856410    0.773521  \n",
       "97                     17  0.868880    0.796204  \n",
       "98                     21  0.843441    0.749965  \n",
       "99                     19  0.856410    0.773521  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to analyze performances during training (output vague/unvague scores per row)\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "FOLDER = 'TEST_4.0.2'\n",
    "PROPERTY = 'is_vague'\n",
    "df = pd.read_csv(FOLDER + '/results_base.csv', index_col=0)\n",
    "df['num_vague_incorrect'] = -1\n",
    "df['num_unvague_incorrect'] = -1\n",
    "df['f1_vague'] = -1\n",
    "df['f1_unvague'] = -1\n",
    "# results = pd.DataFrame(columns=['F1', 'P', 'R', 'tn', 'fp', 'fn', 'tp', 'F1-1', 'P-1', 'R-1', 'F1-0', 'P-0', 'R-0'])\n",
    "\n",
    "incorrect_pos_indices = {} # for analyzing which indices tended to be misclassified\n",
    "incorrect_neg_indices = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    preds = df.loc[i, 'preds'].replace(' ', ', ')\n",
    "    preds = ast.literal_eval(preds)\n",
    "    \n",
    "    num_test = int(i/10)\n",
    "    ref_df = pd.read_csv(FOLDER + '/test_' + str(num_test) + '.csv')\n",
    "    ref_df = ref_df.rename(columns={\"Unnamed: 0\": \"corpus_index\"})\n",
    "    \n",
    "    # approach: get row IDs of pos/neg rows from test file, then pick the preds at the corresponding indices for evaluation\n",
    "    pos_examples = ref_df.loc[ref_df[PROPERTY]==1]\n",
    "    neg_examples = ref_df.loc[ref_df[PROPERTY]==0]\n",
    "    # get labels\n",
    "    pos_labels = pos_examples['is_euph'].tolist()\n",
    "    neg_labels = neg_examples['is_euph'].tolist()\n",
    "    # get predictions\n",
    "    pos_preds = [preds[j] for j in pos_examples.index.tolist()]\n",
    "    neg_preds = [preds[j] for j in neg_examples.index.tolist()] \n",
    "    \n",
    "    # Identify potentially problematic examples:\n",
    "    pos_indices = [j for j in pos_examples['corpus_index'].tolist()]\n",
    "    neg_indices = [j for j in neg_examples['corpus_index'].tolist()]\n",
    "    \n",
    "    pos_pred_results = np.array(np.asarray(pos_labels) == np.asarray(pos_preds)).tolist()\n",
    "    neg_pred_results = np.array(np.asarray(neg_labels) == np.asarray(neg_preds)).tolist()\n",
    "    # print(pos_indices)\n",
    "    # print(incorrect_pos)\n",
    "    \n",
    "    for x in range(0, len(pos_indices)):\n",
    "        if (pos_pred_results[x] == False):\n",
    "            if (pos_indices[x] not in incorrect_pos_indices.keys()):\n",
    "                incorrect_pos_indices[pos_indices[x]] = 1\n",
    "            else:\n",
    "                incorrect_pos_indices[pos_indices[x]] += 1\n",
    "    \n",
    "    for x in range(0, len(neg_indices)):\n",
    "        if (neg_pred_results[x] == False):\n",
    "            if (neg_indices[x] not in incorrect_neg_indices.keys()):\n",
    "                incorrect_neg_indices[neg_indices[x]] = 1\n",
    "            else:\n",
    "                incorrect_neg_indices[neg_indices[x]] += 1\n",
    "\n",
    "    # incorrect_pos_PETs = []\n",
    "    # for index in incorrect_pos_indices:\n",
    "    #     incorrect_pos_PETs.append(ref_df.loc[index, 'type'])\n",
    "    # print(incorrect_pos_PETs)\n",
    "    \n",
    "    # gather metrics for pos examples\n",
    "    pos_f1 = f1_score(pos_labels, pos_preds, average='macro')\n",
    "    # pos_p = precision_score(pos_labels, pos_preds, average='macro')\n",
    "    # pos_r = recall_score(pos_labels, pos_preds, average='macro')\n",
    "    # # neg examples\n",
    "    neg_f1 = f1_score(neg_labels, neg_preds, average='macro')\n",
    "    # neg_p = precision_score(neg_labels, neg_preds, average='macro')\n",
    "    # neg_r = recall_score(neg_labels, neg_preds, average='macro')    \n",
    "    # make stats\n",
    "    # stats = row[0:.tolist() # max_f1[0:7].tolist() # take the base stats from the best row\n",
    "    # print([pos_f1, pos_p, pos_r, neg_f1, neg_p, neg_r]) # add on the pos/neg ones\n",
    "    # results.loc[len(results.index)] = stats\n",
    "    # print(pos_labels)\n",
    "    # print(pos_preds)\n",
    "    # print(len(pos_preds))\n",
    "    # print(np.sum(np.asarray(pos_labels) == np.asarray(pos_preds),axis=0))\n",
    "    # print(np.sum(np.asarray(pos_labels) != np.asarray(pos_preds),axis=0))\n",
    "    df.loc[i, 'f1_vague'] = pos_f1\n",
    "    df.loc[i, 'f1_unvague'] = neg_f1\n",
    "    df.loc[i, 'num_vague_incorrect'] = np.sum(np.asarray(pos_labels) != np.asarray(pos_preds),axis=0)\n",
    "    df.loc[i, 'num_unvague_incorrect'] = np.sum(np.asarray(neg_labels) != np.asarray(neg_preds),axis=0)\n",
    "    # break\n",
    "\n",
    "# print(incorrect_pos_indices)\n",
    "pos_incorrect = {k: v for k, v in sorted(incorrect_pos_indices.items(), key=lambda item: item[1])}\n",
    "neg_incorrect = {k: v for k, v in sorted(incorrect_neg_indices.items(), key=lambda item: item[1])}\n",
    "# print(pos_incorrect)\n",
    "# print(neg_incorrect)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6faedc6-e8b8-4bb1-9a23-04dd2d0ecafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_vague</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tinkle</td>\n",
       "      <td>We're just getting back what was TAKEN from us...</td>\n",
       "      <td>1</td>\n",
       "      <td>body functions/parts</td>\n",
       "      <td>tinkle</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>We're just getting back what was TAKEN from us...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tinkle</td>\n",
       "      <td>I think AB390 will pass next year now that the...</td>\n",
       "      <td>1</td>\n",
       "      <td>body functions/parts</td>\n",
       "      <td>tinkle</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I think AB390 will pass next year now that the...</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>Singled Out Think Like a Man, the new movie ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Anything but Secure A federal program designed...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>Not to be outdone, Sen. Rand Paul (R-Ky. ), so...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>In a post-election interview with POLITICO Pau...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>The law has also galvanized the growing immigr...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Aside from undocumented immigrants the America...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>sleep with</td>\n",
       "      <td>There were other photos she wanted me to see: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep with</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>There were other photos she wanted me to see B...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>sleep with</td>\n",
       "      <td>I am relieved to see two pup tents marked STAF...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep with</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Thank God I don't have to sleep with Ace Wands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>sleep around</td>\n",
       "      <td>Nothing serious, just long nights of me hackin...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep around</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>With all my caterwauling it's a wonder anyone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>with child</td>\n",
       "      <td>sounds more like Jonestown. They cant leave @ ...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>with child</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>They cant leave best advice I can give them is...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>with child</td>\n",
       "      <td>Nickname of a girl named Diana. 41. What you d...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>with child</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>What you do with Child Life 42</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1952 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "0                      tinkle   \n",
       "1                      tinkle   \n",
       "2     undocumented immigrants   \n",
       "3     undocumented immigrants   \n",
       "4     undocumented immigrants   \n",
       "...                       ...   \n",
       "1960               sleep with   \n",
       "1961               sleep with   \n",
       "1962             sleep around   \n",
       "1963               with child   \n",
       "1964               with child   \n",
       "\n",
       "                                            edited_text  is_euph  \\\n",
       "0     We're just getting back what was TAKEN from us...        1   \n",
       "1     I think AB390 will pass next year now that the...        1   \n",
       "2     Singled Out Think Like a Man, the new movie ba...        1   \n",
       "3     Not to be outdone, Sen. Rand Paul (R-Ky. ), so...        1   \n",
       "4     The law has also galvanized the growing immigr...        1   \n",
       "...                                                 ...      ...   \n",
       "1960  There were other photos she wanted me to see: ...        0   \n",
       "1961  I am relieved to see two pup tents marked STAF...        0   \n",
       "1962  Nothing serious, just long nights of me hackin...        0   \n",
       "1963  sounds more like Jonestown. They cant leave @ ...        0   \n",
       "1964  Nickname of a girl named Diana. 41. What you d...        0   \n",
       "\n",
       "                        category                    type     euph_status  \\\n",
       "0           body functions/parts                  tinkle     always_euph   \n",
       "1           body functions/parts                  tinkle     always_euph   \n",
       "2                       politics  undocumented immigrant     always_euph   \n",
       "3                       politics  undocumented immigrant     always_euph   \n",
       "4                       politics  undocumented immigrant     always_euph   \n",
       "...                          ...                     ...             ...   \n",
       "1960             sexual activity              sleep with  sometimes_euph   \n",
       "1961             sexual activity              sleep with  sometimes_euph   \n",
       "1962             sexual activity            sleep around  sometimes_euph   \n",
       "1963  physical/mental attributes              with child  sometimes_euph   \n",
       "1964  physical/mental attributes              with child  sometimes_euph   \n",
       "\n",
       "                                               sentence  is_vague  freq  \n",
       "0     We're just getting back what was TAKEN from us...         0     8  \n",
       "1     I think AB390 will pass next year now that the...         0    17  \n",
       "2     Anything but Secure A federal program designed...         0     0  \n",
       "3     In a post-election interview with POLITICO Pau...         0     0  \n",
       "4     Aside from undocumented immigrants the America...         0     0  \n",
       "...                                                 ...       ...   ...  \n",
       "1960  There were other photos she wanted me to see B...         0     1  \n",
       "1961    Thank God I don't have to sleep with Ace Wands          0     0  \n",
       "1962  With all my caterwauling it's a wonder anyone ...         0     2  \n",
       "1963  They cant leave best advice I can give them is...         0     9  \n",
       "1964                    What you do with Child Life 42          0    10  \n",
       "\n",
       "[1952 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# f = open('Results_3.0_Errors_by_Index.txt', 'r')\n",
    "df = pd.read_csv(\"VET_Corpus_v0.2.csv\", index_col=0)\n",
    "df['freq'] = 0\n",
    "\n",
    "# pos (vague) examples\n",
    "\n",
    "for k, v in pos_incorrect.items():\n",
    "    # df.loc[len(df.index)] = df.loc[k]\n",
    "    df.loc[k, 'freq'] = int(v)\n",
    "\n",
    "# text = f.readline()\n",
    "for k, v in neg_incorrect.items():\n",
    "    # df.loc[len(df.index)] = df.loc[k]\n",
    "    df.loc[k, 'freq'] = int(v)\n",
    "    \n",
    "# df = df.drop(indices)\n",
    "# for i, row in df.iterrows():\n",
    "#     if (df.loc[i, 'freq'] == -1):\n",
    "#         df = df.drop(i)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c51994b2-767f-43fb-9bf6-f41e885e5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Vagueness_Errors_4.0.2_v1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
