{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f2a405-26ac-446a-8056-1e9be964f693",
   "metadata": {},
   "source": [
    "# Vagueness Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9b8a94-1837-4b52-bd19-9e3a3e6d1fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa13564-0add-444b-a9a7-d777d37fb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function, HuggingFace-ify, takes a sample of euphemism corpus and it makes into an appropriate format for the HuggingFace Trainer class\n",
    "def hfify(df):\n",
    "    df = df.drop(['keyword', 'category', 'type', 'euph_status', 'sentence', 'is_vague'], axis=1)\n",
    "    df = df.rename(columns={'edited_text':'text', 'is_euph':'label'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22047f-d40d-4f6f-8f3d-5c81ed2746ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Annotation Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abfc23b-d0e2-4310-86bb-340254929d91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating Annotation Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b4683c-7540-492e-aea6-634463884e00",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pilot Sample (200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee38539d-9110-4139-96c2-9698368d278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "euph_corpus = pd.read_csv('Euphemism_Corpus_2-24.csv', index_col = 0, encoding= 'utf-8')\n",
    "always_euphs = pd.read_csv('always_euphs.csv', index_col = 0, encoding= 'utf-8') # 71 PETs here\n",
    "sometimes_euphs = pd.read_csv('sometimes_euphs.csv', index_col = 0, encoding= 'utf-8') # 58 PETs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "57b97942-7b23-4e6c-9ee8-93cc08e884bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tinkle</td>\n",
       "      <td>I think AB390 will pass next year now that the...</td>\n",
       "      <td>1</td>\n",
       "      <td>bodily functions</td>\n",
       "      <td>tinkle</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I think AB390 will pass next year now that the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>The law has also galvanized the growing immigr...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Aside from undocumented immigrants the America...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>venereal diseases</td>\n",
       "      <td>By 1928, there was a decided colonial response...</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>venereal disease</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Mass surveys movement restrictions monitoring ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sex worker</td>\n",
       "      <td>In fact, all the &lt;sex worker&gt; parents I know s...</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sex worker</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>In fact all the sex worker parents I know say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>mentally disabled</td>\n",
       "      <td>Few battles are truly worth fighting. Stand up...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>mentally disabled</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Stand up for those who can not stand up for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>seeing each other</td>\n",
       "      <td>The guy I date said he loves me and we &lt;seeing...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>seeing someone/each other</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>The guy I date said he loves me and we seeing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>seasoned</td>\n",
       "      <td>More &lt;seasoned&gt; Afghan officials say they are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>seasoned</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>More seasoned Afghan officials say they are cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>sleep with</td>\n",
       "      <td>There were other photos she wanted me to see: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep with</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>There were other photos she wanted me to see B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>sleep around</td>\n",
       "      <td>Nothing serious, just long nights of me hackin...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep around</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>With all my caterwauling it's a wonder anyone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>with child</td>\n",
       "      <td>sounds more like Jonestown. They cant leave @ ...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>with child</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>They cant leave best advice I can give them is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "1                      tinkle   \n",
       "4     undocumented immigrants   \n",
       "26          venereal diseases   \n",
       "31                 sex worker   \n",
       "54          mentally disabled   \n",
       "...                       ...   \n",
       "1952        seeing each other   \n",
       "1955                 seasoned   \n",
       "1960               sleep with   \n",
       "1962             sleep around   \n",
       "1963               with child   \n",
       "\n",
       "                                            edited_text  is_euph  \\\n",
       "1     I think AB390 will pass next year now that the...        1   \n",
       "4     The law has also galvanized the growing immigr...        1   \n",
       "26    By 1928, there was a decided colonial response...        1   \n",
       "31    In fact, all the <sex worker> parents I know s...        1   \n",
       "54    Few battles are truly worth fighting. Stand up...        1   \n",
       "...                                                 ...      ...   \n",
       "1952  The guy I date said he loves me and we <seeing...        0   \n",
       "1955  More <seasoned> Afghan officials say they are ...        0   \n",
       "1960  There were other photos she wanted me to see: ...        0   \n",
       "1962  Nothing serious, just long nights of me hackin...        0   \n",
       "1963  sounds more like Jonestown. They cant leave @ ...        0   \n",
       "\n",
       "                        category                       type     euph_status  \\\n",
       "1               bodily functions                     tinkle     always_euph   \n",
       "4                       politics     undocumented immigrant     always_euph   \n",
       "26               sexual activity           venereal disease     always_euph   \n",
       "31               sexual activity                 sex worker     always_euph   \n",
       "54    physical/mental attributes          mentally disabled     always_euph   \n",
       "...                          ...                        ...             ...   \n",
       "1952             sexual activity  seeing someone/each other  sometimes_euph   \n",
       "1955  physical/mental attributes                   seasoned  sometimes_euph   \n",
       "1960             sexual activity                 sleep with  sometimes_euph   \n",
       "1962             sexual activity               sleep around  sometimes_euph   \n",
       "1963  physical/mental attributes                 with child  sometimes_euph   \n",
       "\n",
       "                                               sentence  \n",
       "1     I think AB390 will pass next year now that the...  \n",
       "4     Aside from undocumented immigrants the America...  \n",
       "26    Mass surveys movement restrictions monitoring ...  \n",
       "31    In fact all the sex worker parents I know say ...  \n",
       "54    Stand up for those who can not stand up for th...  \n",
       "...                                                 ...  \n",
       "1952  The guy I date said he loves me and we seeing ...  \n",
       "1955  More seasoned Afghan officials say they are cl...  \n",
       "1960  There were other photos she wanted me to see B...  \n",
       "1962  With all my caterwauling it's a wonder anyone ...  \n",
       "1963  They cant leave best advice I can give them is...  \n",
       "\n",
       "[187 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "# this pilot sample will be formed by taking one example of each always_euph and one example of a 0 and 1 for each sometimes_euph\n",
    "# this will total 71 + 58*2 = 187 rows\n",
    "selected = [] # will store the indices of rows to select for the sample\n",
    "\n",
    "for PET in always_euphs['type']:\n",
    "    matches = euph_corpus[euph_corpus['type'] == PET]\n",
    "    selection = matches.sample(n=1).index # randomly select an index\n",
    "    selected.append(selection)\n",
    "\n",
    "for PET in sometimes_euphs['type']:\n",
    "    lit_matches = euph_corpus[(euph_corpus['type'] == PET) & (euph_corpus['is_euph'] == 0)]\n",
    "    euph_matches = euph_corpus[(euph_corpus['type'] == PET) & (euph_corpus['is_euph'] == 1)]\n",
    "    lit_selection = lit_matches.sample(n=1).index # randomly select an index\n",
    "    euph_selection = euph_matches.sample(n=1).index\n",
    "    selected.append(lit_selection)\n",
    "    selected.append(euph_selection)\n",
    "\n",
    "# idk, some number wrangling to convert from .index, which returns Int64 lists/\n",
    "selected = np.array(selected).tolist()\n",
    "selected = [item for sublist in selected for item in sublist]\n",
    "\n",
    "# print(selected)\n",
    "euph_corpus = euph_corpus[euph_corpus.index.isin(selected)]\n",
    "display(euph_corpus)\n",
    "print(len(euph_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "45e5386e-569f-42b4-a842-79a3ee0665c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "euph_corpus.to_csv('Pilot_Annotation_Sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894c1fd-7766-47c2-ae92-4e73894ec2b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Full Samples (Mech Turk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a724c47f-0607-48b9-bcee-4a6f7a01eefc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this mech turk sample will be formed by taking 1/3 of the corpus (655 examples) each\n",
    "# two samples will have 461 1's and 194 0's, while the other will have 462 1s and 193 0's\n",
    "euph_corpus = pd.read_csv('Euphemism_Corpus_2-24.csv', index_col = 0, encoding= 'utf-8')\n",
    "\n",
    "euph_corpus_1s = euph_corpus.loc[euph_corpus['is_euph'] == 1]\n",
    "euph_corpus_1s = euph_corpus_1s.sample(frac=1) # idiomatic way of scrambling the rows\n",
    "euph_result = np.array_split(euph_corpus_1s, 3)  \n",
    "\n",
    "euph_corpus_0s = euph_corpus.loc[euph_corpus['is_euph'] == 0]\n",
    "euph_corpus_0s = euph_corpus_0s.sample(frac=1) # idiomatic way of scrambling the rows\n",
    "lit_result = np.array_split(euph_corpus_0s, 3)  \n",
    "\n",
    "# combine each euph chunk (size 461, 461, 460) with each lit chunk (size 195, 194, 194) and shuffle\n",
    "sample_1 = pd.concat([euph_result[0], lit_result[1]]).sample(frac=1)\n",
    "sample_2 = pd.concat([euph_result[1], lit_result[2]]).sample(frac=1)\n",
    "sample_3 = pd.concat([euph_result[2], lit_result[0]]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a807e002-d0c1-4197-98bd-67c48bd502a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>slim</td>\n",
       "      <td>Given the veritable spec sheet overload that S...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>slim</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>In terms of the pursuit of the absolute slimme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>late</td>\n",
       "      <td>Handling was responsive but not squirrelly on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>death</td>\n",
       "      <td>late</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>And it managed to do all this while remaining ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>underdeveloped</td>\n",
       "      <td>We should pursue people-centered cultural crea...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>underdeveloped</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>We should be community-and people-focused in o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>collateral damage</td>\n",
       "      <td>Afghan civilians in proximity to alleged milit...</td>\n",
       "      <td>0</td>\n",
       "      <td>death</td>\n",
       "      <td>collateral damage</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Afghan civilians in proximity to alleged milit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>laid off</td>\n",
       "      <td>\"Let's also repeal laws which encourage sendin...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>lay off</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Those who still send American jobs overseas ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>droppings</td>\n",
       "      <td>Only the freshest and best Ice Cream you'll ev...</td>\n",
       "      <td>1</td>\n",
       "      <td>bodily functions</td>\n",
       "      <td>droppings</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>The line for a half hour once only for him to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>perished</td>\n",
       "      <td>He died in his car. Koehler suffered cardiac a...</td>\n",
       "      <td>1</td>\n",
       "      <td>death</td>\n",
       "      <td>perish</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Koehler suffered cardiac arrest and perished s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>overweight</td>\n",
       "      <td>It depends. If your child is &lt;overweight&gt;, it'...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>overweight</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>If your child is overweight it's not a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>elderly</td>\n",
       "      <td>\"Children 2 through 12 and their parents are i...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>elderly</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Children 2 through 12 and their parents are in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>able-bodied</td>\n",
       "      <td>But not all ATC users back the idea. One is Ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>able-bodied</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>One is Californian John Davis a trailblazer wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>655 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                keyword                                        edited_text  \\\n",
       "1535               slim  Given the veritable spec sheet overload that S...   \n",
       "1924               late  Handling was responsive but not squirrelly on ...   \n",
       "1080     underdeveloped  We should pursue people-centered cultural crea...   \n",
       "1572  collateral damage  Afghan civilians in proximity to alleged milit...   \n",
       "1235           laid off  \"Let's also repeal laws which encourage sendin...   \n",
       "...                 ...                                                ...   \n",
       "721           droppings  Only the freshest and best Ice Cream you'll ev...   \n",
       "806            perished  He died in his car. Koehler suffered cardiac a...   \n",
       "1454         overweight  It depends. If your child is <overweight>, it'...   \n",
       "183             elderly  \"Children 2 through 12 and their parents are i...   \n",
       "560         able-bodied  But not all ATC users back the idea. One is Ca...   \n",
       "\n",
       "      is_euph                    category               type     euph_status  \\\n",
       "1535        0  physical/mental attributes               slim  sometimes_euph   \n",
       "1924        0                       death               late  sometimes_euph   \n",
       "1080        1                    politics     underdeveloped  sometimes_euph   \n",
       "1572        0                       death  collateral damage  sometimes_euph   \n",
       "1235        1                  employment            lay off  sometimes_euph   \n",
       "...       ...                         ...                ...             ...   \n",
       "721         1            bodily functions          droppings     always_euph   \n",
       "806         1                       death             perish  sometimes_euph   \n",
       "1454        0  physical/mental attributes         overweight  sometimes_euph   \n",
       "183         1  physical/mental attributes            elderly     always_euph   \n",
       "560         1  physical/mental attributes        able-bodied     always_euph   \n",
       "\n",
       "                                               sentence  \n",
       "1535  In terms of the pursuit of the absolute slimme...  \n",
       "1924  And it managed to do all this while remaining ...  \n",
       "1080  We should be community-and people-focused in o...  \n",
       "1572  Afghan civilians in proximity to alleged milit...  \n",
       "1235  Those who still send American jobs overseas ou...  \n",
       "...                                                 ...  \n",
       "721   The line for a half hour once only for him to ...  \n",
       "806   Koehler suffered cardiac arrest and perished s...  \n",
       "1454            If your child is overweight it's not a   \n",
       "183   Children 2 through 12 and their parents are in...  \n",
       "560   One is Californian John Davis a trailblazer wh...  \n",
       "\n",
       "[655 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>late</td>\n",
       "      <td>And in a stark warning, Hunt warned that it ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>death</td>\n",
       "      <td>late</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>And in a stark warning Hunt warned that it may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>detainees</td>\n",
       "      <td>It's not just that torture and other violation...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>detainee</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>It's not just that torture and other violation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>aging</td>\n",
       "      <td>When we first meet him, he's undergoing psycho...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>aging</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>On a symbolic level it's about why Van Damme c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>Her primary rhetorical strategy centers around...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>People disgusted with Obama's enthusiasm for k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>pro-choice</td>\n",
       "      <td>We must remember that the first question is no...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>pro-choice</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>The reason why people are Pro-choice rather th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>sleep around</td>\n",
       "      <td>Nothing serious, just long nights of me hackin...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep around</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>With all my caterwauling it's a wonder anyone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>capital punishment</td>\n",
       "      <td>but not every homicide is a murder. \"Homicide ...</td>\n",
       "      <td>1</td>\n",
       "      <td>death</td>\n",
       "      <td>capital punishment</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Killing of one person by another by accident s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>senior citizens</td>\n",
       "      <td>Saturday November 3, 2012, 2:53 pm SOME KIND O...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>senior citizen</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>by leaving all those destroyed senior citizens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>collateral damage</td>\n",
       "      <td>In the current New Yorker sub req'd, Michael S...</td>\n",
       "      <td>0</td>\n",
       "      <td>death</td>\n",
       "      <td>collateral damage</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>It can happen both through indiscriminate use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>disabled</td>\n",
       "      <td>When, during the speech, he belted out \"All Am...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>disabled</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>When during the speech he belted out All Ameri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>655 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "1902                     late   \n",
       "110                 detainees   \n",
       "1479                    aging   \n",
       "15    undocumented immigrants   \n",
       "617                pro-choice   \n",
       "...                       ...   \n",
       "1962             sleep around   \n",
       "411        capital punishment   \n",
       "481           senior citizens   \n",
       "1571        collateral damage   \n",
       "1026                 disabled   \n",
       "\n",
       "                                            edited_text  is_euph  \\\n",
       "1902  And in a stark warning, Hunt warned that it ma...        0   \n",
       "110   It's not just that torture and other violation...        1   \n",
       "1479  When we first meet him, he's undergoing psycho...        0   \n",
       "15    Her primary rhetorical strategy centers around...        1   \n",
       "617   We must remember that the first question is no...        1   \n",
       "...                                                 ...      ...   \n",
       "1962  Nothing serious, just long nights of me hackin...        0   \n",
       "411   but not every homicide is a murder. \"Homicide ...        1   \n",
       "481   Saturday November 3, 2012, 2:53 pm SOME KIND O...        1   \n",
       "1571  In the current New Yorker sub req'd, Michael S...        0   \n",
       "1026  When, during the speech, he belted out \"All Am...        1   \n",
       "\n",
       "                        category                    type     euph_status  \\\n",
       "1902                       death                    late  sometimes_euph   \n",
       "110                     politics                detainee     always_euph   \n",
       "1479  physical/mental attributes                   aging  sometimes_euph   \n",
       "15                      politics  undocumented immigrant     always_euph   \n",
       "617                     politics              pro-choice     always_euph   \n",
       "...                          ...                     ...             ...   \n",
       "1962             sexual activity            sleep around  sometimes_euph   \n",
       "411                        death      capital punishment     always_euph   \n",
       "481   physical/mental attributes          senior citizen     always_euph   \n",
       "1571                       death       collateral damage  sometimes_euph   \n",
       "1026  physical/mental attributes                disabled  sometimes_euph   \n",
       "\n",
       "                                               sentence  \n",
       "1902  And in a stark warning Hunt warned that it may...  \n",
       "110   It's not just that torture and other violation...  \n",
       "1479  On a symbolic level it's about why Van Damme c...  \n",
       "15    People disgusted with Obama's enthusiasm for k...  \n",
       "617   The reason why people are Pro-choice rather th...  \n",
       "...                                                 ...  \n",
       "1962  With all my caterwauling it's a wonder anyone ...  \n",
       "411   Killing of one person by another by accident s...  \n",
       "481   by leaving all those destroyed senior citizens...  \n",
       "1571  It can happen both through indiscriminate use ...  \n",
       "1026  When during the speech he belted out All Ameri...  \n",
       "\n",
       "[655 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>go all the way</td>\n",
       "      <td>Lets &lt;go all the way&gt; (Lets go all the way ) L...</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>go all the way</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Lets go all the way Lets go all the way Lets g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>economical</td>\n",
       "      <td>he is a political head of them, or in such sen...</td>\n",
       "      <td>0</td>\n",
       "      <td>employment</td>\n",
       "      <td>economical</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>he is a political head of them or in such sens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>income inequality</td>\n",
       "      <td>Here is an older paper which finds that change...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>income inequality</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Here is an older paper which finds that change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>intoxicated</td>\n",
       "      <td>- Be &lt;intoxicated&gt; during their time with the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>substances</td>\n",
       "      <td>intoxicated</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Be intoxicated during their time with the virgin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>aging</td>\n",
       "      <td>At first glance, raising the retirement age se...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>aging</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>At first glance raising the retirement age see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>overweight</td>\n",
       "      <td>Exercise regularly and maintain a healthy body...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>overweight</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Lose weight if you are overweight but avoid lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>special needs</td>\n",
       "      <td>She's still in the hospital. Baby Nozomi is st...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>special needs</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Baby Nozomi is still in the NICU while big sis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>correctional facilities</td>\n",
       "      <td>Confinement Youth with disabilities who are ad...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>correctional facility</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Very few correctional facilities have formal v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>let go of</td>\n",
       "      <td>It would be easier than her breaking up with h...</td>\n",
       "      <td>0</td>\n",
       "      <td>employment</td>\n",
       "      <td>let go of</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>He knew he had to let go of her before he beca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>pass on</td>\n",
       "      <td>Continuing from the Draenei warlock thread to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>death</td>\n",
       "      <td>pass on</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>We were discussing Sylvanas' leader story and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>655 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "1008           go all the way   \n",
       "1758               economical   \n",
       "256         income inequality   \n",
       "1141              intoxicated   \n",
       "873                     aging   \n",
       "...                       ...   \n",
       "851                overweight   \n",
       "1699            special needs   \n",
       "60    correctional facilities   \n",
       "1385                let go of   \n",
       "813                   pass on   \n",
       "\n",
       "                                            edited_text  is_euph  \\\n",
       "1008  Lets <go all the way> (Lets go all the way ) L...        1   \n",
       "1758  he is a political head of them, or in such sen...        0   \n",
       "256   Here is an older paper which finds that change...        1   \n",
       "1141  - Be <intoxicated> during their time with the ...        1   \n",
       "873   At first glance, raising the retirement age se...        1   \n",
       "...                                                 ...      ...   \n",
       "851   Exercise regularly and maintain a healthy body...        1   \n",
       "1699  She's still in the hospital. Baby Nozomi is st...        0   \n",
       "60    Confinement Youth with disabilities who are ad...        1   \n",
       "1385  It would be easier than her breaking up with h...        0   \n",
       "813   Continuing from the Draenei warlock thread to ...        1   \n",
       "\n",
       "                        category                   type     euph_status  \\\n",
       "1008             sexual activity         go all the way  sometimes_euph   \n",
       "1758                  employment             economical  sometimes_euph   \n",
       "256                   employment      income inequality     always_euph   \n",
       "1141                  substances            intoxicated  sometimes_euph   \n",
       "873   physical/mental attributes                  aging  sometimes_euph   \n",
       "...                          ...                    ...             ...   \n",
       "851   physical/mental attributes             overweight  sometimes_euph   \n",
       "1699  physical/mental attributes          special needs  sometimes_euph   \n",
       "60                    employment  correctional facility     always_euph   \n",
       "1385                  employment              let go of  sometimes_euph   \n",
       "813                        death                pass on  sometimes_euph   \n",
       "\n",
       "                                               sentence  \n",
       "1008  Lets go all the way Lets go all the way Lets g...  \n",
       "1758  he is a political head of them or in such sens...  \n",
       "256   Here is an older paper which finds that change...  \n",
       "1141   Be intoxicated during their time with the virgin  \n",
       "873   At first glance raising the retirement age see...  \n",
       "...                                                 ...  \n",
       "851   Lose weight if you are overweight but avoid lo...  \n",
       "1699  Baby Nozomi is still in the NICU while big sis...  \n",
       "60    Very few correctional facilities have formal v...  \n",
       "1385  He knew he had to let go of her before he beca...  \n",
       "813   We were discussing Sylvanas' leader story and ...  \n",
       "\n",
       "[655 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sample_1)\n",
    "display(sample_2)\n",
    "display(sample_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99d7b297-e867-4fbe-8a5c-a1f340afbc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1.to_csv(\"AMT_Sample_1.csv\")\n",
    "sample_2.to_csv(\"AMT_Sample_2.csv\")\n",
    "sample_3.to_csv(\"AMT_Sample_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dea76eff-9c66-44a8-be5e-f6193676450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate HTML\n",
    "import pandas as pd\n",
    " \n",
    "# to read csv file named \"samplee\"\n",
    "a = pd.read_csv(\"AMT/AMT_Sample_1.1.csv\")\n",
    " \n",
    "# to save as html file\n",
    "# named as \"Table\"\n",
    "a.to_html(\"AMT/AMT_Sample_1.1.html\", index=False, na_rep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e7d50-04eb-43d8-94a1-c9d09729df2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Annotation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a02e99a2-f038-4e8b-b8e1-3eb7e5cb9819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "annotations = pd.read_csv(\"Annotation_Task2.1.csv\", index_col=0, encoding= 'utf-8')\n",
    "euph_corpus = pd.read_csv('Euphemism_Corpus_2-24.csv', index_col = 0, encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "078a0108-fbeb-4581-8d61-a4c4421281bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17322\\AppData\\Local\\Temp/ipykernel_13232/1244562030.py:1: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  indices = pd.Series([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "0    1202\n",
      "0    1006\n",
      "0     826\n",
      "0    1147\n",
      "0    1445\n",
      "     ... \n",
      "0     554\n",
      "0     378\n",
      "0     856\n",
      "0     902\n",
      "0      16\n",
      "Length: 165, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# the next 2 chunks are to fix a mistake i made; leaving out the index in the annotation samples\n",
    "indices = pd.Series([])\n",
    "for i, row in annotations.iterrows():\n",
    "    text = annotations.loc[i, 'text']\n",
    "    for j, row in euph_corpus.iterrows():\n",
    "        t = euph_corpus.loc[j, 'edited_text']\n",
    "        if (t.strip()==text.strip()):\n",
    "            indices = pd.concat([indices, pd.Series([j])])\n",
    "            continue\n",
    "print(len(indices))\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e89a74e-8735-4971-b716-3a75dc1d4058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations = annotations.set_index(indices)\n",
    "annotations.to_csv('Annotation_Task2.1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f154f98f-ee5e-4c28-a44f-abf41d87ad14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>paraphrase1</th>\n",
       "      <th>paraphrase2</th>\n",
       "      <th>paraphrase3</th>\n",
       "      <th>paraphrase4</th>\n",
       "      <th>paraphrase5</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>between jobs</td>\n",
       "      <td>Hannah's mother explained that God was testing...</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>same-sex</td>\n",
       "      <td>So, for instance, when the study finds that ch...</td>\n",
       "      <td>gay</td>\n",
       "      <td>gay</td>\n",
       "      <td>same-sex</td>\n",
       "      <td>homosexual</td>\n",
       "      <td>gay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>pass away</td>\n",
       "      <td>Your story has touched my heart and I wanted t...</td>\n",
       "      <td>die</td>\n",
       "      <td>die</td>\n",
       "      <td>die</td>\n",
       "      <td>die</td>\n",
       "      <td>die</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>intoxicated</td>\n",
       "      <td>Trying difficult is now somehow something to b...</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>neutralize</td>\n",
       "      <td>@#19 Rob and #20 Paul: One thing I think is fa...</td>\n",
       "      <td>neutralize</td>\n",
       "      <td>eliminate</td>\n",
       "      <td>nullify</td>\n",
       "      <td>neutralize</td>\n",
       "      <td>kill</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>same sex</td>\n",
       "      <td>Multiple \"partners\" as you call it was NEVER i...</td>\n",
       "      <td>gay</td>\n",
       "      <td>gay</td>\n",
       "      <td>same sex</td>\n",
       "      <td>homosexual</td>\n",
       "      <td>gay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>inebriated</td>\n",
       "      <td>In a few weeks, tens of thousands of college f...</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>intoxicated</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>overweight</td>\n",
       "      <td>Also, there's a good bit of mounting evidence ...</td>\n",
       "      <td>fat</td>\n",
       "      <td>fat</td>\n",
       "      <td>overweight</td>\n",
       "      <td>fat</td>\n",
       "      <td>fat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>demise</td>\n",
       "      <td>At the Vatican, following the &lt;demise&gt; of the ...</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>fall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>The Arizona Republic reports that young &lt;undoc...</td>\n",
       "      <td>illegals</td>\n",
       "      <td>illegal immigrants</td>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>illegal immigrants</td>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "1202             between jobs   \n",
       "1006                 same-sex   \n",
       "826                 pass away   \n",
       "1147              intoxicated   \n",
       "1445               neutralize   \n",
       "...                       ...   \n",
       "554                  same sex   \n",
       "378                inebriated   \n",
       "856                overweight   \n",
       "902                    demise   \n",
       "16    undocumented immigrants   \n",
       "\n",
       "                                                   text paraphrase1  \\\n",
       "1202  Hannah's mother explained that God was testing...  unemployed   \n",
       "1006  So, for instance, when the study finds that ch...         gay   \n",
       "826   Your story has touched my heart and I wanted t...         die   \n",
       "1147  Trying difficult is now somehow something to b...       drunk   \n",
       "1445  @#19 Rob and #20 Paul: One thing I think is fa...  neutralize   \n",
       "...                                                 ...         ...   \n",
       "554   Multiple \"partners\" as you call it was NEVER i...         gay   \n",
       "378   In a few weeks, tens of thousands of college f...       drunk   \n",
       "856   Also, there's a good bit of mounting evidence ...         fat   \n",
       "902   At the Vatican, following the <demise> of the ...       death   \n",
       "16    The Arizona Republic reports that young <undoc...    illegals   \n",
       "\n",
       "             paraphrase2              paraphrase3         paraphrase4  \\\n",
       "1202          unemployed               unemployed          unemployed   \n",
       "1006                 gay                 same-sex          homosexual   \n",
       "826                  die                      die                 die   \n",
       "1147               drunk                    drunk               drunk   \n",
       "1445           eliminate                  nullify          neutralize   \n",
       "...                  ...                      ...                 ...   \n",
       "554                  gay                 same sex          homosexual   \n",
       "378                drunk                    drunk               drunk   \n",
       "856                  fat               overweight                 fat   \n",
       "902                death                    death               death   \n",
       "16    illegal immigrants  undocumented immigrants  illegal immigrants   \n",
       "\n",
       "                  paraphrase5  label  \n",
       "1202               unemployed      1  \n",
       "1006                      gay      1  \n",
       "826                       die      1  \n",
       "1147                    drunk      1  \n",
       "1445                     kill      0  \n",
       "...                       ...    ...  \n",
       "554                       gay      1  \n",
       "378               intoxicated      1  \n",
       "856                       fat      1  \n",
       "902                      fall      1  \n",
       "16    undocumented immigrants      1  \n",
       "\n",
       "[165 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to put the 0 or 1 label back onto the annotations by matching the index numbers\n",
    "# also fill in blanks with the keyword\n",
    "annotations[\"label\"] = -1\n",
    "\n",
    "for i, row in annotations.iterrows():\n",
    "    annotations.loc[i, 'type'] = euph_corpus.loc[i]['type']\n",
    "    \n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd48a043-09d1-4a60-b4bf-ccb5ded68357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03607bea-9b00-446d-9436-660fc2df0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(s1, s2):\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(s1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(s2, convert_to_tensor=True)\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "    # print(\"Similarity score:\", cosine_scores.item())\n",
    "    return cosine_scores.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73971c2e-8c98-44d5-90f6-a52d9c863c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20039547979831696"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"@#19 Rob and #20 Paul: One thing I think is fascinating about the RD-180 powering the Atlas is that it's derived from the RD-170/171, which was developed for the Zenit rocket, used in various configurations to boost the Energia (also used in an analogous role to the SRBs in the US Shuttle program, boosting the Energia-Buran launch stack) @ @ @ @ @ @ @ @ @ @ but in a nutshell, you have a very effective engine design that was originally intended to one-up the US and to launch Soviet military systems like the Polyus, which was intended to <neutralize> the strategic advantage of the US SDI. And now it's powering NASA (as well as US military) launches!\"\n",
    "s2 = \"Little thought seems to have been given the question of whether to commit ground forces. The recommendations were accepted by the President, and a directive was at once sent General MacArthur authorizing him to use @ @ @ @ @ @ @ @ @ @ of the 38th parallel, and instructing him to <neutralize> Formosa by the use of the Seventh Fleet. \"\n",
    "compute_similarity(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29608eb6-c031-42ee-a0ec-89dfcae91e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>paraphrase1</th>\n",
       "      <th>paraphrase2</th>\n",
       "      <th>paraphrase3</th>\n",
       "      <th>paraphrase4</th>\n",
       "      <th>paraphrase5</th>\n",
       "      <th>label</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>between jobs</td>\n",
       "      <td>Hannah's mother explained that God was testing...</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>same-sex</td>\n",
       "      <td>So, for instance, when the study finds that ch...</td>\n",
       "      <td>gay</td>\n",
       "      <td>gay</td>\n",
       "      <td>same-sex</td>\n",
       "      <td>homosexual</td>\n",
       "      <td>gay</td>\n",
       "      <td>1</td>\n",
       "      <td>0.807432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>pass away</td>\n",
       "      <td>Your story has touched my heart and I wanted t...</td>\n",
       "      <td>die</td>\n",
       "      <td>die</td>\n",
       "      <td>die</td>\n",
       "      <td>die</td>\n",
       "      <td>die</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>intoxicated</td>\n",
       "      <td>Trying difficult is now somehow something to b...</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>neutralize</td>\n",
       "      <td>@#19 Rob and #20 Paul: One thing I think is fa...</td>\n",
       "      <td>neutralize</td>\n",
       "      <td>eliminate</td>\n",
       "      <td>nullify</td>\n",
       "      <td>neutralize</td>\n",
       "      <td>kill</td>\n",
       "      <td>0</td>\n",
       "      <td>0.424927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>same sex</td>\n",
       "      <td>Multiple \"partners\" as you call it was NEVER i...</td>\n",
       "      <td>gay</td>\n",
       "      <td>gay</td>\n",
       "      <td>same sex</td>\n",
       "      <td>homosexual</td>\n",
       "      <td>gay</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>inebriated</td>\n",
       "      <td>In a few weeks, tens of thousands of college f...</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>intoxicated</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>overweight</td>\n",
       "      <td>Also, there's a good bit of mounting evidence ...</td>\n",
       "      <td>fat</td>\n",
       "      <td>fat</td>\n",
       "      <td>overweight</td>\n",
       "      <td>fat</td>\n",
       "      <td>fat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.867237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>demise</td>\n",
       "      <td>At the Vatican, following the &lt;demise&gt; of the ...</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>fall</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>The Arizona Republic reports that young &lt;undoc...</td>\n",
       "      <td>illegals</td>\n",
       "      <td>illegal immigrants</td>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>illegal immigrants</td>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>1</td>\n",
       "      <td>0.827272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "1202             between jobs   \n",
       "1006                 same-sex   \n",
       "826                 pass away   \n",
       "1147              intoxicated   \n",
       "1445               neutralize   \n",
       "...                       ...   \n",
       "554                  same sex   \n",
       "378                inebriated   \n",
       "856                overweight   \n",
       "902                    demise   \n",
       "16    undocumented immigrants   \n",
       "\n",
       "                                                   text paraphrase1  \\\n",
       "1202  Hannah's mother explained that God was testing...  unemployed   \n",
       "1006  So, for instance, when the study finds that ch...         gay   \n",
       "826   Your story has touched my heart and I wanted t...         die   \n",
       "1147  Trying difficult is now somehow something to b...       drunk   \n",
       "1445  @#19 Rob and #20 Paul: One thing I think is fa...  neutralize   \n",
       "...                                                 ...         ...   \n",
       "554   Multiple \"partners\" as you call it was NEVER i...         gay   \n",
       "378   In a few weeks, tens of thousands of college f...       drunk   \n",
       "856   Also, there's a good bit of mounting evidence ...         fat   \n",
       "902   At the Vatican, following the <demise> of the ...       death   \n",
       "16    The Arizona Republic reports that young <undoc...    illegals   \n",
       "\n",
       "             paraphrase2              paraphrase3         paraphrase4  \\\n",
       "1202          unemployed               unemployed          unemployed   \n",
       "1006                 gay                 same-sex          homosexual   \n",
       "826                  die                      die                 die   \n",
       "1147               drunk                    drunk               drunk   \n",
       "1445           eliminate                  nullify          neutralize   \n",
       "...                  ...                      ...                 ...   \n",
       "554                  gay                 same sex          homosexual   \n",
       "378                drunk                    drunk               drunk   \n",
       "856                  fat               overweight                 fat   \n",
       "902                death                    death               death   \n",
       "16    illegal immigrants  undocumented immigrants  illegal immigrants   \n",
       "\n",
       "                  paraphrase5  label  semantic_similarity  \n",
       "1202               unemployed      1             1.000000  \n",
       "1006                      gay      1             0.807432  \n",
       "826                       die      1             1.000000  \n",
       "1147                    drunk      1             1.000000  \n",
       "1445                     kill      0             0.424927  \n",
       "...                       ...    ...                  ...  \n",
       "554                       gay      1             0.818027  \n",
       "378               intoxicated      1             0.869052  \n",
       "856                       fat      1             0.867237  \n",
       "902                      fall      1             0.751394  \n",
       "16    undocumented immigrants      1             0.827272  \n",
       "\n",
       "[165 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import statistics\n",
    "\n",
    "# stores the similarity score\n",
    "annotations['semantic_similarity'] = -1\n",
    "# create list of all pairs of paraphrases\n",
    "iterable = ['paraphrase1', 'paraphrase2', 'paraphrase3', 'paraphrase4', 'paraphrase5']\n",
    "combos = list(itertools.combinations(iterable, 2))\n",
    "\n",
    "n = 0\n",
    "for i, row in annotations.iterrows():\n",
    "    annotations.loc[i] = annotations.loc[i].fillna(annotations.loc[i, 'keyword'])\n",
    "    similarities = []\n",
    "    # seems like only binary comparisons allowed, so have to compare each para with each other and then average them all\n",
    "    for c in combos:\n",
    "        p1 = annotations.loc[i, c[0]]\n",
    "        p2 = annotations.loc[i, c[1]]\n",
    "        sim = compute_similarity(p1, p2)\n",
    "        similarities.append(sim)\n",
    "    annotations.loc[i, 'semantic_similarity'] = statistics.mean(similarities)\n",
    "    if (n % 20 == 0):\n",
    "        print(n)\n",
    "    n+=1\n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7387b9-8edb-40e2-8b04-09e9d99d36b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>paraphrase1</th>\n",
       "      <th>paraphrase2</th>\n",
       "      <th>paraphrase3</th>\n",
       "      <th>paraphrase4</th>\n",
       "      <th>paraphrase5</th>\n",
       "      <th>label</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>is_vague</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>between jobs</td>\n",
       "      <td>Hannah's mother explained that God was testing...</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>between jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>same-sex</td>\n",
       "      <td>So, for instance, when the study finds that ch...</td>\n",
       "      <td>gay</td>\n",
       "      <td>gay</td>\n",
       "      <td>same-sex</td>\n",
       "      <td>homosexual</td>\n",
       "      <td>gay</td>\n",
       "      <td>1</td>\n",
       "      <td>0.807432</td>\n",
       "      <td>0</td>\n",
       "      <td>same-sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>pass away</td>\n",
       "      <td>Your story has touched my heart and I wanted t...</td>\n",
       "      <td>die</td>\n",
       "      <td>die</td>\n",
       "      <td>die</td>\n",
       "      <td>die</td>\n",
       "      <td>die</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>pass away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>intoxicated</td>\n",
       "      <td>Trying difficult is now somehow something to b...</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>intoxicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>neutralize</td>\n",
       "      <td>@#19 Rob and #20 Paul: One thing I think is fa...</td>\n",
       "      <td>neutralize</td>\n",
       "      <td>eliminate</td>\n",
       "      <td>nullify</td>\n",
       "      <td>neutralize</td>\n",
       "      <td>kill</td>\n",
       "      <td>0</td>\n",
       "      <td>0.424927</td>\n",
       "      <td>1</td>\n",
       "      <td>neutralize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>same sex</td>\n",
       "      <td>Multiple \"partners\" as you call it was NEVER i...</td>\n",
       "      <td>gay</td>\n",
       "      <td>gay</td>\n",
       "      <td>same sex</td>\n",
       "      <td>homosexual</td>\n",
       "      <td>gay</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818027</td>\n",
       "      <td>0</td>\n",
       "      <td>same sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>inebriated</td>\n",
       "      <td>In a few weeks, tens of thousands of college f...</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>intoxicated</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869052</td>\n",
       "      <td>0</td>\n",
       "      <td>inebriated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>overweight</td>\n",
       "      <td>Also, there's a good bit of mounting evidence ...</td>\n",
       "      <td>fat</td>\n",
       "      <td>fat</td>\n",
       "      <td>overweight</td>\n",
       "      <td>fat</td>\n",
       "      <td>fat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.867237</td>\n",
       "      <td>0</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>demise</td>\n",
       "      <td>At the Vatican, following the &lt;demise&gt; of the ...</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>fall</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751394</td>\n",
       "      <td>0</td>\n",
       "      <td>demise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>The Arizona Republic reports that young &lt;undoc...</td>\n",
       "      <td>illegals</td>\n",
       "      <td>illegal immigrants</td>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>illegal immigrants</td>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>1</td>\n",
       "      <td>0.827272</td>\n",
       "      <td>0</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "1202             between jobs   \n",
       "1006                 same-sex   \n",
       "826                 pass away   \n",
       "1147              intoxicated   \n",
       "1445               neutralize   \n",
       "...                       ...   \n",
       "554                  same sex   \n",
       "378                inebriated   \n",
       "856                overweight   \n",
       "902                    demise   \n",
       "16    undocumented immigrants   \n",
       "\n",
       "                                                   text paraphrase1  \\\n",
       "1202  Hannah's mother explained that God was testing...  unemployed   \n",
       "1006  So, for instance, when the study finds that ch...         gay   \n",
       "826   Your story has touched my heart and I wanted t...         die   \n",
       "1147  Trying difficult is now somehow something to b...       drunk   \n",
       "1445  @#19 Rob and #20 Paul: One thing I think is fa...  neutralize   \n",
       "...                                                 ...         ...   \n",
       "554   Multiple \"partners\" as you call it was NEVER i...         gay   \n",
       "378   In a few weeks, tens of thousands of college f...       drunk   \n",
       "856   Also, there's a good bit of mounting evidence ...         fat   \n",
       "902   At the Vatican, following the <demise> of the ...       death   \n",
       "16    The Arizona Republic reports that young <undoc...    illegals   \n",
       "\n",
       "             paraphrase2              paraphrase3         paraphrase4  \\\n",
       "1202          unemployed               unemployed          unemployed   \n",
       "1006                 gay                 same-sex          homosexual   \n",
       "826                  die                      die                 die   \n",
       "1147               drunk                    drunk               drunk   \n",
       "1445           eliminate                  nullify          neutralize   \n",
       "...                  ...                      ...                 ...   \n",
       "554                  gay                 same sex          homosexual   \n",
       "378                drunk                    drunk               drunk   \n",
       "856                  fat               overweight                 fat   \n",
       "902                death                    death               death   \n",
       "16    illegal immigrants  undocumented immigrants  illegal immigrants   \n",
       "\n",
       "                  paraphrase5  label  semantic_similarity  is_vague  \\\n",
       "1202               unemployed      1             1.000000         0   \n",
       "1006                      gay      1             0.807432         0   \n",
       "826                       die      1             1.000000         0   \n",
       "1147                    drunk      1             1.000000         0   \n",
       "1445                     kill      0             0.424927         1   \n",
       "...                       ...    ...                  ...       ...   \n",
       "554                       gay      1             0.818027         0   \n",
       "378               intoxicated      1             0.869052         0   \n",
       "856                       fat      1             0.867237         0   \n",
       "902                      fall      1             0.751394         0   \n",
       "16    undocumented immigrants      1             0.827272         0   \n",
       "\n",
       "                        type  \n",
       "1202            between jobs  \n",
       "1006                same-sex  \n",
       "826                pass away  \n",
       "1147             intoxicated  \n",
       "1445              neutralize  \n",
       "...                      ...  \n",
       "554                 same sex  \n",
       "378               inebriated  \n",
       "856               overweight  \n",
       "902                   demise  \n",
       "16    undocumented immigrant  \n",
       "\n",
       "[165 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euph_corpus = pd.read_csv('Euphemism_Corpus_2-24.csv', index_col = 0, encoding= 'utf-8')\n",
    "\n",
    "a1[\"type\"] = -1\n",
    "a2[\"type\"] = -1\n",
    "\n",
    "for i, row in a1.iterrows():\n",
    "    a1.loc[i, 'type'] = euph_corpus.loc[i]['type']\n",
    "    \n",
    "for i, row in a2.iterrows():\n",
    "    a2.loc[i, 'type'] = euph_corpus.loc[i]['type']\n",
    "    \n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "724c4272-a0e6-471b-aad9-67e5554364be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# annotations.to_csv(\"Annotation_Task2_Analysis.csv\")\n",
    "\n",
    "# perform one-time linking\n",
    "# a1 = pd.read_csv('Annotation_Task1_Analysis_v2.csv', index_col=0)\n",
    "# a2 = pd.read_csv('Annotation_Task2_Analysis.csv', index_col=0)\n",
    "\n",
    "d = {}\n",
    "            \n",
    "for i, row in a1.iterrows():\n",
    "    keyword = a1.loc[i, 'type']\n",
    "    label = a1.loc[i, 'label']\n",
    "    keyword = keyword + '_' + str(label)\n",
    "    is_vague = a1.loc[i, 'is_vague']\n",
    "    if keyword not in d:\n",
    "        d[keyword] = [-1, -1] # -1 in the second annotation sample means this PET didn't show up (only one example in dataset)\n",
    "    d[keyword][0] = is_vague\n",
    "\n",
    "for j, row in a2.iterrows():\n",
    "    keyword = a2.loc[j, 'type']\n",
    "    label = a2.loc[j, 'label']\n",
    "    keyword = keyword + '_' + str(label)\n",
    "    is_vague = a2.loc[j, 'is_vague']\n",
    "    \n",
    "    d[keyword][1] = is_vague\n",
    "\n",
    "df = pd.DataFrame(columns = ['PET', 'is_vague_1', 'is_vague_2'])\n",
    "\n",
    "for PET, e in d.items():\n",
    "    new_row = pd.Series({'PET': PET,\n",
    "                         'is_vague_1': e[0],\n",
    "                         'is_vague_2': e[1]\n",
    "                   })\n",
    "    df = pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "    \n",
    "df\n",
    "df.to_csv('VET_List.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ac3500-4d4b-4d21-89a5-37967f18a4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 HIGH SIM PETs: [(1, 'terminating a pregnancy'), (1, 'under the weather'), (0, 'overweight'), (1, 'street person'), (0, 'to go to heaven'), (1, 'sex worker'), (0, 'aging'), (1, 'portly'), (1, 'ethnic cleansing'), (0, 'stout'), (0, 'passed away'), (0, 'between jobs'), (0, 'slept with'), (1, 'time of the month'), (1, 'lavatory'), (1, 'substance abuse'), (0, 'a certain age'), (1, 'hearing impaired'), (1, 'pass gas'), (0, 'gluteus maximus'), (0, 'expecting'), (1, 'rear end'), (0, 'with child'), (0, 'dismissed'), (1, 'pro-life'), (1, 'enhanced interrogation techniques'), (1, 'correctional facility'), (1, 'tinkle'), (0, 'seasoned'), (0, 'exterminate'), (1, 'substance abusers'), (1, 'made love'), (1, 'elderly'), (1, 'underprivileged'), (1, 'advanced age'), (1, 'fatalities'), (1, 'capital punishment'), (0, 'let him go'), (1, 'undocumented workers'), (1, 'less fortunate'), (1, 'latrine'), (1, 'drinking problem'), (0, 'weed'), (0, 'passing on'), (1, 'low-income'), (0, 'perish'), (1, 'dearly departed'), (0, 'went to heaven'), (1, 'indigent'), (1, 'adult beverages'), (1, 'birds and the bees'), (1, 'inebriated'), (0, 'wealthy'), (0, 'mixed up'), (0, 'demise'), (0, 'plump'), (1, 'plus-sized'), (0, 'same-sex'), (1, 'undocumented immigrants'), (0, 'neutralize'), (1, 'same sex'), (1, 'deceased'), (0, 'well off'), (0, 'late'), (0, 'chest'), (0, 'let go of'), (0, 'put to sleep'), (0, 'intoxicated'), (0, 'deprived')]\n",
      "\n",
      "20 MED SIM PETs: [(0, 'sleep around'), (0, 'custodian'), (0, 'sober'), (0, 'seeing someone'), (1, 'venereal diseases'), (1, 'pro-choice'), (1, 'detention camp'), (1, 'targeted killings'), (1, 'detainee'), (0, 'oldest profession'), (0, 'got clean'), (1, 'homemaker'), (0, 'economical'), (1, 'senior citizen'), (1, 'lose your lunch'), (1, 'droppings'), (1, 'people of color'), (1, 'mistruth'), (1, 'psychiatric hospital'), (0, 'slim')]\n",
      "\n",
      "40 LOW SIM PETs: [(1, 'golden years'), (1, 'developmentally disabled'), (0, 'getting clean'), (1, 'able-bodied'), (1, 'physically challenged'), (0, 'underdeveloped'), (0, 'special needs'), (0, 'over the hill'), (1, 'broken home'), (0, 'disadvantaged'), (1, 'differently-abled'), (0, 'accident'), (1, 'mentally disabled'), (0, 'troubled'), (1, 'income inequality'), (1, 'developing country'), (1, 'freedom fighters'), (1, 'mentally challenged'), (1, 'pre-owned'), (1, 'economical with the truth'), (1, 'comfort women'), (0, 'outspoken'), (1, 'sanitation workers'), (0, 'downsize'), (1, 'custodians'), (0, 'long sleep'), (1, 'inner city'), (0, 'lay off'), (1, 'armed conflict'), (0, 'collateral damage'), (0, 'invalid'), (1, 'running behind'), (0, 'experienced'), (1, 'negative cash flow'), (1, 'global south'), (0, 'regime change'), (0, 'go all the way'), (1, 'full figured'), (0, 'disabled'), (0, 'outlived her usefulness')]\n"
     ]
    }
   ],
   "source": [
    "# output the PETs from (theoretically) less vague to vague based on 2 thresholds of similarity (for euphemistic examples)\n",
    "high_sim_PETs = []\n",
    "low_sim_PETs = []\n",
    "med_sim_PETs = []\n",
    "\n",
    "high_threshold = 0.7\n",
    "low_threshold = 0.6\n",
    "for i, row in annotations.iterrows():\n",
    "    # for now, only look at euphemistic examples\n",
    "    if (annotations.loc[i, 'label'] == 0):\n",
    "        continue\n",
    "    \n",
    "    euph_status = 1 if (euph_corpus.loc[i, 'euph_status'] == \"always_euph\") else 0 # no apparent pattern to whether a euph is sometimes_euph and whether it's vague\n",
    "    PET = (euph_status, annotations.loc[i, 'keyword'])\n",
    "    if (annotations.loc[i, 'semantic_similarity'] > high_threshold):\n",
    "        high_sim_PETs.append(PET)\n",
    "    elif (annotations.loc[i, 'semantic_similarity'] < low_threshold):\n",
    "        low_sim_PETs.append(PET)\n",
    "    else:\n",
    "        med_sim_PETs.append(PET)\n",
    "    \n",
    "print(\"{} HIGH SIM PETs: {}\\n\".format(len(high_sim_PETs), high_sim_PETs))\n",
    "print(\"{} MED SIM PETs: {}\\n\".format(len(med_sim_PETs), med_sim_PETs))\n",
    "print(\"{} LOW SIM PETs: {}\".format(len(low_sim_PETs), low_sim_PETs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31cace60-b7cb-46b4-9961-637a31522ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 HIGH SIM PETs: ['plump', 'stout', 'same-sex', 'seasoned', 'overweight', 'wealthy', 'aging', 'exterminate', 'chest', 'sleep with', 'getting clean']\n",
      "\n",
      "8 MED SIM PETs: ['disabled', 'between jobs', 'pass away', 'expecting', 'sleep around', 'seeing each other', 'a certain age', 'with child']\n",
      "\n",
      "39 LOW SIM PETs: ['outspoken', 'economical', 'custodian', 'got clean', 'demise', 'neutralize', 'went to heaven', 'perish', 'over the hill', 'let them go', 'put to sleep', 'weed', 'outlived their usefulness', 'collateral damage', 'lay off', 'accident', 'passing on', 'downsize', 'regime change', 'special needs', 'mixed up', 'intoxicated', 'disadvantaged', 'sober', 'slim', 'invalid', 'experienced', 'oldest profession', 'go all the way', 'troubled', 'long sleep', 'let go of', 'dismissed', 'late', 'well off', 'gluteus maximus', 'to go to heaven', 'deprived', 'underdeveloped']\n"
     ]
    }
   ],
   "source": [
    "high_sim_PETs = []\n",
    "low_sim_PETs = []\n",
    "med_sim_PETs = []\n",
    "\n",
    "high_threshold = 0.7\n",
    "low_threshold = 0.6\n",
    "for i, row in annotations.iterrows():\n",
    "    # for now, only look at euphemistic examples\n",
    "    if (annotations.loc[i, 'label'] == 1):\n",
    "        continue\n",
    "    PET = annotations.loc[i, 'keyword']\n",
    "    if (annotations.loc[i, 'semantic_similarity'] > high_threshold):\n",
    "        high_sim_PETs.append(PET)\n",
    "    elif (annotations.loc[i, 'semantic_similarity'] < low_threshold):\n",
    "        low_sim_PETs.append(PET)\n",
    "    else:\n",
    "        med_sim_PETs.append(PET)\n",
    "    \n",
    "print(\"{} HIGH SIM PETs: {}\\n\".format(len(high_sim_PETs), high_sim_PETs))\n",
    "print(\"{} MED SIM PETs: {}\\n\".format(len(med_sim_PETs), med_sim_PETs))\n",
    "print(\"{} LOW SIM PETs: {}\".format(len(low_sim_PETs), low_sim_PETs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece62101-611b-4f0f-b380-f75933920499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35d93ace-47c6-41d7-9d8f-cd87a850a0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.6795, 0.7529]), tensor([0.6519, 0.8765])]\n"
     ]
    }
   ],
   "source": [
    "from parascore import ParaScorer\n",
    "scorer = ParaScorer(lang=\"en\", model_type = 'bert-base-uncased')\n",
    "cands = [\"A young person is skating.\", \"Terminating a pregnancy is murder.\"]\n",
    "sources = [\"There's a child on a skateboard.\", \"Abortion is murder.\"]\n",
    "refs = [\"A kid is skateboarding.\"]\n",
    "# score = scorer.base_score(cands, sources, refs, batch_size=16)\n",
    "score = scorer.free_score(cands, sources, batch_size=16)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ef9b8-442c-42d3-b959-27a1783a7c12",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Secondary Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ff16ac-ef25-43c1-8eb4-b9abbe5186dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tinkle</td>\n",
       "      <td>We're just getting back what was TAKEN from us...</td>\n",
       "      <td>1</td>\n",
       "      <td>bodily functions</td>\n",
       "      <td>tinkle</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>We're just getting back what was TAKEN from us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>The Arizona Republic reports that young &lt;undoc...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>The Arizona Republic reports that young undocu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>venereal disease</td>\n",
       "      <td>He's being sued by a woman who claims he gave ...</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>venereal disease</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>And Kris Humphries plans on being' relentless'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sex worker</td>\n",
       "      <td>I cofounded a magazine by and for sex workers....</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sex worker</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I found community in the sex worker rights mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>mentally disabled</td>\n",
       "      <td>Thank you so much for sharing your story and y...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>mentally disabled</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I think though that you assume that those that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>troubled</td>\n",
       "      <td>Speaking to the Santa Clara County Board of Su...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>troubled</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Sara Cody health officer for Santa Clara Count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>seeing each other</td>\n",
       "      <td>This guy replies with an awesome e-mail and wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>seeing someone/each other</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>We clicked immediately and have been seeing ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>seasoned</td>\n",
       "      <td>I am-- it depends on the location, though. Bec...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>seasoned</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Because in Miami for example and in Richmond V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>slept with</td>\n",
       "      <td>Calves and baby goats were sheltered in the fr...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep with</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>The other two rooms were the father's sleeping...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>with child</td>\n",
       "      <td>Nickname of a girl named Diana. 41. What you d...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>with child</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>What you do with Child Life 42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "0                      tinkle   \n",
       "16    undocumented immigrants   \n",
       "23           venereal disease   \n",
       "34                 sex worker   \n",
       "57          mentally disabled   \n",
       "...                       ...   \n",
       "1938                 troubled   \n",
       "1953        seeing each other   \n",
       "1954                 seasoned   \n",
       "1956               slept with   \n",
       "1964               with child   \n",
       "\n",
       "                                            edited_text  is_euph  \\\n",
       "0     We're just getting back what was TAKEN from us...        1   \n",
       "16    The Arizona Republic reports that young <undoc...        1   \n",
       "23    He's being sued by a woman who claims he gave ...        1   \n",
       "34    I cofounded a magazine by and for sex workers....        1   \n",
       "57    Thank you so much for sharing your story and y...        1   \n",
       "...                                                 ...      ...   \n",
       "1938  Speaking to the Santa Clara County Board of Su...        0   \n",
       "1953  This guy replies with an awesome e-mail and wa...        0   \n",
       "1954  I am-- it depends on the location, though. Bec...        0   \n",
       "1956  Calves and baby goats were sheltered in the fr...        0   \n",
       "1964  Nickname of a girl named Diana. 41. What you d...        0   \n",
       "\n",
       "                        category                       type     euph_status  \\\n",
       "0               bodily functions                     tinkle     always_euph   \n",
       "16                      politics     undocumented immigrant     always_euph   \n",
       "23               sexual activity           venereal disease     always_euph   \n",
       "34               sexual activity                 sex worker     always_euph   \n",
       "57    physical/mental attributes          mentally disabled     always_euph   \n",
       "...                          ...                        ...             ...   \n",
       "1938  physical/mental attributes                   troubled  sometimes_euph   \n",
       "1953             sexual activity  seeing someone/each other  sometimes_euph   \n",
       "1954  physical/mental attributes                   seasoned  sometimes_euph   \n",
       "1956             sexual activity                 sleep with  sometimes_euph   \n",
       "1964  physical/mental attributes                 with child  sometimes_euph   \n",
       "\n",
       "                                               sentence  \n",
       "0     We're just getting back what was TAKEN from us...  \n",
       "16    The Arizona Republic reports that young undocu...  \n",
       "23    And Kris Humphries plans on being' relentless'...  \n",
       "34    I found community in the sex worker rights mov...  \n",
       "57    I think though that you assume that those that...  \n",
       "...                                                 ...  \n",
       "1938  Sara Cody health officer for Santa Clara Count...  \n",
       "1953  We clicked immediately and have been seeing ea...  \n",
       "1954  Because in Miami for example and in Richmond V...  \n",
       "1956  The other two rooms were the father's sleeping...  \n",
       "1964                    What you do with Child Life 42   \n",
       "\n",
       "[165 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_sample = pd.read_csv('Pilot_Annotation_Sample.csv', index_col = 0)\n",
    "euph_corpus = pd.read_csv('Euphemism_Corpus_2-24.csv', index_col = 0)\n",
    "\n",
    "print(len(np.unique(euph_corpus['type'])))\n",
    "remaining = euph_corpus.drop(first_sample.index)\n",
    "always_euphs = np.unique((remaining.loc[remaining['euph_status']=='always_euph'])['type'])\n",
    "sometimes_euphs = np.unique((remaining.loc[remaining['euph_status']=='sometimes_euph'])['type'])\n",
    "\n",
    "selected = []\n",
    "\n",
    "for PET in always_euphs:\n",
    "    matches = remaining[remaining['type'] == PET]\n",
    "    selection = matches.sample(n=1).index # randomly select an index\n",
    "    selected.append(selection)\n",
    "\n",
    "for PET in sometimes_euphs:\n",
    "    lit_matches = remaining[(remaining['type'] == PET) & (remaining['is_euph'] == 0)]\n",
    "    euph_matches = remaining[(remaining['type'] == PET) & (remaining['is_euph'] == 1)]\n",
    "    lit_selection = lit_matches.sample(n=1).index # randomly select an index\n",
    "    euph_selection = euph_matches.sample(n=1).index\n",
    "    selected.append(lit_selection)\n",
    "    selected.append(euph_selection)\n",
    "\n",
    "# idk, some number wrangling to convert from .index, which returns Int64 lists/\n",
    "selected = np.array(selected).tolist()\n",
    "selected = [item for sublist in selected for item in sublist]\n",
    "\n",
    "remaining = remaining[remaining.index.isin(selected)]\n",
    "display(remaining)\n",
    "remaining.to_csv('Annotation_Sample_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac6914-2ec2-4a21-a4a5-f52dcdde8d99",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vagueness Extrapolation - Strong Assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384b3a51-8a99-46ac-9305-17d421d9a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# SINCE THE LAST TIME THE FILE \"Annotation_Task1_Analysis.csv\" WAS REFERENCED...\n",
    "# ..I have manually (using Excel) put a \"1\" for Vague for paraphrase similarity scores < 0.6, \"0\" for > 0.7, and my own judgments for in between (Experimental: <0.63=vague)\n",
    "annotations = pd.read_csv(\"Annotation_Task1_Analysis.csv\", index_col = 0, encoding= 'utf-8')\n",
    "euph_corpus = pd.read_csv('Euphemism_Corpus_v2.1.csv', index_col = 0, encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffd5af2-a5eb-49fb-a99a-187c62836d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VET_dict = {} # will contain a dict for each PET and its vagueness label. If PET is sometimes_euph, dict will have 2 keys: '1' is euphemistic use and '0' literal\n",
    "missing = i\n",
    "for i, row in annotations.iterrows():\n",
    "    # if (i == 1807):\n",
    "    #     VET_dict['exterminate'][0] = 0\n",
    "    #     continue\n",
    "    keyword = euph_corpus.loc[i, 'type']\n",
    "    label = annotations.loc[i, 'label']\n",
    "    is_vague = annotations.loc[i, 'is_vague']\n",
    "    if (keyword not in VET_dict):\n",
    "        VET_dict[keyword] = {}\n",
    "    VET_dict[keyword][label] = is_vague\n",
    "\n",
    "# VET_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5352ba55-d696-451c-abce-08c6414972e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_vague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tinkle</td>\n",
       "      <td>We're just getting back what was TAKEN from us...</td>\n",
       "      <td>1</td>\n",
       "      <td>body functions/parts</td>\n",
       "      <td>tinkle</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>We're just getting back what was TAKEN from us...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tinkle</td>\n",
       "      <td>I think AB390 will pass next year now that the...</td>\n",
       "      <td>1</td>\n",
       "      <td>body functions/parts</td>\n",
       "      <td>tinkle</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I think AB390 will pass next year now that the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>Singled Out Think Like a Man, the new movie ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Anything but Secure A federal program designed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>Not to be outdone, Sen. Rand Paul (R-Ky. ), so...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>In a post-election interview with POLITICO Pau...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>The law has also galvanized the growing immigr...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Aside from undocumented immigrants the America...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>sleep with</td>\n",
       "      <td>There were other photos she wanted me to see: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep with</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>There were other photos she wanted me to see B...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>sleep with</td>\n",
       "      <td>I am relieved to see two pup tents marked STAF...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep with</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Thank God I don't have to sleep with Ace Wands</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>sleep around</td>\n",
       "      <td>Nothing serious, just long nights of me hackin...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep around</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>With all my caterwauling it's a wonder anyone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>with child</td>\n",
       "      <td>sounds more like Jonestown. They cant leave @ ...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>with child</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>They cant leave best advice I can give them is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>with child</td>\n",
       "      <td>Nickname of a girl named Diana. 41. What you d...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>with child</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>What you do with Child Life 42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1952 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "0                      tinkle   \n",
       "1                      tinkle   \n",
       "2     undocumented immigrants   \n",
       "3     undocumented immigrants   \n",
       "4     undocumented immigrants   \n",
       "...                       ...   \n",
       "1960               sleep with   \n",
       "1961               sleep with   \n",
       "1962             sleep around   \n",
       "1963               with child   \n",
       "1964               with child   \n",
       "\n",
       "                                            edited_text  is_euph  \\\n",
       "0     We're just getting back what was TAKEN from us...        1   \n",
       "1     I think AB390 will pass next year now that the...        1   \n",
       "2     Singled Out Think Like a Man, the new movie ba...        1   \n",
       "3     Not to be outdone, Sen. Rand Paul (R-Ky. ), so...        1   \n",
       "4     The law has also galvanized the growing immigr...        1   \n",
       "...                                                 ...      ...   \n",
       "1960  There were other photos she wanted me to see: ...        0   \n",
       "1961  I am relieved to see two pup tents marked STAF...        0   \n",
       "1962  Nothing serious, just long nights of me hackin...        0   \n",
       "1963  sounds more like Jonestown. They cant leave @ ...        0   \n",
       "1964  Nickname of a girl named Diana. 41. What you d...        0   \n",
       "\n",
       "                        category                    type     euph_status  \\\n",
       "0           body functions/parts                  tinkle     always_euph   \n",
       "1           body functions/parts                  tinkle     always_euph   \n",
       "2                       politics  undocumented immigrant     always_euph   \n",
       "3                       politics  undocumented immigrant     always_euph   \n",
       "4                       politics  undocumented immigrant     always_euph   \n",
       "...                          ...                     ...             ...   \n",
       "1960             sexual activity              sleep with  sometimes_euph   \n",
       "1961             sexual activity              sleep with  sometimes_euph   \n",
       "1962             sexual activity            sleep around  sometimes_euph   \n",
       "1963  physical/mental attributes              with child  sometimes_euph   \n",
       "1964  physical/mental attributes              with child  sometimes_euph   \n",
       "\n",
       "                                               sentence  is_vague  \n",
       "0     We're just getting back what was TAKEN from us...         0  \n",
       "1     I think AB390 will pass next year now that the...         0  \n",
       "2     Anything but Secure A federal program designed...         0  \n",
       "3     In a post-election interview with POLITICO Pau...         0  \n",
       "4     Aside from undocumented immigrants the America...         0  \n",
       "...                                                 ...       ...  \n",
       "1960  There were other photos she wanted me to see B...         0  \n",
       "1961    Thank God I don't have to sleep with Ace Wands          0  \n",
       "1962  With all my caterwauling it's a wonder anyone ...         0  \n",
       "1963  They cant leave best advice I can give them is...         0  \n",
       "1964                    What you do with Child Life 42          0  \n",
       "\n",
       "[1952 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now apply to rest of corpus\n",
    "euph_corpus['is_vague'] = -1\n",
    "\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    PET = euph_corpus.loc[i, 'type']\n",
    "    label = euph_corpus.loc[i, 'is_euph']\n",
    "    is_vague = VET_dict[PET][label]\n",
    "    euph_corpus.loc[i, 'is_vague'] = is_vague\n",
    "\n",
    "euph_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "254fff14-6979-4327-9603-ec8a0a8b4374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 769 vague examples. Of these, 408 are 1s and 361 are 0s\n",
      "There are 1183 un-vague examples. Of these, 975 are 1s and 208 are 0s\n"
     ]
    }
   ],
   "source": [
    "vague_examples = euph_corpus.loc[euph_corpus['is_vague']==1]\n",
    "unvague_examples = euph_corpus.loc[euph_corpus['is_vague']==0]\n",
    "\n",
    "vague_1s = vague_examples.loc[vague_examples['is_euph']==1]\n",
    "vague_0s = vague_examples.loc[vague_examples['is_euph']==0]\n",
    "unvague_1s = unvague_examples.loc[unvague_examples['is_euph']==1]\n",
    "unvague_0s = unvague_examples.loc[unvague_examples['is_euph']==0]\n",
    "\n",
    "print(\"There are {} vague examples. Of these, {} are 1s and {} are 0s\".format(\n",
    "    len(vague_examples), len(vague_1s), len(vague_0s)))\n",
    "print(\"There are {} un-vague examples. Of these, {} are 1s and {} are 0s\".format(\n",
    "    len(unvague_examples), len(unvague_1s), len(unvague_0s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "583012f1-76c2-4909-84ea-67857175a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "euph_corpus.to_csv(\"VET_Corpus_0.2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35075379-cad9-4d55-92f9-859c12362ffe",
   "metadata": {},
   "source": [
    "### Vagueness Extrapolation - Weak Assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71461b18-b393-47ec-a1d4-da707d8f3a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>paraphrase1</th>\n",
       "      <th>paraphrase2</th>\n",
       "      <th>paraphrase3</th>\n",
       "      <th>paraphrase4</th>\n",
       "      <th>paraphrase5</th>\n",
       "      <th>paraphrase6</th>\n",
       "      <th>label</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>human_sim</th>\n",
       "      <th>is_vague</th>\n",
       "      <th>changes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>terminating a pregnancy</td>\n",
       "      <td>There's no difference. &lt;Terminating a pregnanc...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>abortion</td>\n",
       "      <td>abortion</td>\n",
       "      <td>abortion</td>\n",
       "      <td>abortion</td>\n",
       "      <td>Abortion</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>sleep around</td>\n",
       "      <td>I would too, just as I would for any other gir...</td>\n",
       "      <td>have sexual relationships with multiple people</td>\n",
       "      <td>have sex with a lot of people</td>\n",
       "      <td>practice adultery</td>\n",
       "      <td>have sex with many partners</td>\n",
       "      <td>have casual sex</td>\n",
       "      <td>have sex with many people</td>\n",
       "      <td>1</td>\n",
       "      <td>0.605805</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>custodian</td>\n",
       "      <td>Only one Princeton faculty member, James Shinn...</td>\n",
       "      <td>janitor</td>\n",
       "      <td>cleaner</td>\n",
       "      <td>janitor</td>\n",
       "      <td>janitor</td>\n",
       "      <td>custodian</td>\n",
       "      <td>janitor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.662722</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>under the weather</td>\n",
       "      <td>We don't know how bad things will get, Morocco...</td>\n",
       "      <td>sick</td>\n",
       "      <td>sick</td>\n",
       "      <td>sick</td>\n",
       "      <td>ill</td>\n",
       "      <td>sick</td>\n",
       "      <td>sick</td>\n",
       "      <td>1</td>\n",
       "      <td>0.853640</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>overweight</td>\n",
       "      <td>Obesity is seen as the great pandemia of the X...</td>\n",
       "      <td>being fat</td>\n",
       "      <td>obesity</td>\n",
       "      <td>fatness</td>\n",
       "      <td>being fat</td>\n",
       "      <td>overweight</td>\n",
       "      <td>being fat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.740032</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>same sex</td>\n",
       "      <td>Multiple \"partners\" as you call it was NEVER i...</td>\n",
       "      <td>gay</td>\n",
       "      <td>gay</td>\n",
       "      <td>same sex</td>\n",
       "      <td>homosexual</td>\n",
       "      <td>gay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>inebriated</td>\n",
       "      <td>In a few weeks, tens of thousands of college f...</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>intoxicated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>overweight</td>\n",
       "      <td>Also, there's a good bit of mounting evidence ...</td>\n",
       "      <td>fat</td>\n",
       "      <td>fat</td>\n",
       "      <td>overweight</td>\n",
       "      <td>fat</td>\n",
       "      <td>fat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.867237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>demise</td>\n",
       "      <td>At the Vatican, following the &lt;demise&gt; of the ...</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>fall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>The Arizona Republic reports that young &lt;undoc...</td>\n",
       "      <td>illegals</td>\n",
       "      <td>illegal immigrants</td>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>illegal immigrants</td>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.827272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "465   terminating a pregnancy   \n",
       "1347             sleep around   \n",
       "1188                custodian   \n",
       "565         under the weather   \n",
       "847                overweight   \n",
       "...                       ...   \n",
       "554                  same sex   \n",
       "378                inebriated   \n",
       "856                overweight   \n",
       "902                    demise   \n",
       "16    undocumented immigrants   \n",
       "\n",
       "                                                   text  \\\n",
       "465   There's no difference. <Terminating a pregnanc...   \n",
       "1347  I would too, just as I would for any other gir...   \n",
       "1188  Only one Princeton faculty member, James Shinn...   \n",
       "565   We don't know how bad things will get, Morocco...   \n",
       "847   Obesity is seen as the great pandemia of the X...   \n",
       "...                                                 ...   \n",
       "554   Multiple \"partners\" as you call it was NEVER i...   \n",
       "378   In a few weeks, tens of thousands of college f...   \n",
       "856   Also, there's a good bit of mounting evidence ...   \n",
       "902   At the Vatican, following the <demise> of the ...   \n",
       "16    The Arizona Republic reports that young <undoc...   \n",
       "\n",
       "                                          paraphrase1  \\\n",
       "465                                          abortion   \n",
       "1347  have sexual relationships with multiple people    \n",
       "1188                                          janitor   \n",
       "565                                              sick   \n",
       "847                                         being fat   \n",
       "...                                               ...   \n",
       "554                                               gay   \n",
       "378                                             drunk   \n",
       "856                                               fat   \n",
       "902                                             death   \n",
       "16                                           illegals   \n",
       "\n",
       "                        paraphrase2              paraphrase3  \\\n",
       "465                        abortion                 abortion   \n",
       "1347  have sex with a lot of people        practice adultery   \n",
       "1188                        cleaner                  janitor   \n",
       "565                            sick                     sick   \n",
       "847                         obesity                  fatness   \n",
       "...                             ...                      ...   \n",
       "554                             gay                 same sex   \n",
       "378                           drunk                    drunk   \n",
       "856                             fat               overweight   \n",
       "902                           death                    death   \n",
       "16               illegal immigrants  undocumented immigrants   \n",
       "\n",
       "                      paraphrase4              paraphrase5  \\\n",
       "465                      abortion                 abortion   \n",
       "1347  have sex with many partners          have casual sex   \n",
       "1188                      janitor                custodian   \n",
       "565                           ill                     sick   \n",
       "847                     being fat               overweight   \n",
       "...                           ...                      ...   \n",
       "554                    homosexual                      gay   \n",
       "378                         drunk              intoxicated   \n",
       "856                           fat                      fat   \n",
       "902                         death                     fall   \n",
       "16             illegal immigrants  undocumented immigrants   \n",
       "\n",
       "                    paraphrase6  label  semantic_similarity  human_sim  \\\n",
       "465                    Abortion      1             1.000000        5.0   \n",
       "1347  have sex with many people      1             0.605805        3.0   \n",
       "1188                    janitor      1             0.662722        4.0   \n",
       "565                        sick      1             0.853640        4.0   \n",
       "847                   being fat      1             0.740032        3.0   \n",
       "...                         ...    ...                  ...        ...   \n",
       "554                         NaN      1             0.818027        NaN   \n",
       "378                         NaN      1             0.869052        NaN   \n",
       "856                         NaN      1             0.867237        NaN   \n",
       "902                         NaN      1             0.751394        NaN   \n",
       "16                          NaN      1             0.827272        NaN   \n",
       "\n",
       "      is_vague  changes  \n",
       "465          0      NaN  \n",
       "1347         1      NaN  \n",
       "1188         0      NaN  \n",
       "565          0      NaN  \n",
       "847          0      NaN  \n",
       "...        ...      ...  \n",
       "554          0      NaN  \n",
       "378          0      NaN  \n",
       "856          0      NaN  \n",
       "902          0      NaN  \n",
       "16           0      NaN  \n",
       "\n",
       "[352 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, we need to get both annotations in one file\n",
    "import pandas as pd\n",
    "\n",
    "a1 = pd.read_csv(\"Annotation_Task1_Analysis_v2.csv\", index_col=0)\n",
    "a2 = pd.read_csv(\"Annotation_Task2_Analysis.csv\", index_col=0)\n",
    "\n",
    "a3 = pd.concat([a1, a2])\n",
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b407f6-74b2-49ed-8402-b8c0321c2d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>paraphrase1</th>\n",
       "      <th>paraphrase2</th>\n",
       "      <th>paraphrase3</th>\n",
       "      <th>paraphrase4</th>\n",
       "      <th>paraphrase5</th>\n",
       "      <th>paraphrase6</th>\n",
       "      <th>label</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>human_sim</th>\n",
       "      <th>is_vague</th>\n",
       "      <th>changes</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>terminating a pregnancy</td>\n",
       "      <td>There's no difference. &lt;Terminating a pregnanc...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>abortion</td>\n",
       "      <td>abortion</td>\n",
       "      <td>abortion</td>\n",
       "      <td>abortion</td>\n",
       "      <td>Abortion</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pregnancy termination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>sleep around</td>\n",
       "      <td>I would too, just as I would for any other gir...</td>\n",
       "      <td>have sexual relationships with multiple people</td>\n",
       "      <td>have sex with a lot of people</td>\n",
       "      <td>practice adultery</td>\n",
       "      <td>have sex with many partners</td>\n",
       "      <td>have casual sex</td>\n",
       "      <td>have sex with many people</td>\n",
       "      <td>1</td>\n",
       "      <td>0.605805</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sleep around</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>custodian</td>\n",
       "      <td>Only one Princeton faculty member, James Shinn...</td>\n",
       "      <td>janitor</td>\n",
       "      <td>cleaner</td>\n",
       "      <td>janitor</td>\n",
       "      <td>janitor</td>\n",
       "      <td>custodian</td>\n",
       "      <td>janitor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.662722</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>custodian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>under the weather</td>\n",
       "      <td>We don't know how bad things will get, Morocco...</td>\n",
       "      <td>sick</td>\n",
       "      <td>sick</td>\n",
       "      <td>sick</td>\n",
       "      <td>ill</td>\n",
       "      <td>sick</td>\n",
       "      <td>sick</td>\n",
       "      <td>1</td>\n",
       "      <td>0.853640</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>under the weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>overweight</td>\n",
       "      <td>Obesity is seen as the great pandemia of the X...</td>\n",
       "      <td>being fat</td>\n",
       "      <td>obesity</td>\n",
       "      <td>fatness</td>\n",
       "      <td>being fat</td>\n",
       "      <td>overweight</td>\n",
       "      <td>being fat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.740032</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>same sex</td>\n",
       "      <td>Multiple \"partners\" as you call it was NEVER i...</td>\n",
       "      <td>gay</td>\n",
       "      <td>gay</td>\n",
       "      <td>same sex</td>\n",
       "      <td>homosexual</td>\n",
       "      <td>gay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>same sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>inebriated</td>\n",
       "      <td>In a few weeks, tens of thousands of college f...</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>drunk</td>\n",
       "      <td>intoxicated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inebriated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>overweight</td>\n",
       "      <td>Also, there's a good bit of mounting evidence ...</td>\n",
       "      <td>fat</td>\n",
       "      <td>fat</td>\n",
       "      <td>overweight</td>\n",
       "      <td>fat</td>\n",
       "      <td>fat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.867237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>demise</td>\n",
       "      <td>At the Vatican, following the &lt;demise&gt; of the ...</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>death</td>\n",
       "      <td>fall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>demise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>The Arizona Republic reports that young &lt;undoc...</td>\n",
       "      <td>illegals</td>\n",
       "      <td>illegal immigrants</td>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>illegal immigrants</td>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.827272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "465   terminating a pregnancy   \n",
       "1347             sleep around   \n",
       "1188                custodian   \n",
       "565         under the weather   \n",
       "847                overweight   \n",
       "...                       ...   \n",
       "554                  same sex   \n",
       "378                inebriated   \n",
       "856                overweight   \n",
       "902                    demise   \n",
       "16    undocumented immigrants   \n",
       "\n",
       "                                                   text  \\\n",
       "465   There's no difference. <Terminating a pregnanc...   \n",
       "1347  I would too, just as I would for any other gir...   \n",
       "1188  Only one Princeton faculty member, James Shinn...   \n",
       "565   We don't know how bad things will get, Morocco...   \n",
       "847   Obesity is seen as the great pandemia of the X...   \n",
       "...                                                 ...   \n",
       "554   Multiple \"partners\" as you call it was NEVER i...   \n",
       "378   In a few weeks, tens of thousands of college f...   \n",
       "856   Also, there's a good bit of mounting evidence ...   \n",
       "902   At the Vatican, following the <demise> of the ...   \n",
       "16    The Arizona Republic reports that young <undoc...   \n",
       "\n",
       "                                          paraphrase1  \\\n",
       "465                                          abortion   \n",
       "1347  have sexual relationships with multiple people    \n",
       "1188                                          janitor   \n",
       "565                                              sick   \n",
       "847                                         being fat   \n",
       "...                                               ...   \n",
       "554                                               gay   \n",
       "378                                             drunk   \n",
       "856                                               fat   \n",
       "902                                             death   \n",
       "16                                           illegals   \n",
       "\n",
       "                        paraphrase2              paraphrase3  \\\n",
       "465                        abortion                 abortion   \n",
       "1347  have sex with a lot of people        practice adultery   \n",
       "1188                        cleaner                  janitor   \n",
       "565                            sick                     sick   \n",
       "847                         obesity                  fatness   \n",
       "...                             ...                      ...   \n",
       "554                             gay                 same sex   \n",
       "378                           drunk                    drunk   \n",
       "856                             fat               overweight   \n",
       "902                           death                    death   \n",
       "16               illegal immigrants  undocumented immigrants   \n",
       "\n",
       "                      paraphrase4              paraphrase5  \\\n",
       "465                      abortion                 abortion   \n",
       "1347  have sex with many partners          have casual sex   \n",
       "1188                      janitor                custodian   \n",
       "565                           ill                     sick   \n",
       "847                     being fat               overweight   \n",
       "...                           ...                      ...   \n",
       "554                    homosexual                      gay   \n",
       "378                         drunk              intoxicated   \n",
       "856                           fat                      fat   \n",
       "902                         death                     fall   \n",
       "16             illegal immigrants  undocumented immigrants   \n",
       "\n",
       "                    paraphrase6  label  semantic_similarity  human_sim  \\\n",
       "465                    Abortion      1             1.000000        5.0   \n",
       "1347  have sex with many people      1             0.605805        3.0   \n",
       "1188                    janitor      1             0.662722        4.0   \n",
       "565                        sick      1             0.853640        4.0   \n",
       "847                   being fat      1             0.740032        3.0   \n",
       "...                         ...    ...                  ...        ...   \n",
       "554                         NaN      1             0.818027        NaN   \n",
       "378                         NaN      1             0.869052        NaN   \n",
       "856                         NaN      1             0.867237        NaN   \n",
       "902                         NaN      1             0.751394        NaN   \n",
       "16                          NaN      1             0.827272        NaN   \n",
       "\n",
       "      is_vague  changes                    type  \n",
       "465          0      NaN   pregnancy termination  \n",
       "1347         1      NaN            sleep around  \n",
       "1188         0      NaN               custodian  \n",
       "565          0      NaN       under the weather  \n",
       "847          0      NaN              overweight  \n",
       "...        ...      ...                     ...  \n",
       "554          0      NaN                same sex  \n",
       "378          0      NaN              inebriated  \n",
       "856          0      NaN              overweight  \n",
       "902          0      NaN                  demise  \n",
       "16           0      NaN  undocumented immigrant  \n",
       "\n",
       "[352 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve types \n",
    "euph_corpus = pd.read_csv('Euphemism_Corpus_2-24.csv', index_col = 0, encoding= 'utf-8')\n",
    "a3['type'] = \"\"\n",
    "for i, row in a3.iterrows():\n",
    "    PET = euph_corpus.loc[i, 'type'] # retrieve type for this index\n",
    "    a3.loc[i, 'type'] = PET\n",
    "    \n",
    "a3\n",
    "# a3.to_csv(\"Weak_Assumption_Corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e718a703-48fd-4bb3-86ba-0c33e7db7c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cda91a654ff43fdb6c711b0d08270dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_vague</th>\n",
       "      <th>highest_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tinkle</td>\n",
       "      <td>We're just getting back what was TAKEN from us...</td>\n",
       "      <td>1</td>\n",
       "      <td>bodily functions</td>\n",
       "      <td>tinkle</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>We're just getting back what was TAKEN from us...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tinkle</td>\n",
       "      <td>I think AB390 will pass next year now that the...</td>\n",
       "      <td>1</td>\n",
       "      <td>bodily functions</td>\n",
       "      <td>tinkle</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I think AB390 will pass next year now that the...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>Singled Out Think Like a Man, the new movie ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Anything but Secure A federal program designed...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.281206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>Not to be outdone, Sen. Rand Paul (R-Ky. ), so...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>In a post-election interview with POLITICO Pau...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.238935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>The law has also galvanized the growing immigr...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Aside from undocumented immigrants the America...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>sleep with</td>\n",
       "      <td>There were other photos she wanted me to see: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep with</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>There were other photos she wanted me to see B...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>sleep with</td>\n",
       "      <td>I am relieved to see two pup tents marked STAF...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep with</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Thank God I don't have to sleep with Ace Wands</td>\n",
       "      <td>0</td>\n",
       "      <td>0.337824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>sleep around</td>\n",
       "      <td>Nothing serious, just long nights of me hackin...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>sleep around</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>With all my caterwauling it's a wonder anyone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>with child</td>\n",
       "      <td>sounds more like Jonestown. They cant leave @ ...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>with child</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>They cant leave best advice I can give them is...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>with child</td>\n",
       "      <td>Nickname of a girl named Diana. 41. What you d...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>with child</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>What you do with Child Life 42</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1965 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "0                      tinkle   \n",
       "1                      tinkle   \n",
       "2     undocumented immigrants   \n",
       "3     undocumented immigrants   \n",
       "4     undocumented immigrants   \n",
       "...                       ...   \n",
       "1960               sleep with   \n",
       "1961               sleep with   \n",
       "1962             sleep around   \n",
       "1963               with child   \n",
       "1964               with child   \n",
       "\n",
       "                                            edited_text  is_euph  \\\n",
       "0     We're just getting back what was TAKEN from us...        1   \n",
       "1     I think AB390 will pass next year now that the...        1   \n",
       "2     Singled Out Think Like a Man, the new movie ba...        1   \n",
       "3     Not to be outdone, Sen. Rand Paul (R-Ky. ), so...        1   \n",
       "4     The law has also galvanized the growing immigr...        1   \n",
       "...                                                 ...      ...   \n",
       "1960  There were other photos she wanted me to see: ...        0   \n",
       "1961  I am relieved to see two pup tents marked STAF...        0   \n",
       "1962  Nothing serious, just long nights of me hackin...        0   \n",
       "1963  sounds more like Jonestown. They cant leave @ ...        0   \n",
       "1964  Nickname of a girl named Diana. 41. What you d...        0   \n",
       "\n",
       "                        category                    type     euph_status  \\\n",
       "0               bodily functions                  tinkle     always_euph   \n",
       "1               bodily functions                  tinkle     always_euph   \n",
       "2                       politics  undocumented immigrant     always_euph   \n",
       "3                       politics  undocumented immigrant     always_euph   \n",
       "4                       politics  undocumented immigrant     always_euph   \n",
       "...                          ...                     ...             ...   \n",
       "1960             sexual activity              sleep with  sometimes_euph   \n",
       "1961             sexual activity              sleep with  sometimes_euph   \n",
       "1962             sexual activity            sleep around  sometimes_euph   \n",
       "1963  physical/mental attributes              with child  sometimes_euph   \n",
       "1964  physical/mental attributes              with child  sometimes_euph   \n",
       "\n",
       "                                               sentence  is_vague  highest_sim  \n",
       "0     We're just getting back what was TAKEN from us...         0     1.000000  \n",
       "1     I think AB390 will pass next year now that the...         0     1.000000  \n",
       "2     Anything but Secure A federal program designed...         0     0.281206  \n",
       "3     In a post-election interview with POLITICO Pau...         0     0.238935  \n",
       "4     Aside from undocumented immigrants the America...         0     1.000001  \n",
       "...                                                 ...       ...          ...  \n",
       "1960  There were other photos she wanted me to see B...         0     1.000000  \n",
       "1961    Thank God I don't have to sleep with Ace Wands          0     0.337824  \n",
       "1962  With all my caterwauling it's a wonder anyone ...         0     1.000000  \n",
       "1963  They cant leave best advice I can give them is...         0     1.000000  \n",
       "1964                    What you do with Child Life 42          0     1.000000  \n",
       "\n",
       "[1965 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "def compute_similarity(s1, s2):\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(s1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(s2, convert_to_tensor=True)\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "    # print(\"Similarity score:\", cosine_scores.item())\n",
    "    return cosine_scores.item()\n",
    "\n",
    "# form weakness corpus 0.1 - using sentence_transformers, copy vagueness label of sentence with highest cos similarity in annotation samples\n",
    "euph_corpus = pd.read_csv('Euphemism_Corpus_2-24.csv', index_col = 0, encoding= 'utf-8')\n",
    "euph_corpus['is_vague'] = -1\n",
    "euph_corpus['highest_sim'] = -1 # represents the highest similarity score with representative example in the vague-annotated data\n",
    "                                # this may be useful for the next assumption level, to decide on a threshold\n",
    "weak_corpus = pd.read_csv(\"Weak_Assumption_Corpus.csv\", index_col = 0)\n",
    "\n",
    "for i, row in tqdm(euph_corpus.iterrows()):\n",
    "    PET = euph_corpus.loc[i, 'type']\n",
    "    label = euph_corpus.loc[i, 'is_euph']\n",
    "    text = euph_corpus.loc[i, 'edited_text'] # this example's text\n",
    "    # print(text)\n",
    "    \n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    sel = weak_corpus.loc[weak_corpus['type']==PET]\n",
    "    examples = sel.loc[sel['label']==label]\n",
    "    # display(examples)\n",
    "    \n",
    "    highest_sim = -1\n",
    "    highest_sim_label = -1\n",
    "    for j, row in examples.iterrows():\n",
    "        # print(examples.loc[j, 'text'])\n",
    "        sim = compute_similarity(text, examples.loc[j, 'text'])\n",
    "        if (sim > highest_sim):\n",
    "            highest_sim = sim\n",
    "            highest_sim_label = examples.loc[j, 'is_vague'] # update vagueness label to match highest sim example\n",
    "    euph_corpus.loc[i, 'is_vague'] = highest_sim_label # set vagueness label\n",
    "    euph_corpus.loc[i, 'highest_sim'] = highest_sim \n",
    "\n",
    "euph_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69afb4a1-66d9-4982-b7ce-08c809f8725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "euph_corpus.to_csv(\"VET_Corpus_1.0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c736bf4c-7bce-4c89-91fc-42bb5dd8226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386\n",
      "305\n",
      "996\n",
      "278\n"
     ]
    }
   ],
   "source": [
    "# analyze \n",
    "vet_corpus = pd.read_csv(\"VET_Corpus_1.0.csv\")\n",
    "vague_examples = vet_corpus.loc[vet_corpus['is_vague']==1]\n",
    "unvague_examples = vet_corpus.loc[vet_corpus['is_vague']==0]\n",
    "vague_1s = vague_examples.loc[vague_examples['is_euph']==1]\n",
    "vague_0s = vague_examples.loc[vague_examples['is_euph']==0]\n",
    "unvague_1s = unvague_examples.loc[unvague_examples['is_euph']==1]\n",
    "unvague_0s = unvague_examples.loc[unvague_examples['is_euph']==0]\n",
    "\n",
    "print(len(vague_1s))\n",
    "print(len(vague_0s))\n",
    "print(len(unvague_1s))\n",
    "print(len(unvague_0s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf91854-a478-4be1-bb47-942cee07ee7a",
   "metadata": {},
   "source": [
    "### Weakest assumption - for each sentence in euph corpus, get closest cos sim label from combined annotation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc135f2c-1c8d-4057-86f9-9bdd5966e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form weakness corpus 0.1 - using sentence_transformers, copy vagueness label of sentence with highest cos similarity in annotation samples\n",
    "euph_corpus = pd.read_csv('Euphemism_Corpus_2-24.csv', index_col = 0, encoding= 'utf-8')\n",
    "euph_corpus['is_vague'] = -1\n",
    "weak_corpus = pd.read_csv(\"Weak_Assumption_Corpus.csv\", index_col = 0)\n",
    "\n",
    "for i, row in tqdm(euph_corpus.iterrows()):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb10eff-1d92-4ba4-9efb-fd08da168ff0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Archives: Creating Vagueness Train-Test Splits, Data Analyses..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7be903-38d0-457b-ac25-3cf805fd5baf",
   "metadata": {},
   "source": [
    "## Create Vagueness Train-Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25c505-c0f6-4e0c-a2b5-7b9718c549c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TEST_0 - TRAIN: Mixed, TEST: Only Vague/Non-Vague"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d957aa-765a-4c88-bfde-15bd6e2cb3c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Vague-only Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a74a65d4-a9b9-494c-b24e-3540b49287f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial experimental run (assumption: PETs in a 1 or 0 context will always have the same vagueness)\n",
    "# requires variables from previous chunk \n",
    "\n",
    "# create test set\n",
    "vague_1s_sample = vague_1s.sample(197).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "vague_0s_sample = vague_0s.sample(196).sample(frac=1)\n",
    "vague_1s = vague_1s.drop(vague_1s_sample.index).sample(frac=1) # remove the selected ones from the vague examples (the remaining will serve as training examples)\n",
    "vague_0s = vague_0s.drop(vague_0s_sample.index).sample(frac=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84f6d0e6-cd55-4a4b-a661-a7a68ad65c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vague_train = pd.concat([vague_1s, vague_0s]) # the portion of vague examples that should will go in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d7d5efc-6bbe-40a2-983f-4e718c42966c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_vague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>overweight</td>\n",
       "      <td>It's not OK because when kids don't eat well o...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>overweight</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>It's not OK because when kids don't eat well o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>intoxicated</td>\n",
       "      <td>Technically the fact that he was &lt;intoxicated&gt;...</td>\n",
       "      <td>1</td>\n",
       "      <td>substances</td>\n",
       "      <td>intoxicated</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Technically the fact that he was intoxicated m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>collateral damage</td>\n",
       "      <td>Keep in mind, folks, that this guy claims to b...</td>\n",
       "      <td>0</td>\n",
       "      <td>death</td>\n",
       "      <td>collateral damage</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>And yet he is in essence saying the murder of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>able-bodied</td>\n",
       "      <td>Certain precautions may have to be taken in ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>able-bodied</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>For instance every ATC user may have to be eit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>troubled</td>\n",
       "      <td>In Youngstown, the Corrections Corporation of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>troubled</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>It found them in the troubled Lorton prison in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>aging</td>\n",
       "      <td>The outside table has been super-soaked in woo...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>aging</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>I don't mind some aging just don't want it to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>late</td>\n",
       "      <td>He said the situation in Kaura Namida is espec...</td>\n",
       "      <td>1</td>\n",
       "      <td>death</td>\n",
       "      <td>late</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>He said the family of the late Emir has been q...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>plump</td>\n",
       "      <td>It appears to be wearing a coat two sizes too ...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>plump</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>It perches over a small pool then lunges with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>drinking problem</td>\n",
       "      <td>From civics class, you may remember that the 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>substances</td>\n",
       "      <td>drinking problem</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>But while the amendment made it once again leg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>weed</td>\n",
       "      <td>PBO has created more jobs without the help of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>substances</td>\n",
       "      <td>weed</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>These men will never change and the only way t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1572 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                keyword                                        edited_text  \\\n",
       "1464         overweight  It's not OK because when kids don't eat well o...   \n",
       "1137        intoxicated  Technically the fact that he was <intoxicated>...   \n",
       "1590  collateral damage  Keep in mind, folks, that this guy claims to b...   \n",
       "561         able-bodied  Certain precautions may have to be taken in ca...   \n",
       "1331           troubled  In Youngstown, the Corrections Corporation of ...   \n",
       "...                 ...                                                ...   \n",
       "1475              aging  The outside table has been super-soaked in woo...   \n",
       "1287               late  He said the situation in Kaura Namida is espec...   \n",
       "1603              plump  It appears to be wearing a coat two sizes too ...   \n",
       "207    drinking problem  From civics class, you may remember that the 2...   \n",
       "1623               weed  PBO has created more jobs without the help of ...   \n",
       "\n",
       "      is_euph                    category               type     euph_status  \\\n",
       "1464        0  physical/mental attributes         overweight  sometimes_euph   \n",
       "1137        1                  substances        intoxicated  sometimes_euph   \n",
       "1590        0                       death  collateral damage  sometimes_euph   \n",
       "561         1  physical/mental attributes        able-bodied     always_euph   \n",
       "1331        1  physical/mental attributes           troubled  sometimes_euph   \n",
       "...       ...                         ...                ...             ...   \n",
       "1475        0  physical/mental attributes              aging  sometimes_euph   \n",
       "1287        1                       death               late  sometimes_euph   \n",
       "1603        0  physical/mental attributes              plump  sometimes_euph   \n",
       "207         1                  substances   drinking problem     always_euph   \n",
       "1623        0                  substances               weed  sometimes_euph   \n",
       "\n",
       "                                               sentence  is_vague  \n",
       "1464  It's not OK because when kids don't eat well o...         0  \n",
       "1137  Technically the fact that he was intoxicated m...         0  \n",
       "1590  And yet he is in essence saying the murder of ...         1  \n",
       "561   For instance every ATC user may have to be eit...         1  \n",
       "1331  It found them in the troubled Lorton prison in...         1  \n",
       "...                                                 ...       ...  \n",
       "1475  I don't mind some aging just don't want it to ...         0  \n",
       "1287  He said the family of the late Emir has been q...         0  \n",
       "1603  It perches over a small pool then lunges with ...         0  \n",
       "207   But while the amendment made it once again leg...         0  \n",
       "1623  These men will never change and the only way t...         1  \n",
       "\n",
       "[1572 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.concat([vague_train, unvague_examples]) # train set consists of the remaining vague examples and unvague_examples\n",
    "train_set = train_set.sample(frac=1)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b167cd5f-a9ed-415b-acd2-31b28695f90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_vague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>disadvantaged</td>\n",
       "      <td>To me, saying, \"You are Black/Native American/...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>disadvantaged</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>you must be disadvantaged and need help is a h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>disadvantaged</td>\n",
       "      <td>By contrast, Chapter 6 of What Is To Be Done? ...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>disadvantaged</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Now'proposes beginning those local economic re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>disadvantaged</td>\n",
       "      <td>Immediately prior to his Senate confirmation Z...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>disadvantaged</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>He also co-founded The Urban Alliance Foundati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>disadvantaged</td>\n",
       "      <td>The current study examined 205 youth and mento...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>disadvantaged</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>The current study examined 205 youth and mento...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>mentally challenged</td>\n",
       "      <td>But the biggest by far is the generous work ru...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>mentally challenged</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I'm thinking of the cancer-support groups; the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>underdeveloped</td>\n",
       "      <td>Dr. Sun Qingwei, a coal campaigner for Greenpe...</td>\n",
       "      <td>0</td>\n",
       "      <td>politics</td>\n",
       "      <td>underdeveloped</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>The reason is that the technology is underdeve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>over the hill</td>\n",
       "      <td>Who knows, perhaps beneath the starry night sk...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>over the hill</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Who knows perhaps beneath the starry night sky...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>pass away</td>\n",
       "      <td>If he were certain that there were no life to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>death</td>\n",
       "      <td>pass away</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>The greatest act of faith the only faith which...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>weed</td>\n",
       "      <td>Personal success, right? You are all scum, don...</td>\n",
       "      <td>0</td>\n",
       "      <td>substances</td>\n",
       "      <td>weed</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>hopefully a hurricane or snowstorm will weed t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>weed</td>\n",
       "      <td>A dramatic way @ @ @ @ @ @ @ @ @ @ live audien...</td>\n",
       "      <td>0</td>\n",
       "      <td>substances</td>\n",
       "      <td>weed</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>To weed out questions about Area 51 and JFK's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keyword                                        edited_text  \\\n",
       "1073        disadvantaged  To me, saying, \"You are Black/Native American/...   \n",
       "1071        disadvantaged  By contrast, Chapter 6 of What Is To Be Done? ...   \n",
       "1065        disadvantaged  Immediately prior to his Senate confirmation Z...   \n",
       "1066        disadvantaged  The current study examined 205 youth and mento...   \n",
       "714   mentally challenged  But the biggest by far is the generous work ru...   \n",
       "...                   ...                                                ...   \n",
       "1725       underdeveloped  Dr. Sun Qingwei, a coal campaigner for Greenpe...   \n",
       "1766        over the hill  Who knows, perhaps beneath the starry night sk...   \n",
       "1428            pass away  If he were certain that there were no life to ...   \n",
       "1615                 weed  Personal success, right? You are all scum, don...   \n",
       "1624                 weed  A dramatic way @ @ @ @ @ @ @ @ @ @ live audien...   \n",
       "\n",
       "      is_euph                    category                 type  \\\n",
       "1073        1                  employment        disadvantaged   \n",
       "1071        1                  employment        disadvantaged   \n",
       "1065        1                  employment        disadvantaged   \n",
       "1066        1                  employment        disadvantaged   \n",
       "714         1  physical/mental attributes  mentally challenged   \n",
       "...       ...                         ...                  ...   \n",
       "1725        0                    politics       underdeveloped   \n",
       "1766        0  physical/mental attributes        over the hill   \n",
       "1428        0                       death            pass away   \n",
       "1615        0                  substances                 weed   \n",
       "1624        0                  substances                 weed   \n",
       "\n",
       "         euph_status                                           sentence  \\\n",
       "1073  sometimes_euph  you must be disadvantaged and need help is a h...   \n",
       "1071  sometimes_euph  Now'proposes beginning those local economic re...   \n",
       "1065  sometimes_euph  He also co-founded The Urban Alliance Foundati...   \n",
       "1066  sometimes_euph  The current study examined 205 youth and mento...   \n",
       "714      always_euph  I'm thinking of the cancer-support groups; the...   \n",
       "...              ...                                                ...   \n",
       "1725  sometimes_euph  The reason is that the technology is underdeve...   \n",
       "1766  sometimes_euph  Who knows perhaps beneath the starry night sky...   \n",
       "1428  sometimes_euph  The greatest act of faith the only faith which...   \n",
       "1615  sometimes_euph  hopefully a hurricane or snowstorm will weed t...   \n",
       "1624  sometimes_euph  To weed out questions about Area 51 and JFK's ...   \n",
       "\n",
       "      is_vague  \n",
       "1073         1  \n",
       "1071         1  \n",
       "1065         1  \n",
       "1066         1  \n",
       "714          1  \n",
       "...        ...  \n",
       "1725         1  \n",
       "1766         1  \n",
       "1428         1  \n",
       "1615         1  \n",
       "1624         1  \n",
       "\n",
       "[393 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.concat([vague_1s_sample, vague_0s_sample])\n",
    "test_set = test_set.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe684175-ca91-4bd3-9ec1-89fe35ee3d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, for huggingface training you need to remove all columns except \"text\" and \"label\" - do this manually\n",
    "train_set.to_csv('Vagueness_Splits/Test_0/hf_train.csv')\n",
    "test_set.to_csv('Vagueness_Splits/Test_0/hf_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76c518-4780-4671-a602-198596aca31a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Unvague-only Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c542f851-e632-4a20-bd4a-5e3c16869896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test set\n",
    "unvague_1s_sample = unvague_1s.sample(197).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "unvague_0s_sample = unvague_0s.sample(196).sample(frac=1)\n",
    "unvague_1s = unvague_1s.drop(unvague_1s_sample.index).sample(frac=1) # remove the selected ones from the vague examples (the remaining will serve as training examples)\n",
    "unvague_0s = unvague_0s.drop(unvague_0s_sample.index).sample(frac=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b204be1-ae5f-431c-98fd-5aef31a271c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_vague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>underdeveloped</td>\n",
       "      <td>I hope you realize that the Daleks used to be ...</td>\n",
       "      <td>0</td>\n",
       "      <td>politics</td>\n",
       "      <td>underdeveloped</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>So in theory the chick turning out to be a Dal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>perish</td>\n",
       "      <td>So true @ @ @ @ @ @ @ @ @ @ the regional schoo...</td>\n",
       "      <td>0</td>\n",
       "      <td>death</td>\n",
       "      <td>perish</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Unfortunately in the following years there wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>collateral damage</td>\n",
       "      <td>The ambassador said Israel has destroyed many ...</td>\n",
       "      <td>1</td>\n",
       "      <td>death</td>\n",
       "      <td>collateral damage</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>The ambassador said Israel has destroyed many ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>demise</td>\n",
       "      <td>Time after time we see the intelligence, patie...</td>\n",
       "      <td>1</td>\n",
       "      <td>death</td>\n",
       "      <td>demise</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Time after time we see the intelligence patien...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>substance abuse</td>\n",
       "      <td>@ @ @ @ @ @ @ @ @ @ 1997, while simply doing m...</td>\n",
       "      <td>1</td>\n",
       "      <td>substances</td>\n",
       "      <td>substance abuse</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>1997 while simply doing my job as the Clinical...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>inebriated</td>\n",
       "      <td>Mary, You are familiar with rhetoric, aren't y...</td>\n",
       "      <td>1</td>\n",
       "      <td>substances</td>\n",
       "      <td>inebriated</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>The reason they are saying this is because in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>between jobs</td>\n",
       "      <td>I would still donate food and clothing for peo...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>between jobs</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>I applied for temporary assistance when I was ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>collateral damage</td>\n",
       "      <td>They didn't have him offed like they had Vince...</td>\n",
       "      <td>0</td>\n",
       "      <td>death</td>\n",
       "      <td>collateral damage</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>But the ambassador and his three protectors di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>armed conflict</td>\n",
       "      <td>A 2005 RAND Corp study found the UN to be succ...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>armed conflict</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>It compared UN nation-building efforts to thos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>ethnic cleansing</td>\n",
       "      <td>When Eastern Europeans fight each other it is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>death</td>\n",
       "      <td>ethnic cleansing</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Ethnic cleansing is intended to make what is h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1572 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                keyword                                        edited_text  \\\n",
       "1718     underdeveloped  I hope you realize that the Daleks used to be ...   \n",
       "1407             perish  So true @ @ @ @ @ @ @ @ @ @ the regional schoo...   \n",
       "974   collateral damage  The ambassador said Israel has destroyed many ...   \n",
       "910              demise  Time after time we see the intelligence, patie...   \n",
       "350     substance abuse  @ @ @ @ @ @ @ @ @ @ 1997, while simply doing m...   \n",
       "...                 ...                                                ...   \n",
       "380          inebriated  Mary, You are familiar with rhetoric, aren't y...   \n",
       "1198       between jobs  I would still donate food and clothing for peo...   \n",
       "1585  collateral damage  They didn't have him offed like they had Vince...   \n",
       "203      armed conflict  A 2005 RAND Corp study found the UN to be succ...   \n",
       "135    ethnic cleansing  When Eastern Europeans fight each other it is ...   \n",
       "\n",
       "      is_euph    category               type     euph_status  \\\n",
       "1718        0    politics     underdeveloped  sometimes_euph   \n",
       "1407        0       death             perish  sometimes_euph   \n",
       "974         1       death  collateral damage  sometimes_euph   \n",
       "910         1       death             demise  sometimes_euph   \n",
       "350         1  substances    substance abuse     always_euph   \n",
       "...       ...         ...                ...             ...   \n",
       "380         1  substances         inebriated     always_euph   \n",
       "1198        1  employment       between jobs  sometimes_euph   \n",
       "1585        0       death  collateral damage  sometimes_euph   \n",
       "203         1    politics     armed conflict     always_euph   \n",
       "135         1       death   ethnic cleansing     always_euph   \n",
       "\n",
       "                                               sentence  is_vague  \n",
       "1718  So in theory the chick turning out to be a Dal...         1  \n",
       "1407  Unfortunately in the following years there wer...         1  \n",
       "974   The ambassador said Israel has destroyed many ...         1  \n",
       "910   Time after time we see the intelligence patien...         0  \n",
       "350   1997 while simply doing my job as the Clinical...         0  \n",
       "...                                                 ...       ...  \n",
       "380   The reason they are saying this is because in ...         0  \n",
       "1198  I applied for temporary assistance when I was ...         0  \n",
       "1585  But the ambassador and his three protectors di...         1  \n",
       "203   It compared UN nation-building efforts to thos...         1  \n",
       "135   Ethnic cleansing is intended to make what is h...         0  \n",
       "\n",
       "[1572 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unvague_train = pd.concat([unvague_1s, unvague_0s])\n",
    "train_set = pd.concat([unvague_train, vague_examples]) # train set consists of the remaining vague examples and unvague_examples\n",
    "train_set = train_set.sample(frac=1)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29dcfd0a-412f-4b78-a8b8-cc92587f1dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_vague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>undocumented immigrants</td>\n",
       "      <td>The Arizona Republic reports that young &lt;undoc...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>undocumented immigrant</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>The Arizona Republic reports that young undocu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>advanced age</td>\n",
       "      <td>These include the receptors for insulin and fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>advanced age</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Rapamycin is an antimicrobial that was recentl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>aging</td>\n",
       "      <td>I thought they made the right move when they t...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>aging</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>And they rebuilt well; they sold off aging vet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>mixed up</td>\n",
       "      <td>Im white and a woman and in my sixtys. I do nt...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>mixed up</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>he got mixed up in drugs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>well off</td>\n",
       "      <td>How do you prove honest @ @ @ @ @ @ @ @ @ @ Br...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>well off</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>he gets matrimonial offers from even well off ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>substance abusers</td>\n",
       "      <td>Indeed, it's useful to compare the Hollywood o...</td>\n",
       "      <td>1</td>\n",
       "      <td>substances</td>\n",
       "      <td>substance abuser</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Both had lots of substance abusers divorce and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>exterminate</td>\n",
       "      <td>They had long ago interacted with earth and ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>politics</td>\n",
       "      <td>exterminate</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Voltar and his people felt that man was being ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>to go to heaven</td>\n",
       "      <td>The short version I tried to commit suicide be...</td>\n",
       "      <td>1</td>\n",
       "      <td>death</td>\n",
       "      <td>to go to heaven</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>A couple days before halloween 2006 I laid dow...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>late</td>\n",
       "      <td>That was really bad for public health. In Nige...</td>\n",
       "      <td>1</td>\n",
       "      <td>death</td>\n",
       "      <td>late</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>In Nigeria that situation was what obtained be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>droppings</td>\n",
       "      <td>The @ @ @ @ @ @ @ @ @ @ whom the message of fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>bodily functions</td>\n",
       "      <td>droppings</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>The whom the message of freedom and self relia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      keyword  \\\n",
       "16    undocumented immigrants   \n",
       "682              advanced age   \n",
       "1487                    aging   \n",
       "1091                 mixed up   \n",
       "1105                 well off   \n",
       "...                       ...   \n",
       "488         substance abusers   \n",
       "1801              exterminate   \n",
       "1249          to go to heaven   \n",
       "1310                     late   \n",
       "724                 droppings   \n",
       "\n",
       "                                            edited_text  is_euph  \\\n",
       "16    The Arizona Republic reports that young <undoc...        1   \n",
       "682   These include the receptors for insulin and fo...        1   \n",
       "1487  I thought they made the right move when they t...        0   \n",
       "1091  Im white and a woman and in my sixtys. I do nt...        1   \n",
       "1105  How do you prove honest @ @ @ @ @ @ @ @ @ @ Br...        1   \n",
       "...                                                 ...      ...   \n",
       "488   Indeed, it's useful to compare the Hollywood o...        1   \n",
       "1801  They had long ago interacted with earth and ou...        0   \n",
       "1249  The short version I tried to commit suicide be...        1   \n",
       "1310  That was really bad for public health. In Nige...        1   \n",
       "724   The @ @ @ @ @ @ @ @ @ @ whom the message of fr...        1   \n",
       "\n",
       "                        category                    type     euph_status  \\\n",
       "16                      politics  undocumented immigrant     always_euph   \n",
       "682   physical/mental attributes            advanced age     always_euph   \n",
       "1487  physical/mental attributes                   aging  sometimes_euph   \n",
       "1091  physical/mental attributes                mixed up  sometimes_euph   \n",
       "1105                  employment                well off  sometimes_euph   \n",
       "...                          ...                     ...             ...   \n",
       "488                   substances        substance abuser     always_euph   \n",
       "1801                    politics             exterminate  sometimes_euph   \n",
       "1249                       death         to go to heaven  sometimes_euph   \n",
       "1310                       death                    late  sometimes_euph   \n",
       "724             bodily functions               droppings     always_euph   \n",
       "\n",
       "                                               sentence  is_vague  \n",
       "16    The Arizona Republic reports that young undocu...         0  \n",
       "682   Rapamycin is an antimicrobial that was recentl...         0  \n",
       "1487  And they rebuilt well; they sold off aging vet...         0  \n",
       "1091                          he got mixed up in drugs          0  \n",
       "1105  he gets matrimonial offers from even well off ...         0  \n",
       "...                                                 ...       ...  \n",
       "488   Both had lots of substance abusers divorce and...         0  \n",
       "1801  Voltar and his people felt that man was being ...         0  \n",
       "1249  A couple days before halloween 2006 I laid dow...         0  \n",
       "1310  In Nigeria that situation was what obtained be...         0  \n",
       "724   The whom the message of freedom and self relia...         0  \n",
       "\n",
       "[393 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.concat([unvague_1s_sample, unvague_0s_sample])\n",
    "test_set = test_set.sample(frac=1)\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa5102d7-804e-4785-aa52-5e99a57f78af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, for huggingface training you need to remove all columns except \"text\" and \"label\" - do this manually\n",
    "train_set.to_csv('Vagueness_Splits/Test_0/unvague_only_test/hf_train.csv')\n",
    "test_set.to_csv('Vagueness_Splits/Test_0/unvague_only_test/hf_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a8a20-9b73-4599-8173-ba9bdd2a2c4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### TEST_1 - TRAIN: Only Vague, TEST: Only Vague/Non-Vague"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f88a04-e400-4d8b-bac8-4038013148f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Vague-only Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bf1b237-be8d-4ccf-9f7b-14a3449014cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_vague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>troubled</td>\n",
       "      <td>In Youngstown, the Corrections Corporation of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>troubled</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>It found them in the troubled Lorton prison in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>special needs</td>\n",
       "      <td>Hill that year was a group of LDS Scouts who w...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>special needs</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Boy Scout Troop 601 a Church-sponsored unit fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>invalid</td>\n",
       "      <td>She did 143 paintings, more than a third of th...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>invalid</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Her brief life is connected with her solipsist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>custodians</td>\n",
       "      <td>Those include $ 700,000 slashed from instructi...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>custodians</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>The district also will reduce bus transportati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>disabled</td>\n",
       "      <td>The whole point of insurance is to attain it b...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>disabled</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>You can argue that the government ie taxpayers...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>sanitation workers</td>\n",
       "      <td>In much of the world @ @ @ @ @ @ @ @ @ @ the f...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>sanitation worker</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>In much of the world the front lines of the cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>weed</td>\n",
       "      <td>Glyphosate is a strong organic phosphate chela...</td>\n",
       "      <td>0</td>\n",
       "      <td>substances</td>\n",
       "      <td>weed</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>It is this ability to shut down physiological ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>demise</td>\n",
       "      <td>Speaking of doing exactly what people expect, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>death</td>\n",
       "      <td>demise</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Since I started these pieces Nebraska has mana...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>special needs</td>\n",
       "      <td>She's still in the hospital. Baby Nozomi is st...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>special needs</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Baby Nozomi is still in the NICU while big sis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>global south</td>\n",
       "      <td>With the Cold War over, NAM is almost always d...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>global south</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>The bloc is in some respects a failure; as a b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 keyword                                        edited_text  \\\n",
       "1331            troubled  In Youngstown, the Corrections Corporation of ...   \n",
       "1049       special needs  Hill that year was a group of LDS Scouts who w...   \n",
       "1088             invalid  She did 143 paintings, more than a third of th...   \n",
       "551           custodians  Those include $ 700,000 slashed from instructi...   \n",
       "1040            disabled  The whole point of insurance is to attain it b...   \n",
       "...                  ...                                                ...   \n",
       "519   sanitation workers  In much of the world @ @ @ @ @ @ @ @ @ @ the f...   \n",
       "1631                weed  Glyphosate is a strong organic phosphate chela...   \n",
       "1530              demise  Speaking of doing exactly what people expect, ...   \n",
       "1699       special needs  She's still in the hospital. Baby Nozomi is st...   \n",
       "361         global south  With the Cold War over, NAM is almost always d...   \n",
       "\n",
       "      is_euph                    category               type     euph_status  \\\n",
       "1331        1  physical/mental attributes           troubled  sometimes_euph   \n",
       "1049        1  physical/mental attributes      special needs  sometimes_euph   \n",
       "1088        1  physical/mental attributes            invalid  sometimes_euph   \n",
       "551         1                  employment         custodians     always_euph   \n",
       "1040        1  physical/mental attributes           disabled  sometimes_euph   \n",
       "...       ...                         ...                ...             ...   \n",
       "519         1                  employment  sanitation worker     always_euph   \n",
       "1631        0                  substances               weed  sometimes_euph   \n",
       "1530        0                       death             demise  sometimes_euph   \n",
       "1699        0  physical/mental attributes      special needs  sometimes_euph   \n",
       "361         1                    politics       global south     always_euph   \n",
       "\n",
       "                                               sentence  is_vague  \n",
       "1331  It found them in the troubled Lorton prison in...         1  \n",
       "1049  Boy Scout Troop 601 a Church-sponsored unit fr...         1  \n",
       "1088  Her brief life is connected with her solipsist...         1  \n",
       "551   The district also will reduce bus transportati...         1  \n",
       "1040  You can argue that the government ie taxpayers...         1  \n",
       "...                                                 ...       ...  \n",
       "519   In much of the world the front lines of the cr...         1  \n",
       "1631  It is this ability to shut down physiological ...         1  \n",
       "1530  Since I started these pieces Nebraska has mana...         1  \n",
       "1699  Baby Nozomi is still in the NICU while big sis...         1  \n",
       "361   The bloc is in some respects a failure; as a b...         1  \n",
       "\n",
       "[154 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vet_corpus = pd.read_csv('VET_Corpus_0.1.csv', index_col = 0, encoding= 'utf-8') # corpus containing strong assumption PET-generalization\n",
    "\n",
    "vague_examples = vet_corpus.loc[vet_corpus['is_vague']==1]\n",
    "# unvague_examples = vet_corpus.loc[vet_corpus['is_vague']==0]\n",
    "vague_1s = vague_examples.loc[vague_examples['is_euph']==1]\n",
    "vague_0s = vague_examples.loc[vague_examples['is_euph']==0]\n",
    "\n",
    "# construct the test set\n",
    "vague_1s_sample = vague_1s.sample(77)\n",
    "vague_0s_sample = vague_0s.sample(77)\n",
    "vague_test = pd.concat([vague_1s_sample, vague_0s_sample]).sample(frac=1)\n",
    "vague_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3470a5f4-3256-4b11-8a6d-f766f439591a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_vague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>golden years</td>\n",
       "      <td>All three came from situations of terrible neg...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>golden years</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>We hope to find more families like the Johnson...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>pass away</td>\n",
       "      <td>Just as man, by such precepts, impresses a kin...</td>\n",
       "      <td>0</td>\n",
       "      <td>death</td>\n",
       "      <td>pass away</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>As the psalmist says He has made a decree whic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>comfort women</td>\n",
       "      <td>By the way, Why the Chinese act so strongly to...</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>comfort women</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>More than and more than 200 thousand Chinese w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>lay off</td>\n",
       "      <td>We do NOT need some self-imposed book cop tell...</td>\n",
       "      <td>0</td>\n",
       "      <td>employment</td>\n",
       "      <td>lay off</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Lay off</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>freedom fighters</td>\n",
       "      <td>In recent years, ultra-Orthodox Charedi Jews i...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>freedom fighter</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>And now as the freedom fighters did in the Ame...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>disabled</td>\n",
       "      <td>In particular, one important task which the li...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>disabled</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>When this function is disabled by brain damage...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>mentally disabled</td>\n",
       "      <td>In order to sin you have to have a certain amo...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>mentally disabled</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>These things are all true of angels and humans...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>laid off</td>\n",
       "      <td>And layoff thousands and remove millions from ...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>lay off</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>But when he laid off workers at Bain to make m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>weed</td>\n",
       "      <td>In some ways, cultivating for &lt;weed&gt; control i...</td>\n",
       "      <td>0</td>\n",
       "      <td>substances</td>\n",
       "      <td>weed</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>In some ways cultivating for weed control is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>accident</td>\n",
       "      <td>I also do both. I did it by &lt;accident&gt; though....</td>\n",
       "      <td>1</td>\n",
       "      <td>misc.</td>\n",
       "      <td>accident</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>I did it by accident though</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>615 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                keyword                                        edited_text  \\\n",
       "758        golden years  All three came from situations of terrible neg...   \n",
       "1426          pass away  Just as man, by such precepts, impresses a kin...   \n",
       "118       comfort women  By the way, Why the Chinese act so strongly to...   \n",
       "1878            lay off  We do NOT need some self-imposed book cop tell...   \n",
       "82     freedom fighters  In recent years, ultra-Orthodox Charedi Jews i...   \n",
       "...                 ...                                                ...   \n",
       "1666           disabled  In particular, one important task which the li...   \n",
       "49    mentally disabled  In order to sin you have to have a certain amo...   \n",
       "1247           laid off  And layoff thousands and remove millions from ...   \n",
       "1608               weed  In some ways, cultivating for <weed> control i...   \n",
       "1262           accident  I also do both. I did it by <accident> though....   \n",
       "\n",
       "      is_euph                    category               type     euph_status  \\\n",
       "758         1  physical/mental attributes       golden years     always_euph   \n",
       "1426        0                       death          pass away  sometimes_euph   \n",
       "118         1             sexual activity      comfort women     always_euph   \n",
       "1878        0                  employment            lay off  sometimes_euph   \n",
       "82          1                    politics    freedom fighter     always_euph   \n",
       "...       ...                         ...                ...             ...   \n",
       "1666        0  physical/mental attributes           disabled  sometimes_euph   \n",
       "49          1  physical/mental attributes  mentally disabled     always_euph   \n",
       "1247        1                  employment            lay off  sometimes_euph   \n",
       "1608        0                  substances               weed  sometimes_euph   \n",
       "1262        1                       misc.           accident  sometimes_euph   \n",
       "\n",
       "                                               sentence  is_vague  \n",
       "758   We hope to find more families like the Johnson...         1  \n",
       "1426  As the psalmist says He has made a decree whic...         1  \n",
       "118   More than and more than 200 thousand Chinese w...         1  \n",
       "1878                                           Lay off          1  \n",
       "82    And now as the freedom fighters did in the Ame...         1  \n",
       "...                                                 ...       ...  \n",
       "1666  When this function is disabled by brain damage...         1  \n",
       "49    These things are all true of angels and humans...         1  \n",
       "1247  But when he laid off workers at Bain to make m...         1  \n",
       "1608  In some ways cultivating for weed control is a...         1  \n",
       "1262                       I did it by accident though          1  \n",
       "\n",
       "[615 rows x 8 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct the training set\n",
    "vague_1s = vague_1s.drop(vague_1s_sample.index) # remove the selected ones from the vague examples (the remaining will serve as training examples)\n",
    "vague_0s = vague_0s.drop(vague_0s_sample.index)\n",
    "vague_train = pd.concat([vague_1s, vague_0s]).sample(frac=1) # note the second .sample(frac=1) is to shuffle rows\n",
    "vague_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fba04afc-9559-438e-8f8a-ef57a1767431",
   "metadata": {},
   "outputs": [],
   "source": [
    "vague_train.to_csv('Vagueness_Splits/Test_1/vague_only_test/train.csv')\n",
    "# vague_test.to_csv('Vagueness_Splits/Test_1/vague_only_test/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d421b5a-efb6-427c-a7b9-78f47fd501f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Non-vague-only Test Set (154 examples for comparison with Vague-only Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f8c6fad-5523-41a4-90d9-236aad880138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_vague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>same-sex</td>\n",
       "      <td>Males try to hold females during copulation wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>same-sex</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Males are infected at both positions left post...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>making love</td>\n",
       "      <td>Cooking, gardening, &lt;making love&gt;-I love these...</td>\n",
       "      <td>1</td>\n",
       "      <td>sexual activity</td>\n",
       "      <td>make love</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Cooking gardening making love-I love these ele...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>overweight</td>\n",
       "      <td>It's not OK because when kids don't eat well o...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>overweight</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>It's not OK because when kids don't eat well o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>pro-life</td>\n",
       "      <td>Not enough, certainly; and some of the most pr...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>pro-life</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>Finally I think many pro-life people are polit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>demise</td>\n",
       "      <td>We are setting forth hopefully, a blueprint fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>death</td>\n",
       "      <td>demise</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>There are some who say it is a coroner's repor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>weed</td>\n",
       "      <td>Archived Andrew reflected on what it's like to...</td>\n",
       "      <td>1</td>\n",
       "      <td>substances</td>\n",
       "      <td>weed</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Noah Feldman tried to imagine how the SCOTUS w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>late</td>\n",
       "      <td>Access to Ontario Works, housing registry, bui...</td>\n",
       "      <td>0</td>\n",
       "      <td>death</td>\n",
       "      <td>late</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Late payments for the April 2 tax due date wil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>expecting</td>\n",
       "      <td>He had put in almost 20 years at NBC, devoting...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>expecting</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>He recalled how when his Late Night finally bu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>overweight</td>\n",
       "      <td>I also eat well and exercise 5-6 times per wee...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>overweight</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>I do not know what it is like to be overweight...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>less fortunate</td>\n",
       "      <td>Help us to be a light unto other nations and t...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>less fortunate</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>We remember those who are less fortunate than we</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             keyword                                        edited_text  \\\n",
       "1641        same-sex  Males try to hold females during copulation wi...   \n",
       "774      making love  Cooking, gardening, <making love>-I love these...   \n",
       "1464      overweight  It's not OK because when kids don't eat well o...   \n",
       "240         pro-life  Not enough, certainly; and some of the most pr...   \n",
       "899           demise  We are setting forth hopefully, a blueprint fo...   \n",
       "...              ...                                                ...   \n",
       "1363            weed  Archived Andrew reflected on what it's like to...   \n",
       "1913            late  Access to Ontario Works, housing registry, bui...   \n",
       "1842       expecting  He had put in almost 20 years at NBC, devoting...   \n",
       "1457      overweight  I also eat well and exercise 5-6 times per wee...   \n",
       "670   less fortunate  Help us to be a light unto other nations and t...   \n",
       "\n",
       "      is_euph                    category            type     euph_status  \\\n",
       "1641        0             sexual activity        same-sex  sometimes_euph   \n",
       "774         1             sexual activity       make love     always_euph   \n",
       "1464        0  physical/mental attributes      overweight  sometimes_euph   \n",
       "240         1                    politics        pro-life     always_euph   \n",
       "899         1                       death          demise  sometimes_euph   \n",
       "...       ...                         ...             ...             ...   \n",
       "1363        1                  substances            weed  sometimes_euph   \n",
       "1913        0                       death            late  sometimes_euph   \n",
       "1842        0  physical/mental attributes       expecting  sometimes_euph   \n",
       "1457        0  physical/mental attributes      overweight  sometimes_euph   \n",
       "670         1                  employment  less fortunate     always_euph   \n",
       "\n",
       "                                               sentence  is_vague  \n",
       "1641  Males are infected at both positions left post...         0  \n",
       "774   Cooking gardening making love-I love these ele...         0  \n",
       "1464  It's not OK because when kids don't eat well o...         0  \n",
       "240   Finally I think many pro-life people are polit...         0  \n",
       "899   There are some who say it is a coroner's repor...         0  \n",
       "...                                                 ...       ...  \n",
       "1363  Noah Feldman tried to imagine how the SCOTUS w...         0  \n",
       "1913  Late payments for the April 2 tax due date wil...         0  \n",
       "1842  He recalled how when his Late Night finally bu...         0  \n",
       "1457  I do not know what it is like to be overweight...         0  \n",
       "670   We remember those who are less fortunate than we          0  \n",
       "\n",
       "[154 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unvague_examples = vet_corpus.loc[vet_corpus['is_vague']==0]\n",
    "unvague_1s = unvague_examples.loc[unvague_examples['is_euph']==1]\n",
    "unvague_0s = unvague_examples.loc[unvague_examples['is_euph']==0]\n",
    "# construct the test set - NOTE that the train set should be the same (615 rows, made in the previous chunk); it is not constructed here\n",
    "unvague_1s_sample = unvague_1s.sample(77)\n",
    "unvague_0s_sample = unvague_0s.sample(77)\n",
    "unvague_test = pd.concat([unvague_1s_sample, unvague_0s_sample]).sample(frac=1)\n",
    "unvague_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29236684-5628-4eef-833f-10855b6a2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unvague_test.to_csv('Vagueness_Splits/Test_1/unvague_only_test/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361a035-0521-488b-b9db-f61e71ad3d06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### TEST_2 - TRAIN: Only Non-Vague, TEST: Only Vague/Non-Vague\n",
    "- to hopefully verify the vagueness results from TEST_1/TEST_0\n",
    "- as such, the test sets from the previous TEST are used; this code is only used to produce the non-vague train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "543275d5-2504-47e5-8ab7-54f08487e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make use of the same test data in Test_1, but we have to make sure our training non-vague examples do not clash\n",
    "vague_test = pd.read_csv('Vagueness_Splits/Test_1/vague_only_test/test.csv', index_col=0)\n",
    "unvague_test = pd.read_csv('Vagueness_Splits/Test_1/unvague_only_test/test.csv', index_col=0)\n",
    "vet_corpus = pd.read_csv('VET_Corpus_0.1.csv', index_col = 0, encoding= 'utf-8') # corpus containing strong assumption PET-generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95298142-a54f-449b-89fd-b495e7dcc5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_vague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>chest</td>\n",
       "      <td>By his late teens, his bulk had become a menac...</td>\n",
       "      <td>0</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>chest</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>One player knocked himself out running into Sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>correctional facility</td>\n",
       "      <td>Ray Reyna Jr. said Los Banos Police will aid t...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>correctional facility</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>This was such a tragic case and and booked on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>dismissed</td>\n",
       "      <td>Men: wear a tie. If you don't look sharp, you'...</td>\n",
       "      <td>0</td>\n",
       "      <td>employment</td>\n",
       "      <td>dismissed</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>If you don't look sharp you're making it all t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>let go of</td>\n",
       "      <td>At the end of the day, Steven Sinofsky is a hu...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>let go of</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>At the end of the day Steven Sinofsky is a hum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>senior citizen</td>\n",
       "      <td>It is obscene what goes on in that arena. I ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>senior citizen</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I have client's who are far younger and more a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>elderly</td>\n",
       "      <td>My father's mother lived with us at the time a...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>elderly</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I was paid for the weekend to go get her paper...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>pro-life</td>\n",
       "      <td>When I first started blogging a year and a hal...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>pro-life</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I insisted that the pro-life movement wasn't a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>capital punishment</td>\n",
       "      <td>My opinions were as ecumenical as my larder, t...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>capital punishment</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>I had no problem with abortion but abhorred ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>aging</td>\n",
       "      <td>Not enough for long term, but beyond the 72-ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>aging</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Some family lives only miles from us but my ag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>economical</td>\n",
       "      <td>Lesser damage was sustained by at least 19 oth...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>economical</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>Fires occurred on three carriers when planes w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>615 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    keyword  \\\n",
       "1497                  chest   \n",
       "67    correctional facility   \n",
       "1557              dismissed   \n",
       "781               let go of   \n",
       "470          senior citizen   \n",
       "...                     ...   \n",
       "165                 elderly   \n",
       "234                pro-life   \n",
       "427      capital punishment   \n",
       "872                   aging   \n",
       "1116             economical   \n",
       "\n",
       "                                            edited_text  is_euph  \\\n",
       "1497  By his late teens, his bulk had become a menac...        0   \n",
       "67    Ray Reyna Jr. said Los Banos Police will aid t...        1   \n",
       "1557  Men: wear a tie. If you don't look sharp, you'...        0   \n",
       "781   At the end of the day, Steven Sinofsky is a hu...        1   \n",
       "470   It is obscene what goes on in that arena. I ha...        1   \n",
       "...                                                 ...      ...   \n",
       "165   My father's mother lived with us at the time a...        1   \n",
       "234   When I first started blogging a year and a hal...        1   \n",
       "427   My opinions were as ecumenical as my larder, t...        1   \n",
       "872   Not enough for long term, but beyond the 72-ho...        1   \n",
       "1116  Lesser damage was sustained by at least 19 oth...        1   \n",
       "\n",
       "                        category                   type     euph_status  \\\n",
       "1497  physical/mental attributes                  chest  sometimes_euph   \n",
       "67                    employment  correctional facility     always_euph   \n",
       "1557                  employment              dismissed  sometimes_euph   \n",
       "781                   employment              let go of  sometimes_euph   \n",
       "470   physical/mental attributes         senior citizen     always_euph   \n",
       "...                          ...                    ...             ...   \n",
       "165   physical/mental attributes                elderly     always_euph   \n",
       "234                     politics               pro-life     always_euph   \n",
       "427                     politics     capital punishment     always_euph   \n",
       "872   physical/mental attributes                  aging  sometimes_euph   \n",
       "1116                  employment             economical  sometimes_euph   \n",
       "\n",
       "                                               sentence  is_vague  \n",
       "1497  One player knocked himself out running into Sh...         0  \n",
       "67    This was such a tragic case and and booked on ...         0  \n",
       "1557  If you don't look sharp you're making it all t...         0  \n",
       "781   At the end of the day Steven Sinofsky is a hum...         0  \n",
       "470   I have client's who are far younger and more a...         0  \n",
       "...                                                 ...       ...  \n",
       "165   I was paid for the weekend to go get her paper...         0  \n",
       "234   I insisted that the pro-life movement wasn't a...         0  \n",
       "427   I had no problem with abortion but abhorred ca...         0  \n",
       "872   Some family lives only miles from us but my ag...         0  \n",
       "1116  Fires occurred on three carriers when planes w...         0  \n",
       "\n",
       "[615 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vet_corpus = vet_corpus.drop(vague_test.index)\n",
    "vet_corpus = vet_corpus.drop(unvague_test.index)\n",
    "\n",
    "unvague_examples = vet_corpus.loc[vet_corpus['is_vague']==0]\n",
    "\n",
    "unvague_1s = unvague_examples.loc[unvague_examples['is_euph']==1] # there's a bunch of these but let's only take 477 of them to make the training set 615 examples, as it was for the previous test\n",
    "unvague_1s_sample = unvague_1s.sample(477)\n",
    "unvague_0s = unvague_examples.loc[unvague_examples['is_euph']==0] # we want to use all 138 of these\n",
    "\n",
    "unvague_train = pd.concat([unvague_1s_sample, unvague_0s]).sample(frac=1)\n",
    "unvague_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36643cab-9c43-4ee0-85a7-96281c28bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unvague_train.to_csv('Vagueness_Splits/Test_2/vague_only_test/train.csv')\n",
    "unvague_train.to_csv('Vagueness_Splits/Test_2/unvague_only_test/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c685e-7c32-4dd5-b1f9-4a013ddfc73f",
   "metadata": {},
   "source": [
    "#### Test Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e5d151-4181-4653-bdda-ffd31d634339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_1:\n",
      "Number of PETs in train: 58\n",
      "Number of PETs in VAGUE test set: 43\n",
      "Number of PETs in UNVAGUE test set: 58\n"
     ]
    }
   ],
   "source": [
    "# Check le PET distrbutions\n",
    "test1_vague_train = pd.read_csv('Vagueness_Splits/Test_1/vague_only_test/train.csv', index_col=0)\n",
    "test1_vague_test = pd.read_csv('Vagueness_Splits/Test_1/vague_only_test/test.csv', index_col=0)\n",
    "test1_unvague_test = pd.read_csv('Vagueness_Splits/Test_1/unvague_only_test/test.csv', index_col=0)\n",
    "\n",
    "print(\"TEST_1:\")\n",
    "print('Number of PETs in train:', len(test1_vague_train['type'].unique()))\n",
    "print('Number of PETs in VAGUE test set:', len(test1_vague_test['type'].unique()))\n",
    "print('Number of PETs in UNVAGUE test set:', len(test1_unvague_test['type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15e49ad-02d2-4a27-9564-f77191d50dae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>disabled</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collateral damage</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weed</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>underdeveloped</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lay off</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accident</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demise</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>developed/ing country</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perish</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inner city</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disadvantaged</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intoxicated</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>special needs</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>troubled</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>golden years</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pass away</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regime change</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixed up</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income inequality</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go all the way</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sanitation worker</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freedom fighter</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a certain age</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economical</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>venereal disease</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>downsize</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mentally challenged</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able-bodied</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutralize</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custodian</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pass on</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well off</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>armed conflict</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slim</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custodians</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running behind</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invalid</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outspoken</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>let go of</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>over the hill</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>put to sleep</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleep around</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global south</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       type\n",
       "disabled                 13\n",
       "collateral damage        10\n",
       "weed                      8\n",
       "underdeveloped            8\n",
       "lay off                   8\n",
       "accident                  7\n",
       "demise                    6\n",
       "developed/ing country     6\n",
       "perish                    6\n",
       "inner city                6\n",
       "disadvantaged             6\n",
       "intoxicated               5\n",
       "special needs             5\n",
       "troubled                  5\n",
       "golden years              5\n",
       "pass away                 5\n",
       "regime change             4\n",
       "mixed up                  3\n",
       "income inequality         3\n",
       "go all the way            3\n",
       "sanitation worker         3\n",
       "freedom fighter           2\n",
       "a certain age             2\n",
       "economical                2\n",
       "venereal disease          2\n",
       "downsize                  2\n",
       "mentally challenged       2\n",
       "able-bodied               2\n",
       "neutralize                1\n",
       "custodian                 1\n",
       "pass on                   1\n",
       "well off                  1\n",
       "armed conflict            1\n",
       "slim                      1\n",
       "custodians                1\n",
       "running behind            1\n",
       "invalid                   1\n",
       "outspoken                 1\n",
       "let go of                 1\n",
       "over the hill             1\n",
       "put to sleep              1\n",
       "sleep around              1\n",
       "global south              1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test1_vague_train['type'].value_counts())\n",
    "pd.DataFrame(test1_vague_test['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a7d19fe-98a8-45e6-a493-946c302897ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_2:\n",
      "Number of PETs in train: 85\n",
      "Number of PETs in VAGUE test set: 43\n",
      "Number of PETs in UNVAGUE test set: 58\n"
     ]
    }
   ],
   "source": [
    "test2_unvague_train = pd.read_csv('Vagueness_Splits/Test_2/vague_only_test/train.csv', index_col=0)\n",
    "test2_vague_test = pd.read_csv('Vagueness_Splits/Test_2/vague_only_test/test.csv', index_col=0)\n",
    "test2_unvague_test = pd.read_csv('Vagueness_Splits/Test_2/unvague_only_test/test.csv', index_col=0)\n",
    "\n",
    "print(\"TEST_2:\")\n",
    "print('Number of PETs in train:', len(test2_unvague_train['type'].unique()))\n",
    "print('Number of PETs in VAGUE test set:', len(test2_vague_test['type'].unique()))\n",
    "print('Number of PETs in UNVAGUE test set:', len(test2_unvague_test['type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f2b7eab7-77e5-4399-87eb-0611629e2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "89\n",
      "23 DIFFERENT VAGUE ALWAYS_EUPHS TOTALING TO 209 EXAMPLES\n",
      "37 DIFFERENT VAGUE SOMETIMES_EUPHS TOTALING TO 560 EXAMPLES\n",
      "48 DIFFERENT UNVAGUE ALWAYS_EUPHS TOTALING TO 568 EXAMPLES\n",
      "41 DIFFERENT UNVAGUE SOMETIMES_EUPHS TOTALING TO 628 EXAMPLES\n"
     ]
    }
   ],
   "source": [
    "vet_corpus = pd.read_csv('VET_Corpus_0.1.csv', index_col=0)\n",
    "vague_examples = vet_corpus.loc[vet_corpus['is_vague']==1]\n",
    "vague_always_PETs = vague_examples.loc[vague_examples['euph_status']=='always_euph']\n",
    "vague_sometimes_PETs = vague_examples.loc[vague_examples['euph_status']=='sometimes_euph']\n",
    "\n",
    "unvague_examples = vet_corpus.loc[vet_corpus['is_vague']==0]\n",
    "unvague_always_PETs = unvague_examples.loc[unvague_examples['euph_status']=='always_euph']\n",
    "unvague_sometimes_PETs = unvague_examples.loc[unvague_examples['euph_status']=='sometimes_euph']\n",
    "\n",
    "print(len(vague_examples['type'].unique()))\n",
    "print(len(unvague_examples['type'].unique()))\n",
    "\n",
    "print(\"{} DIFFERENT VAGUE ALWAYS_EUPHS TOTALING TO {} EXAMPLES\".format(len(vague_always_PETs['type'].unique()), len(vague_always_PETs)))\n",
    "print(\"{} DIFFERENT VAGUE SOMETIMES_EUPHS TOTALING TO {} EXAMPLES\".format(len(vague_sometimes_PETs['type'].unique()), len(vague_sometimes_PETs)))\n",
    "print(\"{} DIFFERENT UNVAGUE ALWAYS_EUPHS TOTALING TO {} EXAMPLES\".format(len(unvague_always_PETs['type'].unique()), len(unvague_always_PETs)))\n",
    "print(\"{} DIFFERENT UNVAGUE SOMETIMES_EUPHS TOTALING TO {} EXAMPLES\".format(len(unvague_sometimes_PETs['type'].unique()), len(unvague_sometimes_PETs)))\n",
    "\n",
    "# print(len(vague_sometimes_PETs['type'].unique()))\n",
    "# print(len(unvague_always_PETs['type'].unique()))\n",
    "# print(len(unvague_sometimes_PETs['type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8052e23b-18b0-4ba6-ab7c-588fc37ace94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aging',\n",
       " 'between jobs',\n",
       " 'chest',\n",
       " 'deprived',\n",
       " 'dismissed',\n",
       " 'expecting',\n",
       " 'exterminate',\n",
       " 'gluteus maximus',\n",
       " 'got clean',\n",
       " 'late',\n",
       " 'let [pro] go',\n",
       " 'oldest profession',\n",
       " 'overweight',\n",
       " 'plump',\n",
       " 'same-sex',\n",
       " 'seasoned',\n",
       " 'sleep with',\n",
       " 'sober',\n",
       " 'stout',\n",
       " 'wealthy',\n",
       " 'with child'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate lists of PETs: vague-euph, nonvague-euph, vague-lit, nonvague-lit\n",
    "import os\n",
    "vet_corpus = pd.read_csv('VET_Corpus_0.1.csv', index_col = 0, encoding= 'utf-8') # corpus containing strong assumption PET-generalization\n",
    "vague_examples = vet_corpus.loc[vet_corpus['is_vague']==1]\n",
    "unvague_examples = vet_corpus.loc[vet_corpus['is_vague']==0]\n",
    "\n",
    "vague_1s = vague_examples.loc[vague_examples['is_euph']==1]\n",
    "vague_0s = vague_examples.loc[vague_examples['is_euph']==0]\n",
    "unvague_1s = unvague_examples.loc[unvague_examples['is_euph']==1]\n",
    "unvague_0s = unvague_examples.loc[unvague_examples['is_euph']==0]\n",
    "\n",
    "# vague_1_PETs = pd.DataFrame({'vague_euph': vague_1s['type'].unique().tolist()})\n",
    "# vague_0_PETs = pd.DataFrame({'vague_lits': vague_0s['type'].unique().tolist()})\n",
    "# unvague_1_PETs = pd.DataFrame({'unvague_euphs': unvague_1s['type'].unique().tolist()})\n",
    "# unvague_0_PETs = pd.DataFrame({'unvague_lits': unvague_0s['type'].unique().tolist()})\n",
    "\n",
    "set(unvague_0s['type'].unique()).intersection(unvague_1s['type'].unique())\n",
    "    \n",
    "# data = pd.concat([vague_1_PETs, vague_0_PETs, unvague_1_PETs, unvague_0_PETs], axis=1)\n",
    "# data \n",
    "# data.to_csv('VET_List_0.1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4af21484-7a94-4031-95ff-e8a02a2f6349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "a = vague_0s['type'].unique()\n",
    "print(len(a))\n",
    "u = vague_0s.loc[vague_0s['euph_status']=='sometimes_euph']\n",
    "t = u['type'].unique()\n",
    "print(len(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084246b-1888-421f-bb15-88d376995b40",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TEST_3 - TESTS 1 and 2, but 10 random splits each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e864114-29f6-4287-9826-8d57801145b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function, HuggingFace-ify, takes a sample of euphemism corpus and it makes into an appropriate format for the HuggingFace Trainer class\n",
    "def hfify(df):\n",
    "    df = df.drop(['keyword', 'category', 'type', 'euph_status', 'sentence', 'is_vague'], axis=1)\n",
    "    df = df.rename(columns={'edited_text':'text', 'is_euph':'label'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313b40e1-3954-42b6-a271-55394f387a97",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'Vagueness_Splits/Test_3/train_vague_only/test_vague_only/test0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8144/2559812841.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mvague_test_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Vagueness_Splits/Test_3/train_vague_only/test_vague_only/test'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0munvague_test_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Vagueness_Splits/Test_3/train_vague_only/test_unvague_only/test'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvague_test_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munvague_test_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'Vagueness_Splits/Test_3/train_vague_only/test_vague_only/test0'"
     ]
    }
   ],
   "source": [
    "# make the 10 splits for the training_vague_only test\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "vet_corpus = pd.read_csv('VET_Corpus_0.1.csv', index_col = 0, encoding= 'utf-8') # corpus containing strong assumption PET-generalization\n",
    "vague_examples = vet_corpus.loc[vet_corpus['is_vague']==1]\n",
    "unvague_examples = vet_corpus.loc[vet_corpus['is_vague']==0]\n",
    "\n",
    "vague_1s = vague_examples.loc[vague_examples['is_euph']==1]\n",
    "vague_0s = vague_examples.loc[vague_examples['is_euph']==0]\n",
    "unvague_1s = unvague_examples.loc[unvague_examples['is_euph']==1]\n",
    "unvague_0s = unvague_examples.loc[unvague_examples['is_euph']==0]\n",
    "\n",
    "for x in range(0, 10):\n",
    "    vague_test_folder = 'Vagueness_Splits/Test_3/train_vague_only/test_vague_only/test' + str(x)\n",
    "    unvague_test_folder = 'Vagueness_Splits/Test_3/train_vague_only/test_unvague_only/test' + str(x)\n",
    "    os.mkdir(vague_test_folder)\n",
    "    os.mkdir(unvague_test_folder)\n",
    "    \n",
    "    # create the vague-only training sample for the vague-only test sample\n",
    "    vague_1s_sample = vague_1s.sample(324)\n",
    "    vague_0s_sample = vague_0s.sample(291)\n",
    "    vague_train = pd.concat([vague_1s_sample, vague_0s_sample]).sample(frac=1)\n",
    "    vague_train.to_csv(vague_test_folder + '/train.csv')\n",
    "    vague_train = hfify(vague_train)\n",
    "    vague_train.to_csv(vague_test_folder + '/hf_train.csv', index=False)\n",
    "    \n",
    "    # create the corresponding vague-only test sample\n",
    "    rem_vague_1s = vague_1s.drop(vague_1s_sample.index) # we already took some vague examples for the train set; only keep the remaining once\n",
    "    rem_vague_0s = vague_0s.drop(vague_0s_sample.index)\n",
    "    vague_1s_sample = rem_vague_1s.sample(77)\n",
    "    vague_0s_sample = rem_vague_0s.sample(77)\n",
    "    vague_test = pd.concat([vague_1s_sample, vague_0s_sample]).sample(frac=1)\n",
    "    vague_test.to_csv(vague_test_folder + '/test.csv')\n",
    "    vague_test = hfify(vague_test)\n",
    "    vague_test.to_csv(vague_test_folder + '/hf_test.csv', index=False)\n",
    "    \n",
    "    # create another randomly sampled vague-only training sample for the unvague-only test sample\n",
    "    vague_1s_sample = vague_1s.sample(324)\n",
    "    vague_0s_sample = vague_0s.sample(291)\n",
    "    vague_train = pd.concat([vague_1s_sample, vague_0s_sample]).sample(frac=1)\n",
    "    vague_train.to_csv(unvague_test_folder + '/train.csv')\n",
    "    vague_train = hfify(vague_train)\n",
    "    vague_train.to_csv(unvague_test_folder + '/hf_train.csv', index=False)\n",
    "    \n",
    "    # create the corresponding unvague-only test sample\n",
    "    unvague_1s_sample = unvague_1s.sample(77)\n",
    "    unvague_0s_sample = unvague_0s.sample(77)\n",
    "    unvague_test = pd.concat([unvague_1s_sample, unvague_0s_sample]).sample(frac=1)\n",
    "    unvague_test.to_csv(unvague_test_folder + '/test.csv')\n",
    "    unvague_test = hfify(unvague_test)\n",
    "    unvague_test.to_csv(unvague_test_folder + '/hf_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcbe266f-d500-400a-bf29-249365284115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAGUE 1s 401\n",
      "VAGUE 0s 368\n",
      "UNVAGUE 1s 981\n",
      "UNVAGUE 0s 215\n"
     ]
    }
   ],
   "source": [
    "print(\"VAGUE 1s\", len(vague_1s))\n",
    "print(\"VAGUE 0s\", len(vague_0s))\n",
    "print(\"UNVAGUE 1s\", len(unvague_1s))\n",
    "print(\"UNVAGUE 0s\", len(unvague_0s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "601db8a0-ba5f-4d35-ac7a-937451f75b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the 10 splits for the training_UNvague_only test\n",
    "import os\n",
    "\n",
    "vet_corpus = pd.read_csv('VET_Corpus_0.1.csv', index_col = 0, encoding= 'utf-8') # corpus containing strong assumption PET-generalization\n",
    "vague_examples = vet_corpus.loc[vet_corpus['is_vague']==1]\n",
    "unvague_examples = vet_corpus.loc[vet_corpus['is_vague']==0]\n",
    "\n",
    "vague_1s = vague_examples.loc[vague_examples['is_euph']==1]\n",
    "vague_0s = vague_examples.loc[vague_examples['is_euph']==0]\n",
    "unvague_1s = unvague_examples.loc[unvague_examples['is_euph']==1]\n",
    "unvague_0s = unvague_examples.loc[unvague_examples['is_euph']==0]\n",
    "\n",
    "for x in range(0, 10):\n",
    "    vague_test_folder = 'Vagueness_Splits/Test_3/train_unvague_only/test_vague_only/test' + str(x)\n",
    "    unvague_test_folder = 'Vagueness_Splits/Test_3/train_unvague_only/test_unvague_only/test' + str(x)\n",
    "    os.mkdir(vague_test_folder)\n",
    "    os.mkdir(unvague_test_folder)\n",
    "    \n",
    "    # create the UNvague-only training sample for the vague-only test sample\n",
    "    unvague_1s_sample = unvague_1s.sample(477)\n",
    "    unvague_0s_sample = unvague_0s.sample(138)\n",
    "    unvague_train = pd.concat([unvague_1s_sample, unvague_0s_sample]).sample(frac=1)\n",
    "    unvague_train.to_csv(unvague_test_folder + '/train.csv')\n",
    "    unvague_train = hfify(unvague_train)\n",
    "    unvague_train.to_csv(unvague_test_folder + '/hf_train.csv', index=False)\n",
    "    \n",
    "    # create the corresponding UNvague-only test sample\n",
    "    rem_unvague_1s = unvague_1s.drop(unvague_1s_sample.index) # we already took some vague examples for the train set; only keep the remaining once\n",
    "    rem_unvague_0s = unvague_0s.drop(unvague_0s_sample.index)\n",
    "    unvague_1s_sample = rem_unvague_1s.sample(77)\n",
    "    unvague_0s_sample = rem_unvague_0s.sample(77)\n",
    "    unvague_test = pd.concat([unvague_1s_sample, unvague_0s_sample]).sample(frac=1)\n",
    "    unvague_test.to_csv(unvague_test_folder + '/test.csv')\n",
    "    unvague_test = hfify(unvague_test)\n",
    "    unvague_test.to_csv(unvague_test_folder + '/hf_test.csv', index=False)\n",
    "    \n",
    "    # create another randomly sampled UNvague-only training sample for the vague-only test sample\n",
    "    unvague_1s_sample = unvague_1s.sample(477)\n",
    "    unvague_0s_sample = unvague_0s.sample(138)\n",
    "    unvague_train = pd.concat([unvague_1s_sample, unvague_0s_sample]).sample(frac=1)\n",
    "    unvague_train.to_csv(vague_test_folder + '/train.csv')\n",
    "    unvague_train = hfify(unvague_train)\n",
    "    unvague_train.to_csv(vague_test_folder + '/hf_train.csv', index=False)\n",
    "    \n",
    "    # create the corresponding VAGUE-only test sample\n",
    "    vague_1s_sample = vague_1s.sample(77)\n",
    "    vague_0s_sample = vague_0s.sample(77)\n",
    "    vague_test = pd.concat([vague_1s_sample, vague_0s_sample]).sample(frac=1)\n",
    "    vague_test.to_csv(vague_test_folder + '/test.csv')\n",
    "    vague_test = hfify(vague_test)\n",
    "    vague_test.to_csv(vague_test_folder + '/hf_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144ced7-7368-4db3-ab4f-f4ddf72283de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the 10 splits for the training_MIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c57270e-6380-4b7c-b7f4-b1c02ddabcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to pull tests out of each folder and put them into one\n",
    "import shutil\n",
    "\n",
    "for x in range(0, 10):\n",
    "    vague_unvague_folder = 'Vagueness_Splits/Test_3/train_vague_only/test_unvague_only/test' + str(x)\n",
    "    # unvague_test_folder = 'Vagueness_Splits/Test_3/train_unvague_only/test_unvague_only/test' + str(x)\n",
    "    shutil.copyfile(vague_unvague_folder + '/hf_train.csv', 'Vagueness_Splits/Test_3/train_vague_only/test_unvague_only/hf_train_' + str(x) + '.csv')\n",
    "    shutil.copyfile(vague_unvague_folder + '/hf_test.csv', 'Vagueness_Splits/Test_3/train_vague_only/test_unvague_only/hf_test_' + str(x) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e4da4-5de2-47e2-a6a6-88cb417af394",
   "metadata": {},
   "source": [
    "#### Test Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10cbd32d-145c-42a7-bb4a-7debe135ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Investigate the PET distribution in the best vague-vague run\n",
    "\n",
    "best_run_fp = 'Vagueness_Splits/Test_3/train_unvague_only/test_unvague_only/test0'\n",
    "train_df = pd.read_csv(best_run_fp + '/train.csv', index_col=0)\n",
    "test_df = pd.read_csv(best_run_fp + '/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f040e9b0-0e60-4447-8157-c47fb2252c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df['type'].unique()))\n",
    "print(len(test_df['type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c51aeeb-70b5-4f0d-ba1f-b670637df4e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aging', 39),\n",
       " ('late', 34),\n",
       " ('expecting', 24),\n",
       " ('weed', 20),\n",
       " ('overweight', 20),\n",
       " ('sober', 15),\n",
       " ('demise', 13),\n",
       " ('plump', 12),\n",
       " ('capital punishment', 12),\n",
       " ('low-income', 12),\n",
       " ('exterminate', 12),\n",
       " ('dismissed', 12),\n",
       " ('ethnic cleansing', 12),\n",
       " ('perish', 11),\n",
       " ('undocumented immigrant', 11),\n",
       " ('less fortunate', 11),\n",
       " ('sex worker', 11),\n",
       " ('detainee', 11),\n",
       " ('advanced age', 11),\n",
       " ('economical', 10),\n",
       " ('elderly', 10),\n",
       " ('substance abuse', 9),\n",
       " ('correctional facility', 9),\n",
       " ('inebriated', 9),\n",
       " ('underprivileged', 9),\n",
       " ('same-sex', 9),\n",
       " ('indigent', 9),\n",
       " ('make love', 8),\n",
       " ('between jobs', 8),\n",
       " ('pro-choice', 8),\n",
       " ('deceased', 8),\n",
       " ('chest', 8),\n",
       " ('pro-life', 8),\n",
       " ('pass away', 7),\n",
       " ('homemaker', 7),\n",
       " ('psychiatric hospital', 7),\n",
       " ('droppings', 7),\n",
       " ('senior citizen', 7),\n",
       " ('let [pro] go', 7),\n",
       " ('a certain age', 7),\n",
       " ('slim', 7),\n",
       " ('people/persons of color', 7),\n",
       " ('rear end', 7),\n",
       " ('portly', 6),\n",
       " ('fatality', 6),\n",
       " ('sleep with', 6),\n",
       " ('wealthy', 6),\n",
       " ('targeted killing', 5),\n",
       " ('substance abuser', 5),\n",
       " ('well off', 5),\n",
       " ('intoxicated', 5),\n",
       " ('stout', 5),\n",
       " ('undocumented workers', 5),\n",
       " ('mixed up', 5),\n",
       " ('birds and the bees', 4),\n",
       " ('lavatory', 4),\n",
       " ('pregnancy termination', 4),\n",
       " ('detention camp', 4),\n",
       " ('time of the month', 3),\n",
       " ('seasoned', 3),\n",
       " ('drinking problem', 3),\n",
       " ('enhanced interrogation techniques', 3),\n",
       " ('getting clean', 2),\n",
       " ('pass on', 2),\n",
       " ('latrine', 2),\n",
       " ('invalid', 2),\n",
       " ('same sex', 2),\n",
       " ('custodian', 2),\n",
       " ('tinkle', 2),\n",
       " ('with child', 2),\n",
       " ('neutralize', 2),\n",
       " ('lose [pro] lunch', 2),\n",
       " ('hearing impaired', 2),\n",
       " ('went to heaven', 1),\n",
       " ('let go of', 1),\n",
       " ('adult beverage', 1),\n",
       " ('got clean', 1),\n",
       " ('plus-sized', 1),\n",
       " ('put to sleep', 1),\n",
       " ('street person', 1),\n",
       " ('deprived', 1),\n",
       " ('dearly departed', 1),\n",
       " ('gluteus maximus', 1),\n",
       " ('oldest profession', 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "\n",
    "for PET in train_df['type'].unique():\n",
    "    rows = train_df.loc[train_df['type'] == PET]\n",
    "    l.append((PET, len(rows)))\n",
    "    \n",
    "sorted(l, key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6ac8084-0395-4ac2-9b14-454c799f193d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('late', 16),\n",
       " ('dismissed', 10),\n",
       " ('expecting', 8),\n",
       " ('aging', 8),\n",
       " ('overweight', 8),\n",
       " ('exterminate', 7),\n",
       " ('plump', 6),\n",
       " ('sober', 5),\n",
       " ('let [pro] go', 5),\n",
       " ('same-sex', 4),\n",
       " ('droppings', 3),\n",
       " ('homemaker', 3),\n",
       " ('detainee', 3),\n",
       " ('elderly', 3),\n",
       " ('weed', 3),\n",
       " ('perish', 3),\n",
       " ('undocumented workers', 3),\n",
       " ('targeted killing', 3),\n",
       " ('substance abuse', 3),\n",
       " ('capital punishment', 3),\n",
       " ('detention camp', 3),\n",
       " ('chest', 2),\n",
       " ('seasoned', 2),\n",
       " ('birds and the bees', 2),\n",
       " ('pro-life', 2),\n",
       " ('stout', 2),\n",
       " ('a certain age', 2),\n",
       " ('pass away', 2),\n",
       " ('correctional facility', 2),\n",
       " ('pro-choice', 2),\n",
       " ('sleep with', 2),\n",
       " ('advanced age', 2),\n",
       " ('enhanced interrogation techniques', 2),\n",
       " ('people/persons of color', 2),\n",
       " ('custodian', 1),\n",
       " ('put to sleep', 1),\n",
       " ('ethnic cleansing', 1),\n",
       " ('undocumented immigrant', 1),\n",
       " ('senior citizen', 1),\n",
       " ('pass on', 1),\n",
       " ('between jobs', 1),\n",
       " ('low-income', 1),\n",
       " ('mixed up', 1),\n",
       " ('underprivileged', 1),\n",
       " ('invalid', 1),\n",
       " ('less fortunate', 1),\n",
       " ('deprived', 1),\n",
       " ('sex worker', 1),\n",
       " ('with child', 1),\n",
       " ('gluteus maximus', 1),\n",
       " ('time of the month', 1),\n",
       " ('deceased', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[]\n",
    "\n",
    "for PET in test_df['type'].unique():\n",
    "    rows = test_df.loc[test_df['type'] == PET]\n",
    "    l.append((PET, len(rows)))\n",
    "    \n",
    "sorted(l, key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "777cd0ea-76f6-4fd4-b159-3816d433484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'developmentally disabled', 'income inequality', 'slim', 'mentally disabled', 'intoxicated', 'special needs', 'long sleep', 'underdeveloped', 'regime change', 'accident', 'perish', 'demise', 'experienced', 'collateral damage', 'getting clean', 'weed', 'let go of', 'well off', 'inner city', 'global south', 'golden years', 'armed conflict', 'over the hill', 'freedom fighter', 'custodian', 'custodians', 'a certain age', 'neutralize', 'negative cash flow', 'troubled', 'sanitation worker', 'outspoken', 'mixed up', 'pass on', 'disabled', 'lay off', 'economical', 'pre-owned', 'go all the way', 'mentally challenged', 'disadvantaged', 'developed/ing country', 'pass away'}\n"
     ]
    }
   ],
   "source": [
    "print(set(train_df['type'].unique()).intersection(test_df['type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2faa7aaf-7c5e-43ca-9be3-47717837d6aa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('disabled', 46),\n",
       " ('collateral damage', 45),\n",
       " ('lay off', 30),\n",
       " ('troubled', 24),\n",
       " ('disadvantaged', 24),\n",
       " ('weed', 24),\n",
       " ('accident', 24),\n",
       " ('demise', 22),\n",
       " ('underdeveloped', 18),\n",
       " ('inner city', 18),\n",
       " ('special needs', 18),\n",
       " ('income inequality', 18),\n",
       " ('sanitation worker', 16),\n",
       " ('freedom fighter', 15),\n",
       " ('developed/ing country', 15),\n",
       " ('armed conflict', 15),\n",
       " ('pass away', 14),\n",
       " ('economical', 13),\n",
       " ('perish', 13),\n",
       " ('golden years', 13),\n",
       " ('intoxicated', 12),\n",
       " ('mentally challenged', 12),\n",
       " ('slim', 12),\n",
       " ('regime change', 11),\n",
       " ('over the hill', 10),\n",
       " ('well off', 9),\n",
       " ('mentally disabled', 9),\n",
       " ('mixed up', 8),\n",
       " ('neutralize', 8),\n",
       " ('go all the way', 8),\n",
       " ('a certain age', 7),\n",
       " ('global south', 7),\n",
       " ('venereal disease', 6),\n",
       " ('custodian', 6),\n",
       " ('experienced', 6),\n",
       " ('pre-owned', 5),\n",
       " ('able-bodied', 5),\n",
       " ('pass on', 5),\n",
       " ('let go of', 4),\n",
       " ('outspoken', 4),\n",
       " ('downsize', 4),\n",
       " ('seeing someone/each other', 4),\n",
       " ('comfort women', 3),\n",
       " ('invalid', 3),\n",
       " ('put to sleep', 3),\n",
       " ('negative cash flow', 3),\n",
       " ('outlived [pro] usefulness', 3),\n",
       " ('getting clean', 2),\n",
       " ('differently-abled', 2),\n",
       " ('custodians', 1),\n",
       " ('developmentally disabled', 1),\n",
       " ('to go to heaven', 1),\n",
       " ('long sleep', 1),\n",
       " ('broken home', 1),\n",
       " ('economical with the truth', 1),\n",
       " ('running behind', 1),\n",
       " ('went to heaven', 1),\n",
       " ('physically challenged', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate the PET distribution in the best vague-unvague run\n",
    "\n",
    "best_run_fp = 'Vagueness_Splits/Test_3/train_vague_only/test_unvague_only/test0'\n",
    "train_df = pd.read_csv(best_run_fp + '/train.csv', index_col=0)\n",
    "test_df = pd.read_csv(best_run_fp + '/test.csv', index_col=0)\n",
    "\n",
    "l = []\n",
    "\n",
    "for PET in train_df['type'].unique():\n",
    "    rows = train_df.loc[train_df['type'] == PET]\n",
    "    l.append((PET, len(rows)))\n",
    "    \n",
    "sorted(l, key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32d91996-bc8b-4f59-bdbf-ae9391a36cfc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('late', 16),\n",
       " ('dismissed', 10),\n",
       " ('expecting', 8),\n",
       " ('aging', 8),\n",
       " ('overweight', 8),\n",
       " ('exterminate', 7),\n",
       " ('plump', 6),\n",
       " ('sober', 5),\n",
       " ('let [pro] go', 5),\n",
       " ('same-sex', 4),\n",
       " ('droppings', 3),\n",
       " ('homemaker', 3),\n",
       " ('detainee', 3),\n",
       " ('elderly', 3),\n",
       " ('weed', 3),\n",
       " ('perish', 3),\n",
       " ('undocumented workers', 3),\n",
       " ('targeted killing', 3),\n",
       " ('substance abuse', 3),\n",
       " ('capital punishment', 3),\n",
       " ('detention camp', 3),\n",
       " ('chest', 2),\n",
       " ('seasoned', 2),\n",
       " ('birds and the bees', 2),\n",
       " ('pro-life', 2),\n",
       " ('stout', 2),\n",
       " ('a certain age', 2),\n",
       " ('pass away', 2),\n",
       " ('correctional facility', 2),\n",
       " ('pro-choice', 2),\n",
       " ('sleep with', 2),\n",
       " ('advanced age', 2),\n",
       " ('enhanced interrogation techniques', 2),\n",
       " ('people/persons of color', 2),\n",
       " ('custodian', 1),\n",
       " ('put to sleep', 1),\n",
       " ('ethnic cleansing', 1),\n",
       " ('undocumented immigrant', 1),\n",
       " ('senior citizen', 1),\n",
       " ('pass on', 1),\n",
       " ('between jobs', 1),\n",
       " ('low-income', 1),\n",
       " ('mixed up', 1),\n",
       " ('underprivileged', 1),\n",
       " ('invalid', 1),\n",
       " ('less fortunate', 1),\n",
       " ('deprived', 1),\n",
       " ('sex worker', 1),\n",
       " ('with child', 1),\n",
       " ('gluteus maximus', 1),\n",
       " ('time of the month', 1),\n",
       " ('deceased', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[]\n",
    "\n",
    "for PET in test_df['type'].unique():\n",
    "    rows = test_df.loc[test_df['type'] == PET]\n",
    "    l.append((PET, len(rows)))\n",
    "    \n",
    "sorted(l, key=lambda tup: tup[1], reverse=True)\n",
    "print(*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fb17727-89a2-4492-a966-921a2074d229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'invalid', 'custodian', 'weed', 'pass on', 'mixed up', 'a certain age', 'perish', 'pass away', 'put to sleep'}\n"
     ]
    }
   ],
   "source": [
    "print(set(train_df['type'].unique()).intersection(test_df['type'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3b483-87a0-4908-a31b-1d15cda7c6f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TEST_4 - Same as TEST_3, but restricting the sets to the same number of PETs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ebd807f-06a9-40cc-8be2-0b84fc5d683e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('aging', 60) ('late', 60) ('expecting', 46) ('overweight', 38) ('exterminate', 30) ('weed', 30) ('demise', 28) ('dismissed', 26) ('sober', 22) ('undocumented immigrant', 20) ('sex worker', 20) ('detainee', 20) ('ethnic cleansing', 20) ('elderly', 20) ('deceased', 20) ('pro-life', 20) ('substance abuse', 20) ('senior citizen', 20) ('people/persons of color', 20) ('fatality', 20) ('pro-choice', 20) ('low-income', 20) ('less fortunate', 20) ('perish', 20) ('chest', 20) ('plump', 20) ('homemaker', 19) ('capital punishment', 19) ('advanced age', 19) ('correctional facility', 18) ('indigent', 18) ('droppings', 18) ('pass away', 18) ('inebriated', 16) ('same-sex', 16) ('intoxicated', 16) ('let [pro] go', 14) ('economical', 14) ('between jobs', 14) ('undocumented workers', 13) ('stout', 12) ('sleep with', 12) ('psychiatric hospital', 11) ('underprivileged', 11) ('substance abuser', 11) ('targeted killing', 11) ('make love', 11) ('slim', 11) ('mixed up', 11) ('well off', 11) ('a certain age', 11) ('rear end', 10) ('detention camp', 10) ('wealthy', 10) ('neutralize', 8) ('drinking problem', 7) ('lavatory', 7) ('birds and the bees', 7) ('portly', 7) ('put to sleep', 7) ('enhanced interrogation techniques', 6) ('pass on', 6) ('custodian', 6) ('time of the month', 5) ('let go of', 5) ('lose [pro] lunch', 4) ('mistruth', 4) ('pregnancy termination', 4) ('same sex', 4) ('deprived', 4) ('seasoned', 4) ('with child', 4) ('dearly departed', 3) ('street person', 3) ('latrine', 3) ('invalid', 3) ('tinkle', 2) ('plus-sized', 2) ('hearing impaired', 2) ('gluteus maximus', 2) ('got clean', 2) ('oldest profession', 2) ('getting clean', 2) ('pass gas', 1) ('under the weather', 1) ('adult beverage', 1) ('to go to heaven', 1) ('went to heaven', 1) ('sleep around', 1)\n"
     ]
    }
   ],
   "source": [
    "VET_corpus = pd.read_csv('VET_Corpus_0.1.csv', index_col=0)\n",
    "l = []\n",
    "unvague_examples = VET_corpus.loc[VET_corpus['is_vague']==0]\n",
    "for PET in unvague_examples['type'].unique():\n",
    "    rows = unvague_examples.loc[unvague_examples['type'] == PET]\n",
    "    l.append((PET, len(rows)))\n",
    "l = sorted(l, key=lambda tup: tup[1], reverse=True)\n",
    "print(*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "332e2772-b68b-4241-bb5b-456d717c571c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "100\n",
      "60\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "VET_corpus = pd.read_csv('VET_Corpus_0.1.csv', index_col=0)\n",
    "l = []\n",
    "\n",
    "# Here, we form the components for an unvague-unvague test.\n",
    "unvague_examples = VET_corpus.loc[VET_corpus['is_vague']==0]\n",
    "unvague_1s = unvague_examples.loc[unvague_examples['is_euph']==1]\n",
    "unvague_0s = unvague_examples.loc[unvague_examples['is_euph']==0]\n",
    "\n",
    "# form 1s\n",
    "sample_vague_PETs = random.sample(unvague_1s['type'].unique().tolist(), 55)\n",
    "PET_controlled_unvague_1s = unvague_1s.loc[unvague_1s['type'].isin(sample_vague_PETs)]\n",
    "sample_unvague_1s = PET_controlled_unvague_1s.sample(400)\n",
    "train_1s = sample_unvague_1s.sample(350)\n",
    "test_1s = sample_unvague_1s.drop(train_1s.index)\n",
    "\n",
    "# form 0s\n",
    "sample_unvague_0s = unvague_0s.sample(200)\n",
    "train_0s = sample_unvague_0s.sample(150)\n",
    "test_0s = sample_unvague_0s.drop(train_0s.index)\n",
    "# vague_train = hfify(vague_train)\n",
    "\n",
    "train = pd.concat([train_1s, train_0s]).sample(frac=1)\n",
    "test = pd.concat([test_1s, test_0s]).sample(frac=1)\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "# print(len(train_1s))\n",
    "# print(len(test_1s))\n",
    "# print(len(train_0s))\n",
    "# print(len(test_0s))\n",
    "# Let's double check\n",
    "print(len(train['type'].unique()))\n",
    "print(len(test['type'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cd14bdfd-d8b8-4200-b844-16a208dd9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's double check\n",
    "# # print(len(train.loc[train['is_vague']==0]))\n",
    "import os\n",
    "path = 'Vagueness_Splits/Test_4/unvague_unvague_test'\n",
    "train.to_csv(path + '/train.csv')\n",
    "hfify(train).to_csv(path + '/hf_train.csv')\n",
    "test.to_csv(path + '/test.csv')\n",
    "hfify(test).to_csv(path + '/hf_test.csv')\n",
    "# train = VET_corpus.loc[l, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fb7c5e23-20ea-4ef3-a8cb-917a73966769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "100\n",
      "59\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# Here, we form the components for a vague-vague-test, for comparison. Will it still do a lot beter? \n",
    "VET_corpus = pd.read_csv('VET_Corpus_0.1.csv', index_col=0)\n",
    "vague_examples = VET_corpus.loc[VET_corpus['is_vague']==1]\n",
    "vague_1s = vague_examples.loc[vague_examples['is_euph']==1]\n",
    "vague_0s = vague_examples.loc[vague_examples['is_euph']==0]\n",
    "\n",
    "train_1s = vague_1s.sample(350)\n",
    "test_1s = vague_1s.drop(train_1s.index).sample(50)\n",
    "train_0s = vague_0s.sample(150)\n",
    "test_0s = vague_0s.drop(train_0s.index).sample(50)\n",
    "\n",
    "train = pd.concat([train_1s, train_0s]).sample(frac=1)\n",
    "test = pd.concat([test_1s, test_0s]).sample(frac=1)\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "print(len(train['type'].unique()))\n",
    "print(len(test['type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1bb5e93e-e15d-4153-a765-8c15d0ae9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'Vagueness_Splits/Test_4/vague_vague_test'\n",
    "train.to_csv(path + '/train.csv')\n",
    "hfify(train).to_csv(path + '/hf_train.csv')\n",
    "test.to_csv(path + '/test.csv')\n",
    "hfify(test).to_csv(path + '/hf_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "52a6e179-29c4-4c39-af8b-e708de28b06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([], dtype='int64')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.index.intersection(test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbef94ef-e113-40dd-bee4-0aa3ee911978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('disabled', 60) ('collateral damage', 52) ('lay off', 36) ('accident', 32) ('troubled', 30) ('weed', 30) ('disadvantaged', 28) ('demise', 28) ('special needs', 26) ('underdeveloped', 26) ('freedom fighter', 20) ('armed conflict', 20) ('income inequality', 20) ('inner city', 20) ('sanitation worker', 20) ('perish', 20) ('developed/ing country', 19) ('pass away', 18) ('mentally challenged', 17) ('golden years', 17) ('intoxicated', 16) ('regime change', 14) ('economical', 14) ('slim', 13) ('over the hill', 12) ('mentally disabled', 11) ('mixed up', 11) ('well off', 11) ('a certain age', 11) ('go all the way', 10) ('global south', 8) ('neutralize', 8) ('pre-owned', 7) ('able-bodied', 7) ('venereal disease', 6) ('experienced', 6) ('outspoken', 6) ('pass on', 6) ('custodian', 6) ('let go of', 5) ('downsize', 4) ('outlived [pro] usefulness', 4) ('seeing someone/each other', 4) ('comfort women', 3) ('negative cash flow', 3) ('invalid', 3) ('put to sleep', 3) ('differently-abled', 2) ('developmentally disabled', 2) ('custodians', 2) ('long sleep', 2) ('getting clean', 2) ('economical with the truth', 1) ('full figured', 1) ('physically challenged', 1) ('broken home', 1) ('running behind', 1) ('sleep around', 1) ('to go to heaven', 1) ('went to heaven', 1)\n"
     ]
    }
   ],
   "source": [
    "# Here, we get an idea of the number of examples per PET. Idk, just cuz.\n",
    "VET_Corpus = pd.read_csv('VET_Corpus_0.1.csv', index_col=0)\n",
    "l = []\n",
    "VET_Corpus = VET_Corpus.loc[VET_Corpus['is_vague']==1]\n",
    "for PET in VET_Corpus['type'].unique():\n",
    "    rows = VET_Corpus.loc[VET_Corpus['type'] == PET]\n",
    "    l.append((PET, len(rows)))\n",
    "l = sorted(l, key=lambda tup: tup[1], reverse=True)\n",
    "print(*l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b1cce-5fa9-4453-81c4-77a717626f90",
   "metadata": {},
   "source": [
    "### TEST_5 - Mixed-mixed Somewhat Balanced (for comparison with Balanced mixed-mixed, that was made on the server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a983ef-fac8-4d2a-b177-410c1dd309c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will cut down on #examples from the UV1 category, 981, to 381. Also, cut down on 368 V0s to 218 (to compensate for lack of UV0s). If we do this, then:\n",
    "# there are 1212 examples total, 20% of which is about 240 for the test set. \n",
    "# We're interested in evaluating vague/unvague performance in the test set, so it will be balanced by taking 240/4=60 examples from each of V1/V0/UV1/UV0.\n",
    "# Then, in the training set, there will be 401-60=341 V1s, 218-60=158 V0s, 381-60=321 UV1s, and 215-60=155 UV0s.  \n",
    "\n",
    "import os\n",
    "\n",
    "vet_corpus = pd.read_csv('VET_Corpus_0.1.csv', index_col = 0, encoding= 'utf-8') # corpus containing strong assumption PET-generalization\n",
    "vague_examples = vet_corpus.loc[vet_corpus['is_vague']==1]\n",
    "unvague_examples = vet_corpus.loc[vet_corpus['is_vague']==0]\n",
    "vague_1s = vague_examples.loc[vague_examples['is_euph']==1]\n",
    "vague_0s = vague_examples.loc[vague_examples['is_euph']==0]\n",
    "unvague_1s = unvague_examples.loc[unvague_examples['is_euph']==1]\n",
    "unvague_0s = unvague_examples.loc[unvague_examples['is_euph']==0]\n",
    "\n",
    "for x in range(0, 10):\n",
    "    dest_path = 'Vagueness_Splits/Test_5/PET_less_balanced_mixed_mixed/'\n",
    "    test_V1s = vague_1s.sample(60)\n",
    "    test_V0s = vague_0s.sample(60)\n",
    "    test_UV1s = unvague_1s.sample(60)\n",
    "    test_UV0s = unvague_0s.sample(60)\n",
    "    # now form the training set\n",
    "    train_V1s = vague_1s.drop(test_V1s.index).sample(314)\n",
    "    train_V0s = vague_0s.drop(test_V0s.index).sample(158)\n",
    "    train_UV1s = unvague_1s.drop(test_UV1s.index)\n",
    "    train_UV0s = unvague_0s.drop(test_UV0s.index)\n",
    "    # now combine the sets\n",
    "    train_set = pd.concat([train_V1s, train_V0s, train_UV1s, train_UV0s]).sample(frac=1)\n",
    "    test_set = pd.concat([test_V1s, test_V0s, test_UV1s, test_UV0s]).sample(frac=1)\n",
    "    train_set.to_csv(dest_path + 'train' + str(x) + '.csv')\n",
    "    test_set.to_csv(dest_path + 'test' + str(x) + '.csv')\n",
    "    hfify(train_set).to_csv(dest_path + 'hf_train_' + str(x) + '.csv', index=False)\n",
    "    hfify(test_set).to_csv(dest_path + 'hf_test_' + str(x) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540096b-10c7-4738-a512-3c3ba70fa293",
   "metadata": {},
   "source": [
    "### TEST_5 - Mini-test on the 1st annotation sample only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "294e86c4-cb08-42cf-8100-d610b575a146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "28\n",
      "86\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Annotation_Task1_Analysis_v2.csv', index_col=0)\n",
    "\n",
    "vague = df.loc[df['is_vague'] == 1]\n",
    "vague_euphs = vague.loc[vague['label'] == 1]\n",
    "vague_noneuphs = vague.loc[vague['label'] == 0]\n",
    "print(len(vague_euphs))\n",
    "print(len(vague_noneuphs))\n",
    "\n",
    "unvague = df.loc[df['is_vague'] == 0]\n",
    "unvague_euphs = unvague.loc[unvague['label'] == 1]\n",
    "unvague_noneuphs = unvague.loc[unvague['label'] == 0]\n",
    "print(len(unvague_euphs))\n",
    "print(len(unvague_noneuphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57e6b186-ec8c-4385-bae3-54ddf1525685",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13360/1163359960.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0ms3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvague_euphs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0ms4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvague_noneuphs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# normally, we'd want to save the train and test subsets before removing columns, but we'll skip for now...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\euphemisms\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[0;32m   5452\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5454\u001b[1;33m         \u001b[0msampled_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5455\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\euphemisms\\lib\\site-packages\\pandas\\core\\sample.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid weights: weights sum to zero\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m     return random_state.choice(obj_len, size=size, replace=replace, p=weights).astype(\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     )\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# make the 10 splits for the training_vague_only test\n",
    "vague_vague_folder = 'Vagueness_Splits/Test_4/vague_vague_test' \n",
    "vague_unvague_folder = 'Vagueness_Splits/Test_4/vague_unvague_test'\n",
    "unvague_vague_folder = 'Vagueness_Splits/Test_4/unvague_vague_test' \n",
    "unvague_unvague_folder = 'Vagueness_Splits/Test_4/unvague_unvague_test' \n",
    "\n",
    "os.mkdir(vague_vague_folder)\n",
    "os.mkdir(vague_unvague_folder)\n",
    "os.mkdir(unvague_vague_folder)\n",
    "os.mkdir(unvague_unvague_folder)\n",
    "\n",
    "for x in range(0, 10): # create 10 copies of each test\n",
    "    # create a vague-vague test\n",
    "    s1 = vague_euphs.sample(28)\n",
    "    s2 = vague_noneuphs.sample(28)\n",
    "    train = pd.concat([s1, s2]).sample(frac=1)\n",
    "    s3 = vague_euphs.drop(s1.index).sample(7)\n",
    "    s4 = vague_noneuphs.drop(s2.index).sample(7)\n",
    "    test = pd.concat([s3, s4]).sample(frac=1)\n",
    "    # normally, we'd want to save the train and test subsets before removing columns, but we'll skip for now...\n",
    "    train = hfify(train)\n",
    "    test = hfify(test)\n",
    "    train.to_csv(vague_vague_folder + '/hf_train_' + str(x) + '.csv', index=False)\n",
    "    train.to_csv(vague_vague_folder + '/hf_test_' + str(x) + '.csv', index=False)\n",
    "    \n",
    "    # create a vague-unvague test\n",
    "    s1 = vague_euphs.sample(28)\n",
    "    s2 = vague_noneuphs.sample(28)\n",
    "    train = pd.concat([s1, s2]).sample(frac=1)\n",
    "    s3 = unvague_euphs.sample(7)\n",
    "    s4 = unvague_noneuphs.sample(7)\n",
    "    test = pd.concat([s3, s4]).sample(frac=1)\n",
    "    # normally, we'd want to save the train and test subsets before removing columns, but we'll skip for now...\n",
    "    train = hfify(train)\n",
    "    test = hfify(test)\n",
    "    train.to_csv(vague_unvague_folder + '/hf_train_' + str(x) + '.csv', index=False)\n",
    "    train.to_csv(vague_unvague_folder + '/hf_test_' + str(x) + '.csv', index=False)\n",
    "    \n",
    "    # create an unvague-vague test\n",
    "    s1 = unvague_euphs.sample(28)\n",
    "    s2 = unvague_noneuphs.sample(28)\n",
    "    train = pd.concat([s1, s2]).sample(frac=1)\n",
    "    s3 = vague_euphs.sample(7)\n",
    "    s4 = vague_noneuphs.sample(7)\n",
    "    test = pd.concat([s3, s4]).sample(frac=1)\n",
    "    # normally, we'd want to save the train and test subsets before removing columns, but we'll skip for now...\n",
    "    train = hfify(train)\n",
    "    test = hfify(test)\n",
    "    train.to_csv(unvague_vague_folder + '/hf_train_' + str(x) + '.csv', index=False)\n",
    "    train.to_csv(unvague_vague_folder + '/hf_test_' + str(x) + '.csv', index=False)\n",
    "    \n",
    "    # create an unvague-unvague test\n",
    "    s1 = unvague_euphs.sample(28)\n",
    "    s2 = unvague_noneuphs.sample(28)\n",
    "    train = pd.concat([s1, s2]).sample(frac=1)\n",
    "    s3 = unvague_euphs.drop(s1.index).sample(7)\n",
    "    s4 = unvague_noneuphs.drop(s2.index).sample(7)\n",
    "    test = pd.concat([s3, s4]).sample(frac=1)\n",
    "    # normally, we'd want to save the train and test subsets before removing columns, but we'll skip for now...\n",
    "    train = hfify(train)\n",
    "    test = hfify(test)\n",
    "    train.to_csv(unvague_unvague_folder + '/hf_train_' + str(x) + '.csv', index=False)\n",
    "    train.to_csv(unvague_unvague_folder + '/hf_test_' + str(x) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4f840-0ab8-4d1f-9655-612a6b7f3ef6",
   "metadata": {},
   "source": [
    "## Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50465441-99df-4615-96f0-14b1d862c995",
   "metadata": {},
   "source": [
    "### TEST_5 (Mixed-Mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbdddfa2-754b-4380-9dc4-61814a370680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>F1-V</th>\n",
       "      <th>P-V</th>\n",
       "      <th>R-V</th>\n",
       "      <th>F1-UV</th>\n",
       "      <th>P-UV</th>\n",
       "      <th>R-UV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.754959</td>\n",
       "      <td>0.823899</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.737762</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.766900</td>\n",
       "      <td>0.849315</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.796491</td>\n",
       "      <td>0.863946</td>\n",
       "      <td>0.907143</td>\n",
       "      <td>40.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.851190</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.742363</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.871429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.768964</td>\n",
       "      <td>0.837662</td>\n",
       "      <td>0.921429</td>\n",
       "      <td>35.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.797057</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.740125</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.914286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.694157</td>\n",
       "      <td>0.786982</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.737762</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.652343</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.914286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>34.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.870062</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.698465</td>\n",
       "      <td>0.792683</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.721003</td>\n",
       "      <td>0.803681</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.717674</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.723648</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.914286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.768277</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.835714</td>\n",
       "      <td>43.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.786834</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.748881</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.757143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.774295</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.907143</td>\n",
       "      <td>37.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.828925</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.723524</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.842857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.751984</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.836991</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.668752</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>0.828571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.789446</td>\n",
       "      <td>0.841772</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.870062</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.914286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         F1         P         R    tn    fp    fn     tp      F1-V       P-V  \\\n",
       "0  0.754959  0.823899  0.935714  32.0   9.0  28.0  131.0  0.737762  0.802326   \n",
       "1  0.796491  0.863946  0.907143  40.0  13.0  20.0  127.0  0.851190  0.891892   \n",
       "2  0.768964  0.837662  0.921429  35.0  11.0  25.0  129.0  0.797057  0.855263   \n",
       "3  0.694157  0.786982  0.950000  24.0   7.0  36.0  133.0  0.737762  0.802326   \n",
       "4  0.786667  0.837500  0.957143  34.0   6.0  26.0  134.0  0.870062  0.884615   \n",
       "5  0.721003  0.803681  0.935714  28.0   9.0  32.0  131.0  0.717674  0.797619   \n",
       "6  0.768277  0.873134  0.835714  43.0  23.0  17.0  117.0  0.786834  0.853333   \n",
       "7  0.774295  0.846667  0.907143  37.0  13.0  23.0  127.0  0.828925  0.860759   \n",
       "8  0.751984  0.837838  0.885714  36.0  16.0  24.0  124.0  0.836991  0.880000   \n",
       "9  0.789446  0.841772  0.950000  35.0   7.0  25.0  133.0  0.870062  0.884615   \n",
       "\n",
       "        R-V     F1-UV      P-UV      R-UV  \n",
       "0  0.985714  0.766900  0.849315  0.885714  \n",
       "1  0.942857  0.742363  0.835616  0.871429  \n",
       "2  0.928571  0.740125  0.820513  0.914286  \n",
       "3  0.985714  0.652343  0.771084  0.914286  \n",
       "4  0.985714  0.698465  0.792683  0.928571  \n",
       "5  0.957143  0.723648  0.810127  0.914286  \n",
       "6  0.914286  0.748881  0.898305  0.757143  \n",
       "7  0.971429  0.723524  0.830986  0.842857  \n",
       "8  0.942857  0.668752  0.794521  0.828571  \n",
       "9  0.985714  0.706667  0.800000  0.914286  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "path = 'Vagueness_Splits/Test_5/PET_balanced_mixed_mixed'\n",
    "df = pd.read_csv(path + '/PET_balanced_mixed_mixed_results_unscrambled.csv', index_col=0)\n",
    "results = pd.DataFrame(columns=['F1', 'P', 'R', 'tn', 'fp', 'fn', 'tp', 'F1-V', 'P-V', 'R-V', 'F1-UV', 'P-UV', 'R-UV'])\n",
    "\n",
    "# for each test, select the row with the best F1, then evaluate separate F1s for vague vs unvague examples\n",
    "for x in range(0, 10):\n",
    "    test = df.loc[10*x:10*x+9]\n",
    "    max_f1 = test.loc[test['f1'].idxmax()] # this is the best row from this test\n",
    "    \n",
    "    best_preds = max_f1['preds'].replace(' ', ', ') # the labels don't have a comma between them...\n",
    "    best_preds = ast.literal_eval(best_preds)\n",
    "    \n",
    "    ref_df = pd.read_csv(path + '/test_' + str(x) + '.csv')\n",
    "    # print(ref_df['is_euph'].tolist())\n",
    "    # approach: get row IDs of vague/unvague rows from test file, then pick the preds at the corresponding indices for evaluation\n",
    "    vagues = ref_df.loc[ref_df['is_vague']==1]\n",
    "    unvagues = ref_df.loc[ref_df['is_vague']==0]\n",
    "    # get predictions\n",
    "    vague_preds = [best_preds[i] for i in vagues.index.tolist()]\n",
    "    unvague_preds = [best_preds[i] for i in unvagues.index.tolist()]\n",
    "    # get labels\n",
    "    vague_labels = vagues['is_euph'].tolist()\n",
    "    unvague_labels = unvagues['is_euph'].tolist()\n",
    "    # load metrics\n",
    "    # metric_acc = evaluate.load(\"accuracy\")\n",
    "    metric_f1 = evaluate.load(\"f1\")\n",
    "    metric_p = evaluate.load(\"precision\")\n",
    "    metric_r = evaluate.load(\"recall\")\n",
    "    # vague examples\n",
    "    vague_f1 = metric_f1.compute(predictions=vague_preds, \n",
    "                       references=vague_labels, \n",
    "                       average='macro')['f1']\n",
    "    vague_p = metric_p.compute(predictions=vague_preds, \n",
    "                          references=vague_labels)['precision']\n",
    "    vague_r = metric_r.compute(predictions=vague_preds, \n",
    "                          references=vague_labels)['recall']\n",
    "\n",
    "    # unvague examples\n",
    "    unvague_f1 = metric_f1.compute(predictions=unvague_preds, \n",
    "                       references=unvague_labels, \n",
    "                       average='macro')['f1']\n",
    "    unvague_p = metric_p.compute(predictions=unvague_preds, \n",
    "                          references=unvague_labels)['precision']\n",
    "    unvague_r = metric_r.compute(predictions=unvague_preds, \n",
    "                          references=unvague_labels)['recall']\n",
    "\n",
    "    # all examples, just to make sure it matches original scores\n",
    "    # vague_acc = metric_acc.compute(predictions=best_preds, \n",
    "    #                           references=ref_df['is_euph'].tolist())\n",
    "    # vague_f1 = metric_f1.compute(predictions=best_preds, \n",
    "    #                    references=ref_df['is_euph'].tolist(), \n",
    "    #                    average='macro')\n",
    "    # print(vague_f1)\n",
    "    # print(vague_acc)\n",
    "    # print()\n",
    "    # add to an overall dataframe\n",
    "    stats = max_f1[0:7].tolist() # take the base stats from the best row\n",
    "    stats.extend([vague_f1, vague_p, vague_r, unvague_f1, unvague_p, unvague_r]) # add on the vague/unvague ones\n",
    "    results.loc[len(results.index)] = stats\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c4288b-a0d8-4248-9caf-cbef295f139e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20aebfd4-c6ef-4847-9b87-8d437c1169b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "# Correct hf_train and hf_test, idk lol\n",
    "import pandas as pd\n",
    "\n",
    "corpus = pd.read_csv('Euphemism_Corpus_v2.0.csv')\n",
    "train_df = pd.read_csv('BERT/hf_train.csv')\n",
    "test_df = pd.read_csv('BERT/hf_test.csv')\n",
    "\n",
    "counter = 0\n",
    "# i know this loop is an efficiency nightmare but i am too lazy and it prob doesn't matter\n",
    "for i, row in corpus.iterrows():\n",
    "    text = corpus.loc[i]['edited_text']\n",
    "    label = corpus.loc[i]['is_euph']\n",
    "    for j, row in train_df.iterrows():\n",
    "        a = train_df.loc[j]['text']\n",
    "        if (text == a):\n",
    "            l = train_df.loc[j]['label']\n",
    "            if (l != label):\n",
    "                train_df.loc[j, 'label'] = label\n",
    "                counter += 1\n",
    "            break\n",
    "    for k, row in test_df.iterrows():\n",
    "        b = test_df.loc[k]['text']\n",
    "        if (text == b):\n",
    "            l = test_df.loc[k]['label']\n",
    "            if (l != label):\n",
    "                test_df.loc[k, 'label'] = label\n",
    "                counter += 1\n",
    "            break\n",
    "    if (i % 100 == 0):\n",
    "        print(i)\n",
    "            \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "465acf04-e1fd-4229-95f1-461290b401d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('hf_train_2.csv')\n",
    "test_df.to_csv('hf_test_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b078661b-fb9e-42c8-93a9-fb4d050ad14e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Using Previous Annotator Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ad759d-e7e4-4245-886e-41adc3cbb101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>MP_is_euph</th>\n",
       "      <th>kira_is_euph</th>\n",
       "      <th>kira_diff</th>\n",
       "      <th>kira_interpretation</th>\n",
       "      <th>kira_confidence</th>\n",
       "      <th>raz_is_euph</th>\n",
       "      <th>raz_diff</th>\n",
       "      <th>raz_interpretation</th>\n",
       "      <th>...</th>\n",
       "      <th>kenna_diff</th>\n",
       "      <th>kenna_interpretation</th>\n",
       "      <th>kenna_confidence</th>\n",
       "      <th>kelly_is_euph</th>\n",
       "      <th>kelly_diff</th>\n",
       "      <th>kelly_interpretation</th>\n",
       "      <th>sum diff</th>\n",
       "      <th>abs_diff</th>\n",
       "      <th>kelly_confidence</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disabled</td>\n",
       "      <td>I was homeschooled up until 5th grade, which i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>disabled</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>disabled is the most commonly accepted term by...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>impaired health condition</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>individuals with mental disability</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>detainees</td>\n",
       "      <td>1. (U ) The US Army Criminal Investigation Com...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>those who are detained</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>those in detainment</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>people held by the police</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>person held in custody literal meaning</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disabled</td>\n",
       "      <td>No no no no. I'm in the same situation-- &lt;disa...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>disabled</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>having a disability</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>physically or mentally impaired</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>incapacitated in some way</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>latrine</td>\n",
       "      <td>Access to basic sanitation includes safety and...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>latrine</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a pit in the earth for defacation</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>bathroom</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>latrine - not bathroom or lavatory</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disabled</td>\n",
       "      <td>Got all the working people and stay-home moms ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>disabled</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>not ablebodied</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>physically or mentally impaired</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>incapacitated in some way</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>plump</td>\n",
       "      <td>Hello, Mr. Biddle! chirped a beautiful blonde ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>round</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>fat</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>round</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>chunky</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>armed conflict</td>\n",
       "      <td>The \"Durand Line\" that @ @ @ @ @ @ @ @ @ @ lar...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>war</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>war</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>war</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>war</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>less fortunate</td>\n",
       "      <td>We are excited as a church to partner with Uni...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>poorer</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>poor</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>poor</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>more poor</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>overweight</td>\n",
       "      <td>As someone who struggles with their weight at ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>overweight or fat</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>fat</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>fat</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>fat</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>late</td>\n",
       "      <td>That was really bad for public health. In Nige...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dead or deceased</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Dead</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>dead</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dead</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            keyword                                        edited_text  \\\n",
       "0          disabled  I was homeschooled up until 5th grade, which i...   \n",
       "1         detainees  1. (U ) The US Army Criminal Investigation Com...   \n",
       "2          disabled  No no no no. I'm in the same situation-- <disa...   \n",
       "3           latrine  Access to basic sanitation includes safety and...   \n",
       "4          disabled  Got all the working people and stay-home moms ...   \n",
       "..              ...                                                ...   \n",
       "495           plump  Hello, Mr. Biddle! chirped a beautiful blonde ...   \n",
       "496  armed conflict  The \"Durand Line\" that @ @ @ @ @ @ @ @ @ @ lar...   \n",
       "497  less fortunate  We are excited as a church to partner with Uni...   \n",
       "498      overweight  As someone who struggles with their weight at ...   \n",
       "499            late  That was really bad for public health. In Nige...   \n",
       "\n",
       "     MP_is_euph  kira_is_euph  kira_diff     kira_interpretation  \\\n",
       "0             1             0          1                disabled   \n",
       "1             1             0          1  those who are detained   \n",
       "2             1             0          1                disabled   \n",
       "3             1             0          1                 latrine   \n",
       "4             1             0          1                disabled   \n",
       "..          ...           ...        ...                     ...   \n",
       "495           1             1          0                   round   \n",
       "496           1             1          0                     war   \n",
       "497           1             1          0                  poorer   \n",
       "498           1             1          0       overweight or fat   \n",
       "499           1             1          0        dead or deceased   \n",
       "\n",
       "     kira_confidence  raz_is_euph  raz_diff  \\\n",
       "0                  3            0         1   \n",
       "1                  3            0         1   \n",
       "2                  3            0         1   \n",
       "3                  3            0         1   \n",
       "4                  3            0         1   \n",
       "..               ...          ...       ...   \n",
       "495                3            1         0   \n",
       "496                3            1         0   \n",
       "497                3            1         0   \n",
       "498                3            1         0   \n",
       "499                3            1         0   \n",
       "\n",
       "                                    raz_interpretation  ...  kenna_diff  \\\n",
       "0    disabled is the most commonly accepted term by...  ...           1   \n",
       "1                                  those in detainment  ...           1   \n",
       "2                                  having a disability  ...           1   \n",
       "3                    a pit in the earth for defacation  ...           1   \n",
       "4                                       not ablebodied  ...           1   \n",
       "..                                                 ...  ...         ...   \n",
       "495                                                fat  ...           0   \n",
       "496                                                war  ...           0   \n",
       "497                                               poor  ...           0   \n",
       "498                                                fat  ...           0   \n",
       "499                                               Dead  ...           0   \n",
       "\n",
       "                kenna_interpretation  kenna_confidence kelly_is_euph  \\\n",
       "0          impaired health condition                 3             0   \n",
       "1          people held by the police                 3             0   \n",
       "2    physically or mentally impaired                 3             0   \n",
       "3                           bathroom                 2             0   \n",
       "4    physically or mentally impaired                 3             0   \n",
       "..                               ...               ...           ...   \n",
       "495                            round                 3             1   \n",
       "496                              war                 3             1   \n",
       "497                             poor                 3             1   \n",
       "498                              fat                 3             1   \n",
       "499                             dead                 3             1   \n",
       "\n",
       "     kelly_diff                    kelly_interpretation  sum diff abs_diff  \\\n",
       "0             1      individuals with mental disability         0        0   \n",
       "1             1  person held in custody literal meaning         0        0   \n",
       "2             1               incapacitated in some way         0        0   \n",
       "3             1      latrine - not bathroom or lavatory         0        0   \n",
       "4             1               incapacitated in some way         0        0   \n",
       "..          ...                                     ...       ...      ...   \n",
       "495           0                                  chunky         4        4   \n",
       "496           0                                     war         4        4   \n",
       "497           0                               more poor         4        4   \n",
       "498           0                                     fat         4        4   \n",
       "499           0                                    dead         4        4   \n",
       "\n",
       "     kelly_confidence  sum  \n",
       "0                   2    1  \n",
       "1                   2    1  \n",
       "2                   3    1  \n",
       "3                   3    1  \n",
       "4                   3    1  \n",
       "..                ...  ...  \n",
       "495                 3    5  \n",
       "496                 3    5  \n",
       "497                 3    5  \n",
       "498                 2    5  \n",
       "499                 3    5  \n",
       "\n",
       "[500 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "euph_corpus = pd.read_csv(\"Euphemism_Corpus_v2.0.csv\", index_col=0)\n",
    "annotator_data = pd.read_csv(\"All_Annotator_Data_Fall_2021.csv\", index_col=False)\n",
    "annotator_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7603381-eea0-4767-94a2-9feab1a29dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "['slim', 'between jobs', 'accident', 'late', 'number one', 'sleep with', 'seasoned', 'wealthy', 'over the hill', 'plump', 'let go of', 'go all the way', 'overweight', 'sober', 'number two', 'slept with', 'dismissed', 'let them go', 'aging', 'expecting', 'stout', 'troubled', 'with child', 'invalid', 'experienced', 'getting clean', 'custodian', 'got clean', 'long sleep', 'mixed up', 'chest', 'same-sex', 'economical', 'passing on', 'neutralize', 'outspoken', 'gluteus maximus', 'sleep around', 'pass on', 'disabled', 'special needs', 'pass away', 'a certain age', 'well off', 'less fortunate', 'mistruths', 'droppings', 'lose your lunch', 'pregnancy termination', 'let him go', 'golden years', 'mentally challenged', 'tinkle', 'demise', 'drinking problem', 'indigent', 'detainee', 'advanced age', 'comfort women', 'time of the month', 'pass gas', 'portly', 'went to heaven', 'venereal disease', 'put to sleep', 'mistruth', 'differently-abled', 'intoxicated', 'economical with the truth', 'lavatory', 'birds and the bees', 'deceased', 'terminating a pregnancy', 'inebriated', 'inner city', 'regime change', 'enhanced interrogation techniques', 'adult beverages', 'to go to heaven', 'dearly departed', 'passed away', 'downsize', 'ethnic cleansing', 'substance abusers', 'broken home', 'made love', 'plus-sized', 'underprivileged', 'rear end', 'armed conflict', 'substance abuse', 'disadvantaged', 'neutralized', 'capital punishment', 'street person', 'making love', 'freedom fighters']\n"
     ]
    }
   ],
   "source": [
    "high_agreement = annotator_data.loc[annotator_data['sum'].isin([0, 5])]\n",
    "mp_wrong_1s = high_agreement.loc[high_agreement['sum'] == 1].loc[high_agreement['MP_is_euph'] == 1]\n",
    "mp_wrong_0s = high_agreement.loc[high_agreement['sum'] == 4].loc[high_agreement['MP_is_euph'] == 0]\n",
    "high_agreement = high_agreement.drop(mp_wrong_1s.index)\n",
    "high_agreement = high_agreement.drop(mp_wrong_0s.index)\n",
    "print(len(high_agreement['keyword'].unique()))\n",
    "print(high_agreement['keyword'].unique().tolist())\n",
    "# low_agreement = annotator_data.loc[annotator_data['sum'].isin([2, 3])]\n",
    "# print(len(set(high_agreement['keyword'].unique()).difference(set(low_agreement['keyword'].unique()))))\n",
    "# print(set(high_agreement['keyword'].unique()).difference(set(low_agreement['keyword'].unique())))\n",
    "# high_agreement.intersection(low_agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbf8acf3-bf07-42cf-b916-84ed88809754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stout', 'homemaker', 'dismissed', 'special needs', 'targeted killings', 'elderly', 'put to sleep', 'psychiatric hospital', 'running behind', 'same-sex', 'same sex', 'underdeveloped', 'outlived her usefulness', 'negative cash flow', 'collateral damage', 'weed', 'income inequality', 'intoxicated', 'latrine', 'long sleep', 'aging', 'let him go', 'downsize', 'custodians', 'perished', 'developing country', 'overweight', 'underprivileged', 'demise', 'global south', 'pass on', 'disadvantaged', 'undocumented immigrants', 'troubled', 'outspoken', 'undocumented workers', 'wealthy', 'chest', 'developed country', 'economical', 'mixed up', 'invalid', 'seasoned', 'developmentally disabled', 'accident', 'pass away', 'let her go', 'venereal disease', 'a certain age', 'with child', 'expecting', 'laying off', 'regime change', 'disabled', 'well off', 'able-bodied', 'sober', 'pro-choice', 'passed away', 'deprived', 'seeing someone', 'outlived his usefulness', 'freedom fighter', 'sanitation worker', 'exterminate', 'slim', 'perish', 'low-income', 'people of color', 'custodian'}\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(set(low_agreement['keyword'].unique()))\n",
    "print(len(set(low_agreement['keyword'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b9030-cd9c-44cc-82a7-452dbab981a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "euphemismEnv",
   "language": "python",
   "name": "euphemismenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
